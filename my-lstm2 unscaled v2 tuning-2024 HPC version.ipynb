{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Tuning) Second version of model (has early stopping, learning rate scheduler, dropout, multiple lstm layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training model: model_b5_e150_lr0.0001_h256_l1_d0.1_w0.6_0.3_0.1.pth\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 1\n",
      "Average Loss: 61.705839\n",
      "Laptime Loss: 101.412381\n",
      "Position Loss: 2.779234\n",
      "Historical Loss: 0.246384\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 2\n",
      "Average Loss: 45.277839\n",
      "Laptime Loss: 74.082872\n",
      "Position Loss: 2.676391\n",
      "Historical Loss: 0.251978\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 3\n",
      "Average Loss: 44.837854\n",
      "Laptime Loss: 73.394351\n",
      "Position Loss: 2.589370\n",
      "Historical Loss: 0.244314\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 4\n",
      "Average Loss: 43.823428\n",
      "Laptime Loss: 71.670018\n",
      "Position Loss: 2.655624\n",
      "Historical Loss: 0.247281\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 5\n",
      "Average Loss: 43.308506\n",
      "Laptime Loss: 70.776675\n",
      "Position Loss: 2.728307\n",
      "Historical Loss: 0.240080\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 6\n",
      "Average Loss: 42.755959\n",
      "Laptime Loss: 69.905891\n",
      "Position Loss: 2.627441\n",
      "Historical Loss: 0.241904\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 7\n",
      "Average Loss: 42.264094\n",
      "Laptime Loss: 69.118090\n",
      "Position Loss: 2.560692\n",
      "Historical Loss: 0.250309\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 8\n",
      "Average Loss: 41.908715\n",
      "Laptime Loss: 68.523458\n",
      "Position Loss: 2.569098\n",
      "Historical Loss: 0.239091\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 9\n",
      "Average Loss: 41.540313\n",
      "Laptime Loss: 67.841573\n",
      "Position Loss: 2.701113\n",
      "Historical Loss: 0.250340\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 10\n",
      "Average Loss: 41.037405\n",
      "Laptime Loss: 66.977364\n",
      "Position Loss: 2.754512\n",
      "Historical Loss: 0.246323\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 11\n",
      "Average Loss: 40.823588\n",
      "Laptime Loss: 66.640104\n",
      "Position Loss: 2.715946\n",
      "Historical Loss: 0.247404\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 12\n",
      "Average Loss: 40.358715\n",
      "Laptime Loss: 65.860300\n",
      "Position Loss: 2.725340\n",
      "Historical Loss: 0.249320\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 13\n",
      "Average Loss: 40.130668\n",
      "Laptime Loss: 65.480602\n",
      "Position Loss: 2.724846\n",
      "Historical Loss: 0.248517\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 14\n",
      "Average Loss: 39.999153\n",
      "Laptime Loss: 65.258108\n",
      "Position Loss: 2.731768\n",
      "Historical Loss: 0.247559\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 15\n",
      "Average Loss: 39.817430\n",
      "Laptime Loss: 64.931241\n",
      "Position Loss: 2.777750\n",
      "Historical Loss: 0.253585\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 16\n",
      "Average Loss: 39.780512\n",
      "Laptime Loss: 64.923971\n",
      "Position Loss: 2.671446\n",
      "Historical Loss: 0.246941\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 17\n",
      "Average Loss: 39.716701\n",
      "Laptime Loss: 64.815960\n",
      "Position Loss: 2.675402\n",
      "Historical Loss: 0.245025\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 18\n",
      "Average Loss: 39.597417\n",
      "Laptime Loss: 64.580483\n",
      "Position Loss: 2.747095\n",
      "Historical Loss: 0.249969\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 19\n",
      "Average Loss: 39.592909\n",
      "Laptime Loss: 64.577508\n",
      "Position Loss: 2.739184\n",
      "Historical Loss: 0.246477\n",
      "--------------------\n",
      "Epoch 20\n",
      "Average Loss: 39.598328\n",
      "Laptime Loss: 64.604380\n",
      "Position Loss: 2.702596\n",
      "Historical Loss: 0.249197\n",
      "--------------------\n",
      "Epoch 21\n",
      "Average Loss: 39.773052\n",
      "Laptime Loss: 64.851288\n",
      "Position Loss: 2.792583\n",
      "Historical Loss: 0.245025\n",
      "--------------------\n",
      "Epoch 22\n",
      "Average Loss: 39.709395\n",
      "Laptime Loss: 64.765852\n",
      "Position Loss: 2.751545\n",
      "Historical Loss: 0.244190\n",
      "--------------------\n",
      "Epoch 23\n",
      "Average Loss: 39.622542\n",
      "Laptime Loss: 64.672735\n",
      "Position Loss: 2.647713\n",
      "Historical Loss: 0.245859\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 24\n",
      "Average Loss: 39.428050\n",
      "Laptime Loss: 64.373788\n",
      "Position Loss: 2.597281\n",
      "Historical Loss: 0.245921\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 25\n",
      "Average Loss: 39.113759\n",
      "Laptime Loss: 63.847281\n",
      "Position Loss: 2.604203\n",
      "Historical Loss: 0.241286\n",
      "--------------------\n",
      "Epoch 26\n",
      "Average Loss: 39.304511\n",
      "Laptime Loss: 64.126108\n",
      "Position Loss: 2.679357\n",
      "Historical Loss: 0.250371\n",
      "--------------------\n",
      "Epoch 27\n",
      "Average Loss: 39.149642\n",
      "Laptime Loss: 63.916629\n",
      "Position Loss: 2.583931\n",
      "Historical Loss: 0.244839\n",
      "--------------------\n",
      "Epoch 28\n",
      "Average Loss: 39.117782\n",
      "Laptime Loss: 63.884656\n",
      "Position Loss: 2.540420\n",
      "Historical Loss: 0.248609\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 29\n",
      "Average Loss: 39.097182\n",
      "Laptime Loss: 63.813786\n",
      "Position Loss: 2.615080\n",
      "Historical Loss: 0.243850\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 30\n",
      "Average Loss: 39.065715\n",
      "Laptime Loss: 63.772409\n",
      "Position Loss: 2.593325\n",
      "Historical Loss: 0.242707\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 31\n",
      "Average Loss: 38.984383\n",
      "Laptime Loss: 63.654326\n",
      "Position Loss: 2.557726\n",
      "Historical Loss: 0.244685\n",
      "--------------------\n",
      "Epoch 32\n",
      "Average Loss: 39.011313\n",
      "Laptime Loss: 63.666334\n",
      "Position Loss: 2.621508\n",
      "Historical Loss: 0.250587\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 33\n",
      "Average Loss: 38.930772\n",
      "Laptime Loss: 63.602866\n",
      "Position Loss: 2.481582\n",
      "Historical Loss: 0.245766\n",
      "--------------------\n",
      "Epoch 34\n",
      "Average Loss: 38.994675\n",
      "Laptime Loss: 63.696954\n",
      "Position Loss: 2.508776\n",
      "Historical Loss: 0.238690\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 35\n",
      "Average Loss: 38.923731\n",
      "Laptime Loss: 63.550371\n",
      "Position Loss: 2.562176\n",
      "Historical Loss: 0.248548\n",
      "--------------------\n",
      "Epoch 36\n",
      "Average Loss: 38.943573\n",
      "Laptime Loss: 63.580273\n",
      "Position Loss: 2.570087\n",
      "Historical Loss: 0.243820\n",
      "--------------------\n",
      "Epoch 37\n",
      "Average Loss: 38.924874\n",
      "Laptime Loss: 63.582951\n",
      "Position Loss: 2.499382\n",
      "Historical Loss: 0.252874\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 38\n",
      "Average Loss: 38.855903\n",
      "Laptime Loss: 63.441866\n",
      "Position Loss: 2.553770\n",
      "Historical Loss: 0.246508\n",
      "--------------------\n",
      "Epoch 39\n",
      "Average Loss: 38.910567\n",
      "Laptime Loss: 63.540141\n",
      "Position Loss: 2.539431\n",
      "Historical Loss: 0.246508\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 40\n",
      "Average Loss: 38.840464\n",
      "Laptime Loss: 63.399731\n",
      "Position Loss: 2.587886\n",
      "Historical Loss: 0.242583\n",
      "--------------------\n",
      "Epoch 41\n",
      "Average Loss: 38.875419\n",
      "Laptime Loss: 63.451464\n",
      "Position Loss: 2.600742\n",
      "Historical Loss: 0.243171\n",
      "--------------------\n",
      "Epoch 42\n",
      "Average Loss: 38.849533\n",
      "Laptime Loss: 63.458558\n",
      "Position Loss: 2.500371\n",
      "Historical Loss: 0.242862\n",
      "--------------------\n",
      "Epoch 43\n",
      "Average Loss: 38.843542\n",
      "Laptime Loss: 63.383044\n",
      "Position Loss: 2.628925\n",
      "Historical Loss: 0.250371\n",
      "--------------------\n",
      "Epoch 44\n",
      "Average Loss: 38.843234\n",
      "Laptime Loss: 63.389220\n",
      "Position Loss: 2.617058\n",
      "Historical Loss: 0.245828\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 45\n",
      "Average Loss: 38.793486\n",
      "Laptime Loss: 63.286556\n",
      "Position Loss: 2.659580\n",
      "Historical Loss: 0.236774\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 46\n",
      "Average Loss: 38.774046\n",
      "Laptime Loss: 63.316165\n",
      "Position Loss: 2.533004\n",
      "Historical Loss: 0.244438\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 47\n",
      "Average Loss: 38.707043\n",
      "Laptime Loss: 63.220867\n",
      "Position Loss: 2.499876\n",
      "Historical Loss: 0.245581\n",
      "--------------------\n",
      "Epoch 48\n",
      "Average Loss: 38.729553\n",
      "Laptime Loss: 63.231865\n",
      "Position Loss: 2.554759\n",
      "Historical Loss: 0.240049\n",
      "--------------------\n",
      "Epoch 49\n",
      "Average Loss: 38.709442\n",
      "Laptime Loss: 63.204012\n",
      "Position Loss: 2.540915\n",
      "Historical Loss: 0.247590\n",
      "--------------------\n",
      "Epoch 50\n",
      "Average Loss: 38.739575\n",
      "Laptime Loss: 63.253811\n",
      "Position Loss: 2.542398\n",
      "Historical Loss: 0.245674\n",
      "--------------------\n",
      "Epoch 51\n",
      "Average Loss: 38.718914\n",
      "Laptime Loss: 63.229311\n",
      "Position Loss: 2.523115\n",
      "Historical Loss: 0.243912\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 52\n",
      "Average Loss: 38.647051\n",
      "Laptime Loss: 63.123033\n",
      "Position Loss: 2.495921\n",
      "Historical Loss: 0.244530\n",
      "--------------------\n",
      "Epoch 53\n",
      "Average Loss: 38.747502\n",
      "Laptime Loss: 63.254687\n",
      "Position Loss: 2.567120\n",
      "Historical Loss: 0.245519\n",
      "--------------------\n",
      "Epoch 54\n",
      "Average Loss: 38.683832\n",
      "Laptime Loss: 63.130432\n",
      "Position Loss: 2.601236\n",
      "Historical Loss: 0.252009\n",
      "--------------------\n",
      "Epoch 55\n",
      "Average Loss: 38.708557\n",
      "Laptime Loss: 63.180962\n",
      "Position Loss: 2.584920\n",
      "Historical Loss: 0.245025\n",
      "--------------------\n",
      "Epoch 56\n",
      "Average Loss: 38.746121\n",
      "Laptime Loss: 63.260756\n",
      "Position Loss: 2.549815\n",
      "Historical Loss: 0.247219\n",
      "--------------------\n",
      "Epoch 57\n",
      "Average Loss: 38.729111\n",
      "Laptime Loss: 63.248547\n",
      "Position Loss: 2.516193\n",
      "Historical Loss: 0.251236\n",
      "--------------------\n",
      "Epoch 58\n",
      "Average Loss: 38.757791\n",
      "Laptime Loss: 63.287715\n",
      "Position Loss: 2.534487\n",
      "Historical Loss: 0.248146\n",
      "--------------------\n",
      "Epoch 59\n",
      "Average Loss: 38.728834\n",
      "Laptime Loss: 63.249162\n",
      "Position Loss: 2.513226\n",
      "Historical Loss: 0.253677\n",
      "--------------------\n",
      "Epoch 60\n",
      "Average Loss: 38.700238\n",
      "Laptime Loss: 63.196964\n",
      "Position Loss: 2.526082\n",
      "Historical Loss: 0.242336\n",
      "--------------------\n",
      "Epoch 61\n",
      "Average Loss: 38.738274\n",
      "Laptime Loss: 63.257844\n",
      "Position Loss: 2.530037\n",
      "Historical Loss: 0.245550\n",
      "--------------------\n",
      "Epoch 62\n",
      "Average Loss: 38.733645\n",
      "Laptime Loss: 63.218289\n",
      "Position Loss: 2.593325\n",
      "Historical Loss: 0.246724\n",
      "--------------------\n",
      "Epoch 63\n",
      "Average Loss: 38.765274\n",
      "Laptime Loss: 63.221788\n",
      "Position Loss: 2.691718\n",
      "Historical Loss: 0.246848\n",
      "--------------------\n",
      "Epoch 64\n",
      "Average Loss: 38.771890\n",
      "Laptime Loss: 63.285003\n",
      "Position Loss: 2.586897\n",
      "Historical Loss: 0.248177\n",
      "--------------------\n",
      "Epoch 65\n",
      "Average Loss: 38.742470\n",
      "Laptime Loss: 63.236227\n",
      "Position Loss: 2.585909\n",
      "Historical Loss: 0.249598\n",
      "--------------------\n",
      "Epoch 66\n",
      "Average Loss: 38.729876\n",
      "Laptime Loss: 63.237075\n",
      "Position Loss: 2.542398\n",
      "Historical Loss: 0.249104\n",
      "Early stopping triggered at epoch 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54359096/ipykernel_857652/2456267903.py:670: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('./2024/best_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 64.73300966964318\n",
      "Outputs\n",
      "O: tensor([[9.0134, 8.9607, 8.7337, 8.5750, 8.4719, 8.4056, 8.3623, 8.3335, 8.3141,\n",
      "         8.3009, 8.2919, 9.0811, 9.9602, 9.0408, 8.7018, 8.5278, 8.4273, 8.3656,\n",
      "         8.3260, 8.3003, 8.2833, 8.2722, 8.2651, 8.2607, 8.2581, 8.2568, 8.2564,\n",
      "         8.2565, 8.2571, 8.2579, 8.2589, 8.2600, 9.0711, 9.6018, 8.7772, 8.4930,\n",
      "         8.3555, 8.2803, 8.2364, 8.2097, 8.1932, 8.1831, 8.1772, 8.1741, 8.1729,\n",
      "         8.1729, 8.1737, 8.1750, 8.1766, 8.1783, 8.1801, 8.1818, 8.1835, 8.1850,\n",
      "         8.1863, 8.1874, 8.1881, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830]], device='cuda:0')\n",
      "O: tensor([[ 9.1588,  9.0908,  8.8895,  8.7606,  8.6829,  8.6369,  8.6088,  8.5913,\n",
      "          8.5803,  8.5733,  8.5689,  8.5662,  8.5646,  8.5637,  9.3088, 10.0812,\n",
      "          9.1377,  8.8248,  8.6834,  8.6113,  8.5713,  8.5475,  8.5329,  8.5237,\n",
      "          8.5178,  8.5141,  8.5117,  8.5102,  8.5092,  8.5085,  8.5079,  8.5074,\n",
      "          8.5068,  8.5062,  8.5054,  8.5044,  8.5033,  8.5020,  8.5004,  8.4987,\n",
      "          9.2749,  9.7445,  8.8874,  8.6233,  8.5122,  8.4598,  8.4330,  8.4184,\n",
      "          8.4101,  8.4054,  8.4028,  8.4016,  8.4013,  8.4015,  8.4021,  8.4029,\n",
      "          8.4037,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[8.8989, 8.8239, 8.6336, 8.5202, 8.4564, 8.4215, 8.4024, 8.3923, 8.3874,\n",
      "         8.3857, 8.3860, 9.1560, 9.7982, 8.8945, 8.6087, 8.4844, 8.4240, 8.3928,\n",
      "         8.3763, 8.3678, 8.3639, 8.3627, 8.3632, 8.3647, 8.3666, 8.3687, 8.3708,\n",
      "         8.3728, 8.3745, 8.3759, 8.3769, 8.3775, 8.3776, 8.3773, 8.3765, 9.1682,\n",
      "         9.5851, 8.7766, 8.5350, 8.4373, 8.3939, 8.3736, 8.3640, 8.3598, 8.3584,\n",
      "         8.3586, 8.3597, 8.3613, 8.3630, 8.3647, 8.3663, 8.3678, 8.3690, 8.3699,\n",
      "         8.3705, 8.3708, 8.3708, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830]], device='cuda:0')\n",
      "O: tensor([[ 9.2556,  9.1603,  8.9527,  8.8211,  8.7431,  8.6976,  8.6705,  8.6543,\n",
      "          8.6449,  8.6397,  8.6372,  8.6366,  9.3222, 10.1275,  9.2126,  8.9029,\n",
      "          8.7613,  8.6887,  8.6487,  8.6257,  8.6123,  8.6047,  8.6007,  8.5989,\n",
      "          8.5987,  8.5994,  8.6007,  8.6023,  8.6040,  8.6058,  8.6075,  8.6091,\n",
      "          8.6105,  8.6118,  9.3179,  9.8224,  9.0181,  8.7595,  8.6496,  8.5977,\n",
      "          8.5713,  8.5572,  8.5496,  8.5457,  8.5440,  8.5436,  8.5440,  8.5450,\n",
      "          8.5463,  8.5478,  8.5494,  8.5510,  8.5526,  8.5541,  8.5555,  8.5569,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.1497,  9.1768,  8.9904,  8.8750,  8.8124,  8.7785,  8.7594,  8.7480,\n",
      "          8.7408,  8.7362,  8.7330,  9.3160, 10.2102,  9.2722,  8.9594,  8.8273,\n",
      "          8.7659,  8.7349,  8.7178,  8.7078,  8.7016,  8.6976,  8.6948,  8.6928,\n",
      "          8.6913,  8.6899,  8.6886,  8.6873,  8.6859,  9.2900, 10.0289,  9.1442,\n",
      "          8.8574,  8.7407,  8.6889,  8.6640,  8.6511,  8.6438,  8.6396,  8.6370,\n",
      "          8.6353,  8.6343,  8.6335,  8.6330,  8.6326,  8.6321,  8.6317,  8.6312,\n",
      "          8.6306,  8.6299,  8.6291,  8.6281,  8.6271,  8.6258,  8.6244,  8.6228,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[9.2259, 9.0579, 8.8927, 8.7818, 8.7152, 8.6762, 8.6530, 8.6392, 8.6311,\n",
      "         8.6266, 9.3098, 9.9568, 9.1382, 8.8626, 8.7371, 8.6731, 8.6380, 8.6178,\n",
      "         8.6062, 8.5997, 8.5964, 8.5953, 8.5955, 8.5966, 8.5983, 8.6004, 8.6027,\n",
      "         8.6050, 8.6074, 8.6098, 8.6120, 9.3126, 9.7344, 8.9794, 8.7413, 8.6400,\n",
      "         8.5920, 8.5677, 8.5550, 8.5485, 8.5455, 8.5446, 8.5451, 8.5464, 8.5481,\n",
      "         8.5502, 8.5524, 8.5546, 8.5567, 8.5587, 8.5605, 8.5622, 8.5636, 8.5647,\n",
      "         8.5656, 8.5662, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830]], device='cuda:0')\n",
      "O: tensor([[9.1273, 8.8875, 8.6231, 8.4852, 8.4032, 8.3542, 8.3247, 8.3072, 8.2970,\n",
      "         8.2915, 8.2890, 8.2882, 8.2885, 8.2894, 8.2905, 8.2916, 9.1336, 9.8167,\n",
      "         8.8835, 8.5879, 8.4532, 8.3838, 8.3459, 8.3252, 8.3142, 8.3092, 8.3080,\n",
      "         8.3091, 8.3116, 8.3150, 8.3188, 8.3228, 8.3269, 8.3308, 8.3346, 8.3381,\n",
      "         9.2099, 9.5868, 8.7947, 8.5543, 8.4527, 8.4053, 8.3825, 8.3721, 8.3685,\n",
      "         8.3687, 8.3714, 8.3755, 8.3804, 8.3859, 8.3917, 8.3975, 8.4034, 8.4092,\n",
      "         8.4148, 8.4203, 8.4256, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830]], device='cuda:0')\n",
      "O: tensor([[8.8815, 8.8035, 8.6374, 8.5298, 8.4678, 8.4334, 8.4144, 8.4040, 8.3987,\n",
      "         8.3966, 8.3965, 8.3977, 8.3995, 9.1651, 9.7171, 8.8694, 8.6028, 8.4867,\n",
      "         8.4301, 8.4008, 8.3852, 8.3771, 8.3733, 8.3722, 8.3727, 8.3742, 8.3762,\n",
      "         8.3785, 8.3808, 8.3831, 8.3852, 8.3871, 8.3888, 8.3901, 9.1800, 9.5475,\n",
      "         8.7664, 8.5330, 8.4370, 8.3934, 8.3725, 8.3624, 8.3578, 8.3562, 8.3563,\n",
      "         8.3574, 8.3591, 8.3611, 8.3631, 8.3650, 8.3667, 8.3683, 8.3695, 8.3705,\n",
      "         8.3710, 8.3712, 8.3710, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830]], device='cuda:0')\n",
      "O: tensor([[ 9.3958,  9.1768,  9.0231,  8.9181,  8.8559,  8.8198,  8.7982,  8.7849,\n",
      "          8.7766,  9.4046, 10.0522,  9.2660,  8.9970,  8.8760,  8.8150,  8.7817,\n",
      "          8.7624,  8.7508,  8.7438,  8.7397,  8.7374,  8.7363,  8.7359,  8.7361,\n",
      "          8.7365,  8.7372,  8.7379,  8.7385,  8.7392,  9.3795,  9.8488,  9.1161,\n",
      "          8.8794,  8.7791,  8.7319,  8.7079,  8.6950,  8.6879,  8.6839,  8.6819,\n",
      "          8.6810,  8.6808,  8.6811,  8.6815,  8.6820,  8.6826,  8.6830,  8.6833,\n",
      "          8.6834,  8.6833,  8.6830,  8.6825,  8.6817,  8.6806,  8.6792,  8.6775,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[9.0993, 8.9732, 8.8050, 8.6937, 8.6281, 8.5903, 8.5682, 8.5551, 9.2470,\n",
      "         9.9280, 9.0745, 8.7881, 8.6590, 8.5938, 8.5583, 8.5381, 8.5265, 8.5199,\n",
      "         8.5166, 8.5154, 8.5155, 8.5165, 8.5180, 8.5199, 8.5219, 8.5240, 9.2334,\n",
      "         9.7383, 8.9409, 8.6877, 8.5797, 8.5286, 8.5028, 8.4892, 8.4821, 8.4788,\n",
      "         8.4778, 8.4781, 8.4794, 8.4811, 8.4831, 8.4853, 8.4875, 8.4896, 8.4916,\n",
      "         8.4934, 8.4949, 8.4963, 8.4973, 8.4981, 8.4985, 8.4986, 8.4983, 8.4976,\n",
      "         8.4965, 8.4949, 8.4929, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830]], device='cuda:0')\n",
      "O: tensor([[ 9.4110,  9.1730,  9.0255,  8.9241,  8.8644,  8.8298,  8.8091,  8.7963,\n",
      "          8.7883,  8.7832,  8.7801,  9.3975, 10.0208,  9.2479,  8.9869,  8.8712,\n",
      "          8.8139,  8.7831,  8.7654,  8.7548,  8.7483,  8.7445,  8.7423,  8.7412,\n",
      "          8.7407,  8.7407,  8.7409,  8.7412,  8.7416,  8.7419,  9.3769,  9.8319,\n",
      "          9.1106,  8.8781,  8.7804,  8.7350,  8.7121,  8.6999,  8.6930,  8.6892,\n",
      "          8.6870,  8.6860,  9.3469,  9.7062,  9.0443,  8.8304,  8.7432,  8.7044,\n",
      "          8.6858,  8.6762,  8.6709,  8.6679,  8.6661,  8.6650,  8.6643,  8.6638,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[8.8476, 8.8909, 8.7020, 8.5883, 8.5271, 8.4954, 8.4790, 8.4710, 8.4678,\n",
      "         8.4674, 9.2069, 9.9663, 8.9939, 8.6869, 8.5581, 8.4985, 8.4695, 8.4551,\n",
      "         8.4484, 8.4459, 8.4459, 8.4474, 8.4499, 8.4528, 8.4559, 8.4590, 8.4621,\n",
      "         8.4650, 8.4677, 8.4701, 8.4723, 8.4741, 8.4755, 9.2365, 9.7904, 8.8760,\n",
      "         8.6031, 8.4945, 8.4475, 8.4263, 8.4167, 8.4127, 8.4117, 8.4123, 8.4137,\n",
      "         8.4157, 8.4179, 8.4201, 8.4223, 8.4243, 8.4261, 8.4277, 8.4291, 8.4302,\n",
      "         8.4310, 8.4315, 8.4317, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830]], device='cuda:0')\n",
      "O: tensor([[8.9892, 8.9429, 8.7692, 8.6613, 8.6022, 8.5708, 8.5540, 8.5452, 8.5409,\n",
      "         8.5394, 8.5396, 8.5409, 9.2423, 9.9488, 9.0289, 8.7380, 8.6161, 8.5597,\n",
      "         8.5320, 8.5180, 8.5111, 8.5082, 8.5076, 8.5085, 8.5101, 8.5123, 8.5147,\n",
      "         8.5172, 8.5196, 8.5219, 8.5240, 8.5259, 9.2483, 9.7750, 8.9133, 8.6514,\n",
      "         8.5467, 8.5013, 8.4807, 8.4713, 8.4674, 8.4663, 8.4668, 8.4682, 8.4701,\n",
      "         8.4722, 8.4743, 8.4763, 8.4782, 8.4799, 8.4812, 8.4823, 8.4831, 8.4834,\n",
      "         8.4834, 8.4830, 8.4822, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830]], device='cuda:0')\n",
      "O: tensor([[ 8.8668,  8.9195,  8.7257,  8.6069,  8.5434,  8.5108,  8.4941,  8.4859,\n",
      "          8.4825,  8.4821,  9.2045, 10.0417,  9.0302,  8.7094,  8.5753,  8.5138,\n",
      "          8.4840,  8.4693,  8.4624,  8.4598,  8.4598,  8.4614,  8.4639,  8.4669,\n",
      "          8.4702,  8.4736,  8.4770,  8.4803,  8.4834,  8.4863,  9.2310,  9.8702,\n",
      "          8.9164,  8.6264,  8.5106,  8.4605,  8.4379,  8.4276,  8.4234,  8.4223,\n",
      "          8.4230,  8.4247,  8.4269,  8.4293,  8.4319,  8.4344,  8.4368,  8.4390,\n",
      "          8.4411,  8.4428,  8.4443,  8.4455,  8.4464,  8.4470,  8.4473,  8.4472,\n",
      "          8.4468,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.3749,  9.0753,  8.8948,  8.7807,  8.7150,  8.6774,  8.6550,  8.6412,\n",
      "          8.6324,  8.6267,  8.6231,  8.6208,  8.6194,  8.6186,  9.2883, 10.0177,\n",
      "          9.1168,  8.8253,  8.6995,  8.6387,  8.6066,  8.5885,  8.5778,  8.5713,\n",
      "          8.5674,  8.5651,  8.5638,  8.5631,  8.5627,  8.5626,  8.5625,  8.5624,\n",
      "          8.5622,  8.5618,  8.5612,  9.2523,  9.7791,  8.9415,  8.6818,  8.5754,\n",
      "          8.5272,  8.5035,  8.4909,  8.4839,  8.4798,  8.4773,  8.4757,  8.4746,\n",
      "          8.4738,  8.4730,  8.4721,  8.4710,  8.4697,  8.4680,  8.4660,  8.4635,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.1448,  9.0799,  8.8825,  8.7581,  8.6885,  8.6500,  8.6282,  8.6158,\n",
      "          8.6089,  8.6054,  8.6041,  8.6042,  8.6052,  9.2645, 10.1162,  9.1498,\n",
      "          8.8334,  8.6969,  8.6317,  8.5982,  8.5801,  8.5702,  8.5650,  8.5627,\n",
      "          8.5621,  8.5627,  8.5640,  8.5657,  8.5676,  8.5696,  8.5716,  8.5735,\n",
      "          8.5752,  9.2575,  9.8962,  8.9972,  8.7143,  8.5982,  8.5459,  8.5207,\n",
      "          8.5080,  8.5014,  8.4982,  8.4969,  8.4967,  8.4972,  8.4980,  8.4989,\n",
      "          8.4998,  8.5006,  8.5013,  8.5017,  8.5019,  8.5017,  8.5013,  8.5005,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.3844,  9.1079,  8.9493,  8.8446,  8.7846,  8.7505,  8.7300,  8.7170,\n",
      "          9.3502, 10.0697,  9.2097,  8.9226,  8.7980,  8.7375,  8.7051,  8.6862,\n",
      "          8.6743,  8.6664,  8.6608,  8.6567,  8.6535,  8.6509,  8.6485,  8.6463,\n",
      "          8.6440,  8.6417,  8.6392,  9.2822,  9.8538,  9.0382,  8.7784,  8.6709,\n",
      "          8.6215,  8.5966,  8.5827,  8.5742,  8.5685,  8.5643,  8.5611,  8.5582,\n",
      "          8.5555,  8.5528,  8.5500,  8.5469,  8.5435,  8.5396,  8.5353,  8.5305,\n",
      "          8.5252,  8.5192,  8.5126,  8.5053,  8.4972,  8.4884,  8.4787,  8.4680,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[9.0948, 8.9875, 8.8160, 8.7090, 8.6504, 8.6190, 8.6019, 8.5925, 8.5875,\n",
      "         8.5852, 8.5846, 9.2752, 9.9843, 9.0736, 8.7830, 8.6606, 8.6037, 8.5753,\n",
      "         8.5605, 8.5526, 8.5487, 8.5471, 8.5468, 8.5473, 8.5484, 8.5496, 8.5509,\n",
      "         8.5521, 8.5532, 8.5541, 8.5547, 8.5550, 8.5549, 9.2622, 9.7763, 8.9193,\n",
      "         8.6590, 8.5549, 8.5094, 8.4883, 8.4780, 8.4730, 8.4707, 8.4698, 8.4697,\n",
      "         8.4699, 8.4703, 8.4705, 8.4707, 8.4705, 8.4700, 8.4691, 8.4678, 8.4661,\n",
      "         8.4638, 8.4609, 8.4575, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830,\n",
      "         1.6830, 1.6830, 1.6830, 1.6830, 1.6830, 1.6830]], device='cuda:0')\n",
      "O: tensor([[ 9.7110,  9.2473,  9.0600,  8.9419,  8.8760,  8.8385,  8.8156,  8.8004,\n",
      "          8.7897,  9.3895, 10.2598,  9.3291,  9.0156,  8.8803,  8.8149,  8.7799,\n",
      "          8.7589,  8.7452,  8.7355,  8.7281,  8.7222,  8.7173,  8.7128,  8.7087,\n",
      "          8.7047,  8.7007,  8.6966,  9.3042, 10.0294,  9.1463,  8.8593,  8.7404,\n",
      "          8.6857,  8.6578,  8.6418,  8.6315,  8.6241,  8.6183,  8.6133,  9.2440,\n",
      "          9.8619,  9.0390,  8.7699,  8.6608,  8.6122,  8.5880,  8.5742,  8.5650,\n",
      "          8.5580,  8.5519,  8.5463,  8.5408,  8.5352,  8.5293,  8.5231,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,\n",
      "          1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830,  1.6830]],\n",
      "       device='cuda:0')\n",
      "Mask\n",
      "[9.013389  8.960671  8.733699  8.575044  8.471864  8.405627  8.362344\n",
      " 8.33352   8.314082  8.300889  8.291913  9.081141  9.960212  9.040799\n",
      " 8.701848  8.527756  8.42733   8.365556  8.326049  8.300261  8.283317\n",
      " 8.272239  8.265118  8.260692  8.258111  8.256797  8.256353  8.256503\n",
      " 8.257055  8.2578745 8.258863  8.259951  9.071067  9.601848  8.7772455\n",
      " 8.49305   8.355472  8.280337  8.236392  8.209654  8.193161  8.183108\n",
      " 8.177242  8.174149  8.172902  8.172883  8.173665  8.174957  8.176551\n",
      " 8.178297  8.18009   8.181847  8.183501  8.185001  8.186301  8.187358\n",
      " 8.18813   1.6830463 1.6830463 1.6830463 1.6830463 1.6830463 1.6830463\n",
      " 1.6830463 1.6830463 1.6830463 1.6830463 1.6830463 1.6830463 1.6830463\n",
      " 1.6830463 1.6830463 1.6830463 1.6830463 1.6830463 1.6830463 1.6830463\n",
      " 1.6830463 1.6830463 1.6830463 1.6830463 1.6830463 1.6830463 1.6830463\n",
      " 1.6830463 1.6830463 1.6830463]\n",
      "END Mask\n",
      "[[9.013389  8.960671  8.733699  ... 1.6830463 1.6830463 1.6830463]\n",
      " [9.158814  9.090779  8.889539  ... 1.6830463 1.6830463 1.6830463]\n",
      " [8.898871  8.823863  8.633649  ... 1.6830463 1.6830463 1.6830463]\n",
      " ...\n",
      " [9.38438   9.107905  8.94926   ... 1.6830463 1.6830463 1.6830463]\n",
      " [9.0948105 8.987504  8.815982  ... 1.6830463 1.6830463 1.6830463]\n",
      " [9.710996  9.24726   9.059964  ... 1.6830463 1.6830463 1.6830463]]\n",
      "1 Name: 0    hamilton\n",
      "Name: driverRef, dtype: object\n",
      "4 Name: 3    alonso\n",
      "Name: driverRef, dtype: object\n",
      "815 Name: 814    perez\n",
      "Name: driverRef, dtype: object\n",
      "817 Name: 816    ricciardo\n",
      "Name: driverRef, dtype: object\n",
      "822 Name: 821    bottas\n",
      "Name: driverRef, dtype: object\n",
      "825 Name: 824    kevin_magnussen\n",
      "Name: driverRef, dtype: object\n",
      "830 Name: 829    max_verstappen\n",
      "Name: driverRef, dtype: object\n",
      "832 Name: 831    sainz\n",
      "Name: driverRef, dtype: object\n",
      "839 Name: 838    ocon\n",
      "Name: driverRef, dtype: object\n",
      "840 Name: 839    stroll\n",
      "Name: driverRef, dtype: object\n",
      "842 Name: 452    gasly\n",
      "Name: driverRef, dtype: object\n",
      "844 Name: 842    leclerc\n",
      "Name: driverRef, dtype: object\n",
      "846 Name: 844    norris\n",
      "Name: driverRef, dtype: object\n",
      "847 Name: 845    russell\n",
      "Name: driverRef, dtype: object\n",
      "848 Name: 846    albon\n",
      "Name: driverRef, dtype: object\n",
      "852 Name: 850    tsunoda\n",
      "Name: driverRef, dtype: object\n",
      "855 Name: 853    zhou\n",
      "Name: driverRef, dtype: object\n",
      "857 Name: 855    piastri\n",
      "Name: driverRef, dtype: object\n",
      "858 Name: 856    sargeant\n",
      "Name: driverRef, dtype: object\n",
      "\n",
      "\n",
      "Training model: model_b5_e150_lr0.0001_h256_l1_d0.1_w0.4_0.4_0.2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54359096/ipykernel_857652/2456267903.py:693: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame([new_result])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model saved\n",
      "--------------------\n",
      "Epoch 1\n",
      "Average Loss: 41.725889\n",
      "Laptime Loss: 101.412294\n",
      "Position Loss: 2.779234\n",
      "Historical Loss: 0.246384\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 2\n",
      "Average Loss: 30.717480\n",
      "Laptime Loss: 74.015578\n",
      "Position Loss: 2.651174\n",
      "Historical Loss: 0.253894\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 3\n",
      "Average Loss: 30.471289\n",
      "Laptime Loss: 73.425378\n",
      "Position Loss: 2.630902\n",
      "Historical Loss: 0.243881\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 4\n",
      "Average Loss: 29.777141\n",
      "Laptime Loss: 71.651580\n",
      "Position Loss: 2.667491\n",
      "Historical Loss: 0.247559\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 5\n",
      "Average Loss: 29.406205\n",
      "Laptime Loss: 70.701296\n",
      "Position Loss: 2.694190\n",
      "Historical Loss: 0.240049\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 6\n",
      "Average Loss: 29.095292\n",
      "Laptime Loss: 69.953633\n",
      "Position Loss: 2.663041\n",
      "Historical Loss: 0.243109\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 7\n",
      "Average Loss: 28.687810\n",
      "Laptime Loss: 69.012910\n",
      "Position Loss: 2.582942\n",
      "Historical Loss: 0.247342\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 8\n",
      "Average Loss: 28.541183\n",
      "Laptime Loss: 68.600933\n",
      "Position Loss: 2.628925\n",
      "Historical Loss: 0.246199\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 9\n",
      "Average Loss: 28.380829\n",
      "Laptime Loss: 68.251949\n",
      "Position Loss: 2.577998\n",
      "Historical Loss: 0.244252\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 10\n",
      "Average Loss: 28.024290\n",
      "Laptime Loss: 67.123178\n",
      "Position Loss: 2.813350\n",
      "Historical Loss: 0.248393\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 11\n",
      "Average Loss: 27.946412\n",
      "Laptime Loss: 67.013604\n",
      "Position Loss: 2.730284\n",
      "Historical Loss: 0.244283\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 12\n",
      "Average Loss: 27.721176\n",
      "Laptime Loss: 66.468204\n",
      "Position Loss: 2.714957\n",
      "Historical Loss: 0.239555\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 13\n",
      "Average Loss: 27.533854\n",
      "Laptime Loss: 66.057469\n",
      "Position Loss: 2.652658\n",
      "Historical Loss: 0.249011\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 14\n",
      "Average Loss: 27.392367\n",
      "Laptime Loss: 65.625493\n",
      "Position Loss: 2.732262\n",
      "Historical Loss: 0.246323\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 15\n",
      "Average Loss: 27.210750\n",
      "Laptime Loss: 65.152091\n",
      "Position Loss: 2.747095\n",
      "Historical Loss: 0.255377\n",
      "--------------------\n",
      "Epoch 16\n",
      "Average Loss: 27.247281\n",
      "Laptime Loss: 65.256628\n",
      "Position Loss: 2.736218\n",
      "Historical Loss: 0.250711\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 17\n",
      "Average Loss: 27.105538\n",
      "Laptime Loss: 64.905517\n",
      "Position Loss: 2.733745\n",
      "Historical Loss: 0.249166\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 18\n",
      "Average Loss: 27.047853\n",
      "Laptime Loss: 64.821901\n",
      "Position Loss: 2.671941\n",
      "Historical Loss: 0.251576\n",
      "--------------------\n",
      "Epoch 19\n",
      "Average Loss: 27.095710\n",
      "Laptime Loss: 64.949394\n",
      "Position Loss: 2.666996\n",
      "Historical Loss: 0.245766\n",
      "--------------------\n",
      "Epoch 20\n",
      "Average Loss: 27.057463\n",
      "Laptime Loss: 64.879178\n",
      "Position Loss: 2.640297\n",
      "Historical Loss: 0.248362\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 21\n",
      "Average Loss: 27.020142\n",
      "Laptime Loss: 64.774428\n",
      "Position Loss: 2.651174\n",
      "Historical Loss: 0.249506\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 22\n",
      "Average Loss: 27.002073\n",
      "Laptime Loss: 64.745756\n",
      "Position Loss: 2.638319\n",
      "Historical Loss: 0.242213\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 23\n",
      "Average Loss: 27.001778\n",
      "Laptime Loss: 64.740784\n",
      "Position Loss: 2.639802\n",
      "Historical Loss: 0.247713\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 24\n",
      "Average Loss: 26.979590\n",
      "Laptime Loss: 64.617081\n",
      "Position Loss: 2.709024\n",
      "Historical Loss: 0.245735\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 25\n",
      "Average Loss: 26.791072\n",
      "Laptime Loss: 64.324232\n",
      "Position Loss: 2.530532\n",
      "Historical Loss: 0.245828\n",
      "--------------------\n",
      "Epoch 26\n",
      "Average Loss: 26.962026\n",
      "Laptime Loss: 64.573452\n",
      "Position Loss: 2.703090\n",
      "Historical Loss: 0.257046\n",
      "--------------------\n",
      "Epoch 27\n",
      "Average Loss: 26.813261\n",
      "Laptime Loss: 64.409140\n",
      "Position Loss: 2.500371\n",
      "Historical Loss: 0.247281\n",
      "--------------------\n",
      "Epoch 28\n",
      "Average Loss: 26.882433\n",
      "Laptime Loss: 64.474097\n",
      "Position Loss: 2.605192\n",
      "Historical Loss: 0.253585\n",
      "--------------------\n",
      "Epoch 29\n",
      "Average Loss: 26.815462\n",
      "Laptime Loss: 64.400025\n",
      "Position Loss: 2.517676\n",
      "Historical Loss: 0.241904\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 30\n",
      "Average Loss: 26.676579\n",
      "Laptime Loss: 63.999310\n",
      "Position Loss: 2.570087\n",
      "Historical Loss: 0.244098\n",
      "--------------------\n",
      "Epoch 31\n",
      "Average Loss: 26.685472\n",
      "Laptime Loss: 63.934106\n",
      "Position Loss: 2.654635\n",
      "Historical Loss: 0.249876\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 32\n",
      "Average Loss: 26.589925\n",
      "Laptime Loss: 63.775027\n",
      "Position Loss: 2.576020\n",
      "Historical Loss: 0.247528\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 33\n",
      "Average Loss: 26.553724\n",
      "Laptime Loss: 63.727123\n",
      "Position Loss: 2.534487\n",
      "Historical Loss: 0.245396\n",
      "--------------------\n",
      "Epoch 34\n",
      "Average Loss: 26.595802\n",
      "Laptime Loss: 63.790909\n",
      "Position Loss: 2.576514\n",
      "Historical Loss: 0.244159\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 35\n",
      "Average Loss: 26.502436\n",
      "Laptime Loss: 63.601994\n",
      "Position Loss: 2.530037\n",
      "Historical Loss: 0.248115\n",
      "--------------------\n",
      "Epoch 36\n",
      "Average Loss: 26.542668\n",
      "Laptime Loss: 63.672338\n",
      "Position Loss: 2.561187\n",
      "Historical Loss: 0.246292\n",
      "--------------------\n",
      "Epoch 37\n",
      "Average Loss: 26.575531\n",
      "Laptime Loss: 63.724625\n",
      "Position Loss: 2.586403\n",
      "Historical Loss: 0.255593\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 38\n",
      "Average Loss: 26.499972\n",
      "Laptime Loss: 63.574279\n",
      "Position Loss: 2.552287\n",
      "Historical Loss: 0.246724\n",
      "--------------------\n",
      "Epoch 39\n",
      "Average Loss: 26.555009\n",
      "Laptime Loss: 63.663262\n",
      "Position Loss: 2.601236\n",
      "Historical Loss: 0.246045\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 40\n",
      "Average Loss: 26.452443\n",
      "Laptime Loss: 63.366583\n",
      "Position Loss: 2.641286\n",
      "Historical Loss: 0.246477\n",
      "--------------------\n",
      "Epoch 41\n",
      "Average Loss: 26.455032\n",
      "Laptime Loss: 63.446386\n",
      "Position Loss: 2.570087\n",
      "Historical Loss: 0.242213\n",
      "--------------------\n",
      "Epoch 42\n",
      "Average Loss: 26.457621\n",
      "Laptime Loss: 63.459503\n",
      "Position Loss: 2.561681\n",
      "Historical Loss: 0.245735\n",
      "--------------------\n",
      "Epoch 43\n",
      "Average Loss: 26.462762\n",
      "Laptime Loss: 63.417426\n",
      "Position Loss: 2.614092\n",
      "Historical Loss: 0.250773\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 44\n",
      "Average Loss: 26.388106\n",
      "Laptime Loss: 63.345821\n",
      "Position Loss: 2.499876\n",
      "Historical Loss: 0.249135\n",
      "--------------------\n",
      "Epoch 45\n",
      "Average Loss: 26.506509\n",
      "Laptime Loss: 63.416365\n",
      "Position Loss: 2.728801\n",
      "Historical Loss: 0.242213\n",
      "--------------------\n",
      "Epoch 46\n",
      "Average Loss: 26.450896\n",
      "Laptime Loss: 63.404619\n",
      "Position Loss: 2.601731\n",
      "Historical Loss: 0.241780\n",
      "--------------------\n",
      "Epoch 47\n",
      "Average Loss: 26.422936\n",
      "Laptime Loss: 63.345952\n",
      "Position Loss: 2.589864\n",
      "Historical Loss: 0.243047\n",
      "--------------------\n",
      "Epoch 48\n",
      "Average Loss: 26.414507\n",
      "Laptime Loss: 63.331538\n",
      "Position Loss: 2.583931\n",
      "Historical Loss: 0.241595\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 49\n",
      "Average Loss: 26.386444\n",
      "Laptime Loss: 63.375920\n",
      "Position Loss: 2.466749\n",
      "Historical Loss: 0.246879\n",
      "--------------------\n",
      "Epoch 50\n",
      "Average Loss: 26.451311\n",
      "Laptime Loss: 63.439726\n",
      "Position Loss: 2.565142\n",
      "Historical Loss: 0.246817\n",
      "--------------------\n",
      "Epoch 51\n",
      "Average Loss: 26.426544\n",
      "Laptime Loss: 63.394077\n",
      "Position Loss: 2.548826\n",
      "Historical Loss: 0.246910\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 52\n",
      "Average Loss: 26.361565\n",
      "Laptime Loss: 63.284334\n",
      "Position Loss: 2.494932\n",
      "Historical Loss: 0.249289\n",
      "--------------------\n",
      "Epoch 53\n",
      "Average Loss: 26.439702\n",
      "Laptime Loss: 63.420591\n",
      "Position Loss: 2.555253\n",
      "Historical Loss: 0.246817\n",
      "--------------------\n",
      "Epoch 54\n",
      "Average Loss: 26.368982\n",
      "Laptime Loss: 63.251765\n",
      "Position Loss: 2.544870\n",
      "Historical Loss: 0.251638\n",
      "--------------------\n",
      "Epoch 55\n",
      "Average Loss: 26.402294\n",
      "Laptime Loss: 63.328817\n",
      "Position Loss: 2.554265\n",
      "Historical Loss: 0.245303\n",
      "--------------------\n",
      "Epoch 56\n",
      "Average Loss: 26.449364\n",
      "Laptime Loss: 63.399321\n",
      "Position Loss: 2.598270\n",
      "Historical Loss: 0.251638\n",
      "--------------------\n",
      "Epoch 57\n",
      "Average Loss: 26.404620\n",
      "Laptime Loss: 63.381386\n",
      "Position Loss: 2.507293\n",
      "Historical Loss: 0.245735\n",
      "--------------------\n",
      "Epoch 58\n",
      "Average Loss: 26.444932\n",
      "Laptime Loss: 63.411710\n",
      "Position Loss: 2.577503\n",
      "Historical Loss: 0.246230\n",
      "--------------------\n",
      "Epoch 59\n",
      "Average Loss: 26.416132\n",
      "Laptime Loss: 63.427258\n",
      "Position Loss: 2.487021\n",
      "Historical Loss: 0.252101\n",
      "--------------------\n",
      "Epoch 60\n",
      "Average Loss: 26.422759\n",
      "Laptime Loss: 63.390411\n",
      "Position Loss: 2.543387\n",
      "Historical Loss: 0.246199\n",
      "--------------------\n",
      "Epoch 61\n",
      "Average Loss: 26.403339\n",
      "Laptime Loss: 63.429683\n",
      "Position Loss: 2.455871\n",
      "Historical Loss: 0.245581\n",
      "--------------------\n",
      "Epoch 62\n",
      "Average Loss: 26.450062\n",
      "Laptime Loss: 63.394729\n",
      "Position Loss: 2.607664\n",
      "Historical Loss: 0.245519\n",
      "--------------------\n",
      "Epoch 63\n",
      "Average Loss: 26.447877\n",
      "Laptime Loss: 63.407793\n",
      "Position Loss: 2.589864\n",
      "Historical Loss: 0.244067\n",
      "--------------------\n",
      "Epoch 64\n",
      "Average Loss: 26.443576\n",
      "Laptime Loss: 63.483583\n",
      "Position Loss: 2.501360\n",
      "Historical Loss: 0.247991\n",
      "--------------------\n",
      "Epoch 65\n",
      "Average Loss: 26.428091\n",
      "Laptime Loss: 63.448872\n",
      "Position Loss: 2.497899\n",
      "Historical Loss: 0.246910\n",
      "--------------------\n",
      "Epoch 66\n",
      "Average Loss: 26.422303\n",
      "Laptime Loss: 63.397197\n",
      "Position Loss: 2.535970\n",
      "Historical Loss: 0.245179\n",
      "Early stopping triggered at epoch 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54359096/ipykernel_857652/2456267903.py:670: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('./2024/best_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 65.20688824699475\n",
      "Outputs\n",
      "O: tensor([[8.0321, 8.4595, 8.2922, 8.1697, 8.1045, 8.0666, 8.0425, 8.0260, 8.0141,\n",
      "         8.0049, 7.9976, 9.0477, 9.7121, 8.4076, 8.1870, 8.0743, 8.0157, 7.9835,\n",
      "         7.9652, 7.9548, 7.9492, 7.9466, 7.9459, 7.9462, 7.9469, 7.9477, 7.9483,\n",
      "         7.9484, 7.9480, 7.9470, 7.9454, 7.9431, 8.9803, 9.5572, 8.1869, 8.0039,\n",
      "         7.9114, 7.8651, 7.8422, 7.8324, 7.8307, 7.8342, 7.8412, 7.8503, 7.8607,\n",
      "         7.8715, 7.8822, 7.8923, 7.9014, 7.9093, 7.9158, 7.9210, 7.9247, 7.9270,\n",
      "         7.9279, 7.9276, 7.9261, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[7.9774, 8.3141, 8.1428, 8.0308, 7.9777, 7.9526, 7.9416, 7.9381, 7.9388,\n",
      "         7.9419, 7.9463, 7.9512, 7.9560, 7.9604, 8.9844, 9.6439, 8.1986, 7.9840,\n",
      "         7.8828, 7.8362, 7.8161, 7.8100, 7.8119, 7.8187, 7.8286, 7.8403, 7.8528,\n",
      "         7.8655, 7.8778, 7.8893, 7.8997, 7.9087, 7.9163, 7.9224, 7.9271, 7.9303,\n",
      "         7.9321, 7.9327, 7.9321, 7.9305, 8.9508, 9.4574, 7.9924, 7.8123, 7.7283,\n",
      "         7.6887, 7.6712, 7.6664, 7.6697, 7.6786, 7.6917, 7.7080, 7.7263, 7.7461,\n",
      "         7.7664, 7.7867, 7.8064, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[7.4516, 7.8662, 7.7385, 7.6417, 7.5972, 7.5787, 7.5733, 7.5749, 7.5803,\n",
      "         7.5876, 7.5958, 8.8152, 9.2210, 7.7914, 7.5861, 7.4935, 7.4542, 7.4405,\n",
      "         7.4403, 7.4476, 7.4593, 7.4735, 7.4891, 7.5050, 7.5207, 7.5357, 7.5495,\n",
      "         7.5620, 7.5730, 7.5824, 7.5903, 7.5967, 7.6017, 7.6054, 7.6078, 8.7916,\n",
      "         9.1511, 7.6774, 7.5128, 7.4416, 7.4134, 7.4068, 7.4122, 7.4250, 7.4427,\n",
      "         7.4638, 7.4871, 7.5116, 7.5365, 7.5610, 7.5848, 7.6071, 7.6278, 7.6465,\n",
      "         7.6631, 7.6775, 7.6898, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[8.1050, 8.4640, 8.2882, 8.1742, 8.1196, 8.0935, 8.0817, 8.0777, 8.0781,\n",
      "         8.0811, 8.0855, 8.0906, 9.0468, 9.7698, 8.3790, 8.1565, 8.0507, 8.0016,\n",
      "         7.9797, 7.9721, 7.9727, 7.9782, 7.9869, 7.9975, 8.0090, 8.0209, 8.0326,\n",
      "         8.0436, 8.0538, 8.0628, 8.0707, 8.0773, 8.0827, 8.0868, 9.0288, 9.6052,\n",
      "         8.2349, 8.0547, 7.9707, 7.9329, 7.9178, 7.9151, 7.9200, 7.9300, 7.9436,\n",
      "         7.9596, 7.9771, 7.9955, 8.0141, 8.0323, 8.0497, 8.0658, 8.0806, 8.0937,\n",
      "         8.1051, 8.1147, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[ 8.4967,  8.8036,  8.5936,  8.4804,  8.4327,  8.4148,  8.4112,  8.4146,\n",
      "          8.4218,  8.4311,  8.4414,  9.1680, 10.0963,  8.7568,  8.5163,  8.4091,\n",
      "          8.3638,  8.3467,  8.3434,  8.3475,  8.3560,  8.3672,  8.3800,  8.3938,\n",
      "          8.4080,  8.4220,  8.4355,  8.4482,  8.4599,  9.1895,  9.9903,  8.6487,\n",
      "          8.4386,  8.3444,  8.3033,  8.2867,  8.2825,  8.2854,  8.2928,  8.3034,\n",
      "          8.3164,  8.3311,  8.3470,  8.3635,  8.3802,  8.3966,  8.4123,  8.4271,\n",
      "          8.4407,  8.4529,  8.4635,  8.4725,  8.4798,  8.4855,  8.4897,  8.4924,\n",
      "          1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,\n",
      "          1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,\n",
      "          1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,\n",
      "          1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[8.0828, 8.4110, 8.2741, 8.1774, 8.1318, 8.1108, 8.1019, 8.0995, 8.1005,\n",
      "         8.1034, 9.0594, 9.5743, 8.3553, 8.1610, 8.0696, 8.0286, 8.0115, 8.0067,\n",
      "         8.0087, 8.0146, 8.0227, 8.0321, 8.0420, 8.0517, 8.0609, 8.0694, 8.0768,\n",
      "         8.0832, 8.0883, 8.0924, 8.0953, 9.0218, 9.4702, 8.1770, 8.0142, 7.9388,\n",
      "         7.9059, 7.8940, 7.8935, 7.9000, 7.9111, 7.9251, 7.9412, 7.9584, 7.9759,\n",
      "         7.9933, 8.0101, 8.0257, 8.0400, 8.0528, 8.0639, 8.0732, 8.0808, 8.0867,\n",
      "         8.0909, 8.0936, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[7.3421, 7.8814, 7.7764, 7.6673, 7.6069, 7.5716, 7.5493, 7.5341, 7.5230,\n",
      "         7.5142, 7.5069, 7.5004, 7.4942, 7.4882, 7.4821, 7.4759, 8.7340, 9.1889,\n",
      "         7.7805, 7.5846, 7.4903, 7.4452, 7.4241, 7.4155, 7.4138, 7.4161, 7.4207,\n",
      "         7.4263, 7.4323, 7.4380, 7.4432, 7.4474, 7.4507, 7.4528, 7.4539, 7.4539,\n",
      "         8.7608, 9.0644, 7.6753, 7.5231, 7.4541, 7.4260, 7.4184, 7.4220, 7.4321,\n",
      "         7.4463, 7.4627, 7.4802, 7.4980, 7.5152, 7.5315, 7.5464, 7.5595, 7.5709,\n",
      "         7.5804, 7.5879, 7.5937, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[7.4665, 7.9098, 7.8055, 7.7150, 7.6726, 7.6542, 7.6476, 7.6471, 7.6498,\n",
      "         7.6540, 7.6588, 7.6637, 7.6683, 8.8352, 9.1836, 7.8250, 7.6390, 7.5553,\n",
      "         7.5209, 7.5098, 7.5106, 7.5179, 7.5287, 7.5415, 7.5550, 7.5685, 7.5815,\n",
      "         7.5935, 7.6043, 7.6138, 7.6219, 7.6285, 7.6337, 7.6375, 8.8137, 9.0723,\n",
      "         7.6570, 7.5001, 7.4322, 7.4061, 7.4009, 7.4072, 7.4205, 7.4381, 7.4586,\n",
      "         7.4806, 7.5034, 7.5262, 7.5482, 7.5690, 7.5883, 7.6057, 7.6210, 7.6343,\n",
      "         7.6455, 7.6545, 7.6616, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[8.5280, 8.7046, 8.5386, 8.4411, 8.3973, 8.3786, 8.3724, 8.3726, 8.3765,\n",
      "         9.1932, 9.8144, 8.6580, 8.4572, 8.3637, 8.3216, 8.3039, 8.2986, 8.3003,\n",
      "         8.3063, 8.3149, 8.3251, 8.3362, 8.3475, 8.3587, 8.3692, 8.3789, 8.3876,\n",
      "         8.3950, 8.4013, 9.1806, 9.7623, 8.5195, 8.3475, 8.2670, 8.2302, 8.2144,\n",
      "         8.2100, 8.2125, 8.2195, 8.2300, 8.2428, 8.2574, 8.2730, 8.2891, 8.3051,\n",
      "         8.3207, 8.3355, 8.3490, 8.3613, 8.3720, 8.3810, 8.3884, 8.3941, 8.3983,\n",
      "         8.4008, 8.4020, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[7.9757, 8.3525, 8.2109, 8.1102, 8.0629, 8.0413, 8.0323, 8.0297, 9.0280,\n",
      "         9.5397, 8.3172, 8.1078, 8.0104, 7.9669, 7.9488, 7.9433, 7.9446, 7.9497,\n",
      "         7.9569, 7.9652, 7.9738, 7.9821, 7.9899, 7.9969, 8.0029, 8.0079, 8.9674,\n",
      "         9.4720, 8.1512, 7.9741, 7.8929, 7.8582, 7.8461, 7.8459, 7.8526, 7.8635,\n",
      "         7.8770, 7.8921, 7.9078, 7.9236, 7.9389, 7.9533, 7.9665, 7.9782, 7.9883,\n",
      "         7.9968, 8.0036, 8.0088, 8.0125, 8.0147, 8.0156, 8.0152, 8.0136, 8.0111,\n",
      "         8.0076, 8.0033, 7.9982, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[8.5696, 8.7299, 8.5606, 8.4627, 8.4192, 8.4010, 8.3952, 8.3959, 8.4001,\n",
      "         8.4063, 8.4135, 9.1966, 9.8672, 8.6668, 8.4679, 8.3757, 8.3348, 8.3180,\n",
      "         8.3134, 8.3158, 8.3223, 8.3315, 8.3423, 8.3539, 8.3659, 8.3776, 8.3888,\n",
      "         8.3991, 8.4083, 8.4164, 9.1905, 9.7678, 8.5403, 8.3677, 8.2875, 8.2511,\n",
      "         8.2356, 8.2314, 8.2339, 8.2410, 8.2513, 8.2641, 9.1204, 9.4739, 8.5113,\n",
      "         8.3555, 8.2836, 8.2504, 8.2358, 8.2312, 8.2328, 8.2386, 8.2477, 8.2593,\n",
      "         8.2728, 8.2876, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[7.6943, 8.2301, 8.0810, 7.9722, 7.9232, 7.9024, 7.8950, 7.8940, 7.8961,\n",
      "         7.8998, 8.9690, 9.5707, 8.1860, 7.9568, 7.8547, 7.8125, 7.7978, 7.7963,\n",
      "         7.8014, 7.8098, 7.8198, 7.8304, 7.8409, 7.8508, 7.8599, 7.8679, 7.8747,\n",
      "         7.8803, 7.8848, 7.8881, 7.8903, 7.8915, 7.8918, 8.9312, 9.4559, 7.9868,\n",
      "         7.8022, 7.7223, 7.6921, 7.6855, 7.6911, 7.7034, 7.7197, 7.7384, 7.7583,\n",
      "         7.7785, 7.7984, 7.8175, 7.8352, 7.8515, 7.8659, 7.8785, 7.8891, 7.8979,\n",
      "         7.9047, 7.9098, 7.9133, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[7.9995, 8.4208, 8.2632, 8.1577, 8.1101, 8.0897, 8.0823, 8.0815, 8.0840,\n",
      "         8.0883, 8.0932, 8.0983, 9.0329, 9.6868, 8.3460, 8.1334, 8.0365, 7.9952,\n",
      "         7.9798, 7.9774, 7.9817, 7.9899, 8.0002, 8.0115, 8.0230, 8.0343, 8.0448,\n",
      "         8.0544, 8.0629, 8.0702, 8.0761, 8.0808, 9.0176, 9.5439, 8.1775, 7.9975,\n",
      "         7.9165, 7.8828, 7.8719, 7.8729, 7.8809, 7.8933, 7.9085, 7.9255, 7.9434,\n",
      "         7.9615, 7.9792, 7.9961, 8.0118, 8.0260, 8.0386, 8.0493, 8.0582, 8.0653,\n",
      "         8.0706, 8.0743, 8.0762, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[7.7797, 8.2915, 8.1210, 8.0070, 7.9568, 7.9365, 7.9302, 7.9307, 7.9346,\n",
      "         7.9401, 8.9696, 9.6766, 8.2411, 8.0004, 7.8933, 7.8492, 7.8339, 7.8325,\n",
      "         7.8383, 7.8477, 7.8590, 7.8711, 7.8832, 7.8949, 7.9057, 7.9154, 7.9240,\n",
      "         7.9312, 7.9371, 7.9418, 8.9546, 9.5599, 8.0615, 7.8604, 7.7726, 7.7381,\n",
      "         7.7289, 7.7325, 7.7434, 7.7586, 7.7764, 7.7956, 7.8154, 7.8351, 7.8542,\n",
      "         7.8721, 7.8886, 7.9035, 7.9166, 7.9278, 7.9371, 7.9446, 7.9504, 7.9544,\n",
      "         7.9570, 7.9581, 7.9580, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[8.2521, 8.5634, 8.3895, 8.2773, 8.2254, 8.2015, 8.1909, 8.1869, 8.1864,\n",
      "         8.1875, 8.1894, 8.1915, 8.1934, 8.1949, 9.0573, 9.8108, 8.4527, 8.2363,\n",
      "         8.1362, 8.0922, 8.0743, 8.0693, 8.0710, 8.0766, 8.0841, 8.0927, 8.1015,\n",
      "         8.1100, 8.1179, 8.1248, 8.1306, 8.1352, 8.1385, 8.1406, 8.1414, 9.0187,\n",
      "         9.5995, 8.2451, 8.0622, 7.9781, 7.9411, 7.9267, 7.9241, 7.9283, 7.9367,\n",
      "         7.9479, 7.9607, 7.9743, 7.9881, 8.0014, 8.0139, 8.0251, 8.0347, 8.0425,\n",
      "         8.0484, 8.0523, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[8.1932, 8.6023, 8.4051, 8.2814, 8.2250, 8.1995, 8.1885, 8.1848, 8.1846,\n",
      "         8.1863, 8.1889, 8.1916, 8.1943, 9.0529, 9.9469, 8.5114, 8.2680, 8.1565,\n",
      "         8.1081, 8.0886, 8.0832, 8.0850, 8.0907, 8.0986, 8.1075, 8.1166, 8.1255,\n",
      "         8.1338, 8.1412, 8.1475, 8.1527, 8.1568, 8.1596, 9.0294, 9.7500, 8.3216,\n",
      "         8.1166, 8.0236, 7.9839, 7.9693, 7.9676, 7.9731, 7.9829, 7.9955, 8.0096,\n",
      "         8.0246, 8.0397, 8.0544, 8.0682, 8.0808, 8.0920, 8.1015, 8.1093, 8.1154,\n",
      "         8.1196, 8.1222, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[8.4909, 8.7088, 8.5261, 8.4214, 8.3761, 8.3579, 8.3524, 8.3531, 9.1617,\n",
      "         9.8689, 8.6629, 8.4413, 8.3410, 8.2978, 8.2808, 8.2765, 8.2788, 8.2847,\n",
      "         8.2927, 8.3016, 8.3108, 8.3198, 8.3282, 8.3356, 8.3420, 8.3471, 8.3509,\n",
      "         9.1217, 9.7898, 8.4884, 8.2988, 8.2120, 8.1739, 8.1586, 8.1549, 8.1576,\n",
      "         8.1644, 8.1738, 8.1849, 8.1969, 8.2093, 8.2214, 8.2329, 8.2433, 8.2524,\n",
      "         8.2600, 8.2658, 8.2698, 8.2719, 8.2722, 8.2706, 8.2672, 8.2622, 8.2555,\n",
      "         8.2473, 8.2377, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[8.1665, 8.5575, 8.3917, 8.2831, 8.2335, 8.2111, 8.2016, 8.1985, 8.1986,\n",
      "         8.2002, 8.2024, 9.1025, 9.7823, 8.4856, 8.2691, 8.1693, 8.1257, 8.1081,\n",
      "         8.1031, 8.1046, 8.1096, 8.1164, 8.1240, 8.1317, 8.1389, 8.1455, 8.1510,\n",
      "         8.1554, 8.1587, 8.1607, 8.1616, 8.1612, 8.1598, 9.0408, 9.6146, 8.2697,\n",
      "         8.0907, 8.0086, 7.9733, 7.9604, 7.9591, 7.9643, 7.9736, 7.9853, 7.9984,\n",
      "         8.0121, 8.0257, 8.0387, 8.0507, 8.0612, 8.0702, 8.0773, 8.0825, 8.0858,\n",
      "         8.0871, 8.0866, 8.0842, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745,\n",
      "         1.5745, 1.5745, 1.5745, 1.5745, 1.5745, 1.5745]], device='cuda:0')\n",
      "O: tensor([[ 8.7370,  8.8714,  8.6579,  8.5413,  8.4903,  8.4687,  8.4607,  8.4594,\n",
      "          8.4614,  9.1972, 10.0881,  8.8091,  8.5675,  8.4574,  8.4090,  8.3887,\n",
      "          8.3818,  8.3819,  8.3857,  8.3916,  8.3985,  8.4059,  8.4131,  8.4197,\n",
      "          8.4256,  8.4304,  8.4341,  9.1465,  9.9609,  8.6338,  8.4251,  8.3290,\n",
      "          8.2858,  8.2671,  8.2605,  8.2606,  8.2648,  8.2715,  8.2799,  9.0643,\n",
      "          9.6322,  8.5507,  8.3617,  8.2747,  8.2344,  8.2156,  8.2077,  8.2058,\n",
      "          8.2076,  8.2120,  8.2181,  8.2253,  8.2330,  8.2406,  8.2477,  1.5745,\n",
      "          1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,\n",
      "          1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,\n",
      "          1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,\n",
      "          1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745,  1.5745]],\n",
      "       device='cuda:0')\n",
      "Mask\n",
      "[8.0320835 8.459503  8.292208  8.169661  8.104483  8.066553  8.042479\n",
      " 8.026024  8.014078  8.004942  7.9975996 9.047731  9.712055  8.407637\n",
      " 8.18702   8.074284  8.015695  7.9834843 7.9651623 7.9547653 7.9491835\n",
      " 7.9466066 7.9458685 7.946166  7.946925  7.947735  7.9483075 7.948449\n",
      " 7.948042  7.9470234 7.9453726 7.9431    8.980345  9.557241  8.186949\n",
      " 8.003895  7.9113865 7.86506   7.8421874 7.832424  7.8306956 7.8342\n",
      " 7.84116   7.8503103 7.8606825 7.871509  7.882189  7.892253  7.90136\n",
      " 7.909272  7.9158387 7.9209843 7.9246907 7.926986  7.927928  7.927594\n",
      " 7.9260745 1.5744618 1.5744618 1.5744618 1.5744618 1.5744618 1.5744618\n",
      " 1.5744618 1.5744618 1.5744618 1.5744618 1.5744618 1.5744618 1.5744618\n",
      " 1.5744618 1.5744618 1.5744618 1.5744618 1.5744618 1.5744618 1.5744618\n",
      " 1.5744618 1.5744618 1.5744618 1.5744618 1.5744618 1.5744618 1.5744618\n",
      " 1.5744618 1.5744618 1.5744618]\n",
      "END Mask\n",
      "[[8.0320835 8.459503  8.292208  ... 1.5744618 1.5744618 1.5744618]\n",
      " [7.977352  8.314132  8.142781  ... 1.5744618 1.5744618 1.5744618]\n",
      " [7.4515576 7.8662214 7.738507  ... 1.5744618 1.5744618 1.5744618]\n",
      " ...\n",
      " [8.490936  8.708757  8.526054  ... 1.5744618 1.5744618 1.5744618]\n",
      " [8.166544  8.557472  8.391705  ... 1.5744618 1.5744618 1.5744618]\n",
      " [8.736966  8.871429  8.657885  ... 1.5744618 1.5744618 1.5744618]]\n",
      "1 Name: 0    hamilton\n",
      "Name: driverRef, dtype: object\n",
      "4 Name: 3    alonso\n",
      "Name: driverRef, dtype: object\n",
      "815 Name: 814    perez\n",
      "Name: driverRef, dtype: object\n",
      "817 Name: 816    ricciardo\n",
      "Name: driverRef, dtype: object\n",
      "822 Name: 821    bottas\n",
      "Name: driverRef, dtype: object\n",
      "825 Name: 824    kevin_magnussen\n",
      "Name: driverRef, dtype: object\n",
      "830 Name: 829    max_verstappen\n",
      "Name: driverRef, dtype: object\n",
      "832 Name: 831    sainz\n",
      "Name: driverRef, dtype: object\n",
      "839 Name: 838    ocon\n",
      "Name: driverRef, dtype: object\n",
      "840 Name: 839    stroll\n",
      "Name: driverRef, dtype: object\n",
      "842 Name: 452    gasly\n",
      "Name: driverRef, dtype: object\n",
      "844 Name: 842    leclerc\n",
      "Name: driverRef, dtype: object\n",
      "846 Name: 844    norris\n",
      "Name: driverRef, dtype: object\n",
      "847 Name: 845    russell\n",
      "Name: driverRef, dtype: object\n",
      "848 Name: 846    albon\n",
      "Name: driverRef, dtype: object\n",
      "852 Name: 850    tsunoda\n",
      "Name: driverRef, dtype: object\n",
      "855 Name: 853    zhou\n",
      "Name: driverRef, dtype: object\n",
      "857 Name: 855    piastri\n",
      "Name: driverRef, dtype: object\n",
      "858 Name: 856    sargeant\n",
      "Name: driverRef, dtype: object\n",
      "\n",
      "\n",
      "Training model: model_b5_e150_lr0.0001_h256_l1_d0.2_w0.6_0.3_0.1.pth\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 1\n",
      "Average Loss: 61.914706\n",
      "Laptime Loss: 101.736120\n",
      "Position Loss: 2.829666\n",
      "Historical Loss: 0.241316\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 2\n",
      "Average Loss: 45.303801\n",
      "Laptime Loss: 74.160449\n",
      "Position Loss: 2.609147\n",
      "Historical Loss: 0.247868\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 3\n",
      "Average Loss: 44.994021\n",
      "Laptime Loss: 73.606535\n",
      "Position Loss: 2.683807\n",
      "Historical Loss: 0.249567\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 4\n",
      "Average Loss: 44.024390\n",
      "Laptime Loss: 72.031331\n",
      "Position Loss: 2.602225\n",
      "Historical Loss: 0.249227\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 5\n",
      "Average Loss: 43.546171\n",
      "Laptime Loss: 71.174451\n",
      "Position Loss: 2.721879\n",
      "Historical Loss: 0.249351\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 6\n",
      "Average Loss: 42.891546\n",
      "Laptime Loss: 70.085572\n",
      "Position Loss: 2.718418\n",
      "Historical Loss: 0.246755\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 7\n",
      "Average Loss: 42.601651\n",
      "Laptime Loss: 69.608746\n",
      "Position Loss: 2.704574\n",
      "Historical Loss: 0.250309\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 8\n",
      "Average Loss: 41.873386\n",
      "Laptime Loss: 68.443502\n",
      "Position Loss: 2.611125\n",
      "Historical Loss: 0.239462\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 9\n",
      "Average Loss: 41.870799\n",
      "Laptime Loss: 68.388809\n",
      "Position Loss: 2.707540\n",
      "Historical Loss: 0.252503\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 10\n",
      "Average Loss: 41.020402\n",
      "Laptime Loss: 66.962235\n",
      "Position Loss: 2.726329\n",
      "Historical Loss: 0.251607\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 11\n",
      "Average Loss: 40.797228\n",
      "Laptime Loss: 66.584109\n",
      "Position Loss: 2.740173\n",
      "Historical Loss: 0.247095\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 12\n",
      "Average Loss: 40.517984\n",
      "Laptime Loss: 66.097354\n",
      "Position Loss: 2.781706\n",
      "Historical Loss: 0.250587\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 13\n",
      "Average Loss: 40.290864\n",
      "Laptime Loss: 65.743126\n",
      "Position Loss: 2.733251\n",
      "Historical Loss: 0.250124\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 14\n",
      "Average Loss: 39.985581\n",
      "Laptime Loss: 65.212076\n",
      "Position Loss: 2.779728\n",
      "Historical Loss: 0.244159\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 15\n",
      "Average Loss: 39.812635\n",
      "Laptime Loss: 64.941147\n",
      "Position Loss: 2.742151\n",
      "Historical Loss: 0.252998\n",
      "--------------------\n",
      "Epoch 16\n",
      "Average Loss: 39.978929\n",
      "Laptime Loss: 65.286387\n",
      "Position Loss: 2.608158\n",
      "Historical Loss: 0.246477\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 17\n",
      "Average Loss: 39.670771\n",
      "Laptime Loss: 64.739291\n",
      "Position Loss: 2.675402\n",
      "Historical Loss: 0.245735\n",
      "--------------------\n",
      "Epoch 18\n",
      "Average Loss: 39.682786\n",
      "Laptime Loss: 64.690137\n",
      "Position Loss: 2.814833\n",
      "Historical Loss: 0.242522\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 19\n",
      "Average Loss: 39.668781\n",
      "Laptime Loss: 64.708947\n",
      "Position Loss: 2.729790\n",
      "Historical Loss: 0.244747\n",
      "--------------------\n",
      "Epoch 20\n",
      "Average Loss: 39.774917\n",
      "Laptime Loss: 64.970235\n",
      "Position Loss: 2.560692\n",
      "Historical Loss: 0.245674\n",
      "--------------------\n",
      "Epoch 21\n",
      "Average Loss: 39.675505\n",
      "Laptime Loss: 64.728714\n",
      "Position Loss: 2.713473\n",
      "Historical Loss: 0.242336\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 22\n",
      "Average Loss: 39.536758\n",
      "Laptime Loss: 64.570378\n",
      "Position Loss: 2.566131\n",
      "Historical Loss: 0.246910\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 23\n",
      "Average Loss: 39.529750\n",
      "Laptime Loss: 64.558275\n",
      "Position Loss: 2.566131\n",
      "Historical Loss: 0.249444\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 24\n",
      "Average Loss: 39.431382\n",
      "Laptime Loss: 64.392737\n",
      "Position Loss: 2.569098\n",
      "Historical Loss: 0.250093\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 25\n",
      "Average Loss: 39.251798\n",
      "Laptime Loss: 64.091993\n",
      "Position Loss: 2.574537\n",
      "Historical Loss: 0.242398\n",
      "--------------------\n",
      "Epoch 26\n",
      "Average Loss: 39.340947\n",
      "Laptime Loss: 64.212861\n",
      "Position Loss: 2.628925\n",
      "Historical Loss: 0.245519\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 27\n",
      "Average Loss: 39.238666\n",
      "Laptime Loss: 64.083095\n",
      "Position Loss: 2.546354\n",
      "Historical Loss: 0.249011\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 28\n",
      "Average Loss: 39.200292\n",
      "Laptime Loss: 64.009544\n",
      "Position Loss: 2.567614\n",
      "Historical Loss: 0.242800\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 29\n",
      "Average Loss: 39.105872\n",
      "Laptime Loss: 63.827074\n",
      "Position Loss: 2.616564\n",
      "Historical Loss: 0.246570\n",
      "--------------------\n",
      "Epoch 30\n",
      "Average Loss: 39.112304\n",
      "Laptime Loss: 63.840075\n",
      "Position Loss: 2.611619\n",
      "Historical Loss: 0.247713\n",
      "--------------------\n",
      "Epoch 31\n",
      "Average Loss: 39.123075\n",
      "Laptime Loss: 63.861833\n",
      "Position Loss: 2.604203\n",
      "Historical Loss: 0.247126\n",
      "--------------------\n",
      "Epoch 32\n",
      "Average Loss: 39.200281\n",
      "Laptime Loss: 63.990572\n",
      "Position Loss: 2.603214\n",
      "Historical Loss: 0.249722\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 33\n",
      "Average Loss: 39.014414\n",
      "Laptime Loss: 63.676225\n",
      "Position Loss: 2.613103\n",
      "Historical Loss: 0.247466\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 34\n",
      "Average Loss: 38.990771\n",
      "Laptime Loss: 63.676736\n",
      "Position Loss: 2.534982\n",
      "Historical Loss: 0.242336\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 35\n",
      "Average Loss: 38.969453\n",
      "Laptime Loss: 63.609470\n",
      "Position Loss: 2.595303\n",
      "Historical Loss: 0.251792\n",
      "--------------------\n",
      "Epoch 36\n",
      "Average Loss: 38.986268\n",
      "Laptime Loss: 63.678538\n",
      "Position Loss: 2.516193\n",
      "Historical Loss: 0.242862\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 37\n",
      "Average Loss: 38.960333\n",
      "Laptime Loss: 63.637075\n",
      "Position Loss: 2.512237\n",
      "Historical Loss: 0.244159\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 38\n",
      "Average Loss: 38.904275\n",
      "Laptime Loss: 63.516683\n",
      "Position Loss: 2.566131\n",
      "Historical Loss: 0.244252\n",
      "--------------------\n",
      "Epoch 39\n",
      "Average Loss: 38.914368\n",
      "Laptime Loss: 63.542331\n",
      "Position Loss: 2.548331\n",
      "Historical Loss: 0.244685\n",
      "--------------------\n",
      "Epoch 40\n",
      "Average Loss: 38.909664\n",
      "Laptime Loss: 63.487478\n",
      "Position Loss: 2.642769\n",
      "Historical Loss: 0.243449\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 41\n",
      "Average Loss: 38.840312\n",
      "Laptime Loss: 63.463786\n",
      "Position Loss: 2.458838\n",
      "Historical Loss: 0.243881\n",
      "--------------------\n",
      "Epoch 42\n",
      "Average Loss: 38.962415\n",
      "Laptime Loss: 63.612835\n",
      "Position Loss: 2.568109\n",
      "Historical Loss: 0.242800\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 43\n",
      "Average Loss: 38.813641\n",
      "Laptime Loss: 63.393854\n",
      "Position Loss: 2.510260\n",
      "Historical Loss: 0.242491\n",
      "--------------------\n",
      "Epoch 44\n",
      "Average Loss: 38.842159\n",
      "Laptime Loss: 63.414881\n",
      "Position Loss: 2.561187\n",
      "Historical Loss: 0.248733\n",
      "--------------------\n",
      "Epoch 45\n",
      "Average Loss: 38.911690\n",
      "Laptime Loss: 63.512270\n",
      "Position Loss: 2.600742\n",
      "Historical Loss: 0.241038\n",
      "--------------------\n",
      "Epoch 46\n",
      "Average Loss: 38.910218\n",
      "Laptime Loss: 63.513556\n",
      "Position Loss: 2.592831\n",
      "Historical Loss: 0.242336\n",
      "--------------------\n",
      "Epoch 47\n",
      "Average Loss: 38.901285\n",
      "Laptime Loss: 63.461162\n",
      "Position Loss: 2.667491\n",
      "Historical Loss: 0.243387\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 48\n",
      "Average Loss: 38.715392\n",
      "Laptime Loss: 63.207744\n",
      "Position Loss: 2.555253\n",
      "Historical Loss: 0.241687\n",
      "--------------------\n",
      "Epoch 49\n",
      "Average Loss: 38.772151\n",
      "Laptime Loss: 63.311036\n",
      "Position Loss: 2.536959\n",
      "Historical Loss: 0.244407\n",
      "--------------------\n",
      "Epoch 50\n",
      "Average Loss: 38.758483\n",
      "Laptime Loss: 63.295456\n",
      "Position Loss: 2.521632\n",
      "Historical Loss: 0.247188\n",
      "--------------------\n",
      "Epoch 51\n",
      "Average Loss: 38.817209\n",
      "Laptime Loss: 63.351547\n",
      "Position Loss: 2.605686\n",
      "Historical Loss: 0.245735\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 52\n",
      "Average Loss: 38.670430\n",
      "Laptime Loss: 63.116438\n",
      "Position Loss: 2.586403\n",
      "Historical Loss: 0.246446\n",
      "--------------------\n",
      "Epoch 53\n",
      "Average Loss: 38.797204\n",
      "Laptime Loss: 63.287344\n",
      "Position Loss: 2.667491\n",
      "Historical Loss: 0.245488\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 54\n",
      "Average Loss: 38.613798\n",
      "Laptime Loss: 63.041355\n",
      "Position Loss: 2.549815\n",
      "Historical Loss: 0.240389\n",
      "--------------------\n",
      "Epoch 55\n",
      "Average Loss: 38.705158\n",
      "Laptime Loss: 63.224010\n",
      "Position Loss: 2.487515\n",
      "Historical Loss: 0.244963\n",
      "--------------------\n",
      "Epoch 56\n",
      "Average Loss: 38.773821\n",
      "Laptime Loss: 63.286176\n",
      "Position Loss: 2.590853\n",
      "Historical Loss: 0.248578\n",
      "--------------------\n",
      "Epoch 57\n",
      "Average Loss: 38.699500\n",
      "Laptime Loss: 63.216381\n",
      "Position Loss: 2.482077\n",
      "Historical Loss: 0.250464\n",
      "--------------------\n",
      "Epoch 58\n",
      "Average Loss: 38.741106\n",
      "Laptime Loss: 63.233784\n",
      "Position Loss: 2.587886\n",
      "Historical Loss: 0.244685\n",
      "--------------------\n",
      "Epoch 59\n",
      "Average Loss: 38.683376\n",
      "Laptime Loss: 63.132088\n",
      "Position Loss: 2.597775\n",
      "Historical Loss: 0.247899\n",
      "--------------------\n",
      "Epoch 60\n",
      "Average Loss: 38.746293\n",
      "Laptime Loss: 63.265755\n",
      "Position Loss: 2.539431\n",
      "Historical Loss: 0.250093\n",
      "--------------------\n",
      "Epoch 61\n",
      "Average Loss: 38.716341\n",
      "Laptime Loss: 63.236756\n",
      "Position Loss: 2.496910\n",
      "Historical Loss: 0.252132\n",
      "--------------------\n",
      "Epoch 62\n",
      "Average Loss: 38.629633\n",
      "Laptime Loss: 63.048816\n",
      "Position Loss: 2.587392\n",
      "Historical Loss: 0.241255\n",
      "--------------------\n",
      "Epoch 63\n",
      "Average Loss: 38.662569\n",
      "Laptime Loss: 63.068592\n",
      "Position Loss: 2.655624\n",
      "Historical Loss: 0.247250\n",
      "--------------------\n",
      "Epoch 64\n",
      "Average Loss: 38.671783\n",
      "Laptime Loss: 63.146140\n",
      "Position Loss: 2.531520\n",
      "Historical Loss: 0.246415\n",
      "--------------------\n",
      "Epoch 65\n",
      "Average Loss: 38.714968\n",
      "Laptime Loss: 63.200814\n",
      "Position Loss: 2.564153\n",
      "Historical Loss: 0.252318\n",
      "--------------------\n",
      "Epoch 66\n",
      "Average Loss: 38.740599\n",
      "Laptime Loss: 63.233700\n",
      "Position Loss: 2.584425\n",
      "Historical Loss: 0.250494\n",
      "--------------------\n",
      "Epoch 67\n",
      "Average Loss: 38.783769\n",
      "Laptime Loss: 63.333550\n",
      "Position Loss: 2.529048\n",
      "Historical Loss: 0.249227\n",
      "--------------------\n",
      "Epoch 68\n",
      "Average Loss: 38.809539\n",
      "Laptime Loss: 63.341957\n",
      "Position Loss: 2.598270\n",
      "Historical Loss: 0.248826\n",
      "Early stopping triggered at epoch 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54359096/ipykernel_857652/2456267903.py:670: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('./2024/best_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 64.75757303375464\n",
      "Outputs\n",
      "O: tensor([[ 9.1735,  9.3268,  9.0457,  8.8456,  8.7274,  8.6535,  8.6047,  8.5712,\n",
      "          8.5478,  8.5311,  8.5190,  9.2772, 10.4995,  9.3044,  8.9547,  8.7776,\n",
      "          8.6750,  8.6113,  8.5702,  8.5429,  8.5248,  8.5129,  8.5052,  8.5005,\n",
      "          8.4979,  8.4969,  8.4969,  8.4976,  8.4989,  8.5005,  8.5023,  8.5042,\n",
      "          9.2908, 10.1663,  9.0182,  8.7376,  8.6023,  8.5268,  8.4818,  8.4541,\n",
      "          8.4372,  8.4272,  8.4220,  8.4203,  8.4209,  8.4234,  8.4271,  8.4318,\n",
      "          8.4373,  8.4432,  8.4495,  8.4561,  8.4629,  8.4697,  8.4766,  8.4835,\n",
      "          8.4903,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 8.9083,  9.0967,  8.8544,  8.6830,  8.5880,  8.5326,  8.4987,  8.4776,\n",
      "          8.4646,  8.4572,  8.4535,  8.4525,  8.4534,  8.4556,  9.1817, 10.2557,\n",
      "          9.0780,  8.7715,  8.6267,  8.5486,  8.5038,  8.4776,  8.4626,  8.4549,\n",
      "          8.4520,  8.4526,  8.4555,  8.4602,  8.4662,  8.4730,  8.4805,  8.4885,\n",
      "          8.4967,  8.5052,  8.5137,  8.5223,  8.5308,  8.5392,  8.5476,  8.5557,\n",
      "          9.2943, 10.0594,  8.9237,  8.6821,  8.5749,  8.5199,  8.4902,  8.4743,\n",
      "          8.4665,  8.4640,  8.4651,  8.4687,  8.4742,  8.4811,  8.4891,  8.4980,\n",
      "          8.5075,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 8.8779,  9.0343,  8.8304,  8.6670,  8.5711,  8.5145,  8.4804,  8.4599,\n",
      "          8.4480,  8.4418,  8.4393,  9.2183, 10.2220,  9.0702,  8.7580,  8.6115,\n",
      "          8.5330,  8.4885,  8.4629,  8.4486,  8.4415,  8.4394,  8.4405,  8.4440,\n",
      "          8.4491,  8.4554,  8.4624,  8.4699,  8.4777,  8.4858,  8.4938,  8.5018,\n",
      "          8.5097,  8.5175,  8.5250,  9.3103,  9.9798,  8.8750,  8.6302,  8.5224,\n",
      "          8.4682,  8.4396,  8.4248,  8.4181,  8.4167,  8.4186,  8.4230,  8.4290,\n",
      "          8.4361,  8.4441,  8.4526,  8.4613,  8.4700,  8.4786,  8.4868,  8.4946,\n",
      "          8.5018,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.1815,  9.3038,  9.0456,  8.8608,  8.7574,  8.6973,  8.6608,  8.6379,\n",
      "          8.6236,  8.6149,  8.6101,  8.6079,  9.3150, 10.4543,  9.2846,  8.9657,\n",
      "          8.8116,  8.7270,  8.6776,  8.6479,  8.6302,  8.6202,  8.6154,  8.6141,\n",
      "          8.6154,  8.6186,  8.6230,  8.6282,  8.6342,  8.6405,  8.6471,  8.6538,\n",
      "          8.6606,  8.6673,  9.3909, 10.1755,  9.0855,  8.8333,  8.7172,  8.6565,\n",
      "          8.6229,  8.6040,  8.5938,  8.5892,  8.5883,  8.5900,  8.5935,  8.5983,\n",
      "          8.6039,  8.6101,  8.6166,  8.6231,  8.6294,  8.6355,  8.6412,  8.6464,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 8.9806,  9.1507,  8.8980,  8.7369,  8.6554,  8.6119,  8.5871,  8.5723,\n",
      "          8.5635,  8.5585,  8.5562,  9.1478, 10.2281,  9.1397,  8.8392,  8.7021,\n",
      "          8.6329,  8.5956,  8.5746,  8.5625,  8.5560,  8.5530,  8.5525,  8.5539,\n",
      "          8.5565,  8.5601,  8.5645,  8.5695,  8.5749,  9.1922, 10.0428,  9.0135,\n",
      "          8.7568,  8.6420,  8.5853,  8.5554,  8.5390,  8.5300,  8.5255,  8.5240,\n",
      "          8.5245,  8.5266,  8.5298,  8.5339,  8.5388,  8.5442,  8.5501,  8.5565,\n",
      "          8.5631,  8.5701,  8.5772,  8.5845,  8.5919,  8.5993,  8.6068,  8.6143,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.1733,  9.3822,  9.1720,  9.0118,  8.9230,  8.8709,  8.8383,  8.8169,\n",
      "          8.8025,  8.7926,  9.4692, 10.4534,  9.4028,  9.1147,  8.9755,  8.8989,\n",
      "          8.8537,  8.8256,  8.8079,  8.7967,  8.7898,  8.7859,  8.7841,  8.7837,\n",
      "          8.7843,  8.7855,  8.7870,  8.7887,  8.7903,  8.7917,  8.7927,  9.4854,\n",
      "         10.2702,  9.2363,  9.0015,  8.8918,  8.8332,  8.7995,  8.7792,  8.7667,\n",
      "          8.7589,  8.7541,  8.7512,  8.7495,  8.7485,  8.7481,  8.7483,  8.7491,\n",
      "          8.7506,  8.7529,  8.7561,  8.7600,  8.7647,  8.7698,  8.7754,  8.7813,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 8.8094,  9.0987,  8.8736,  8.6889,  8.5760,  8.5069,  8.4632,  8.4348,\n",
      "          8.4159,  8.4031,  8.3945,  8.3886,  8.3845,  8.3816,  8.3795,  8.3777,\n",
      "          9.1895, 10.3740,  9.1456,  8.8134,  8.6543,  8.5664,  8.5146,  8.4833,\n",
      "          8.4644,  8.4534,  8.4478,  8.4456,  8.4459,  8.4477,  8.4505,  8.4539,\n",
      "          8.4576,  8.4613,  8.4650,  8.4685,  9.3281, 10.0437,  8.9474,  8.6835,\n",
      "          8.5633,  8.5000,  8.4647,  8.4449,  8.4343,  8.4295,  8.4285,  8.4300,\n",
      "          8.4333,  8.4377,  8.4428,  8.4482,  8.4538,  8.4593,  8.4645,  8.4693,\n",
      "          8.4735,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 8.8581,  9.0759,  8.9036,  8.7475,  8.6537,  8.5975,  8.5628,  8.5411,\n",
      "          8.5275,  8.5192,  8.5143,  8.5117,  8.5108,  9.2489, 10.2155,  9.1093,\n",
      "          8.8174,  8.6803,  8.6067,  8.5647,  8.5400,  8.5258,  8.5182,  8.5150,\n",
      "          8.5148,  8.5168,  8.5201,  8.5244,  8.5293,  8.5346,  8.5400,  8.5455,\n",
      "          8.5508,  8.5559,  9.3205, 10.0270,  8.9705,  8.7333,  8.6274,  8.5732,\n",
      "          8.5441,  8.5285,  8.5209,  8.5183,  8.5189,  8.5215,  8.5254,  8.5301,\n",
      "          8.5350,  8.5399,  8.5447,  8.5494,  8.5541,  8.5590,  8.5643,  8.5700,\n",
      "          8.5762,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.2071,  9.3673,  9.1529,  9.0010,  8.9186,  8.8705,  8.8403,  8.8203,\n",
      "          8.8067,  9.4240, 10.3873,  9.3872,  9.1091,  8.9746,  8.9015,  8.8585,\n",
      "          8.8318,  8.8148,  8.8039,  8.7971,  8.7931,  8.7911,  8.7906,  8.7911,\n",
      "          8.7924,  8.7943,  8.7965,  8.7989,  8.8015,  9.4323, 10.2660,  9.2541,\n",
      "          9.0232,  8.9153,  8.8583,  8.8259,  8.8066,  8.7950,  8.7881,  8.7844,\n",
      "          8.7830,  8.7830,  8.7841,  8.7858,  8.7878,  8.7900,  8.7921,  8.7940,\n",
      "          8.7958,  8.7976,  8.7995,  8.8017,  8.8042,  8.8073,  8.8108,  8.8148,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.0439,  9.2652,  9.0592,  8.8973,  8.8071,  8.7545,  8.7216,  8.6999,\n",
      "          9.3841, 10.3580,  9.3090,  9.0091,  8.8645,  8.7853,  8.7385,  8.7095,\n",
      "          8.6911,  8.6793,  8.6720,  8.6678,  8.6656,  8.6650,  8.6653,  8.6664,\n",
      "          8.6679,  8.6696,  9.3685, 10.2218,  9.1608,  8.9074,  8.7902,  8.7282,\n",
      "          8.6930,  8.6723,  8.6601,  8.6532,  8.6498,  8.6487,  8.6491,  8.6504,\n",
      "          8.6522,  8.6541,  8.6559,  8.6575,  8.6589,  8.6600,  8.6613,  8.6628,\n",
      "          8.6647,  8.6671,  8.6701,  8.6735,  8.6773,  8.6814,  8.6857,  8.6900,\n",
      "          8.6942,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.2176,  9.3571,  9.1482,  9.0024,  8.9241,  8.8786,  8.8500,  8.8310,\n",
      "          8.8180,  8.8089,  8.8025,  9.4038, 10.3593,  9.3608,  9.0960,  8.9694,\n",
      "          8.9013,  8.8616,  8.8371,  8.8216,  8.8116,  8.8053,  8.8017,  8.7999,\n",
      "          8.7995,  8.8001,  8.8015,  8.8033,  8.8056,  8.8081,  9.4333, 10.2204,\n",
      "          9.2462,  9.0242,  8.9210,  8.8669,  8.8362,  8.8179,  8.8068,  8.8001,\n",
      "          8.7963,  8.7946,  9.4471,  9.9357,  9.1732,  8.9790,  8.8890,  8.8423,\n",
      "          8.8158,  8.7995,  8.7889,  8.7816,  8.7765,  8.7727,  8.7699,  8.7679,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 8.8316,  9.1130,  8.9202,  8.7580,  8.6640,  8.6099,  8.5778,  8.5582,\n",
      "          8.5461,  8.5387,  9.2346, 10.3364,  9.1838,  8.8576,  8.7074,  8.6297,\n",
      "          8.5868,  8.5622,  8.5481,  8.5405,  8.5371,  8.5365,  8.5378,  8.5404,\n",
      "          8.5439,  8.5479,  8.5523,  8.5568,  8.5614,  8.5659,  8.5702,  8.5743,\n",
      "          8.5779,  9.2800, 10.1270,  8.9998,  8.7373,  8.6227,  8.5665,  8.5377,\n",
      "          8.5231,  8.5164,  8.5145,  8.5155,  8.5185,  8.5227,  8.5275,  8.5324,\n",
      "          8.5373,  8.5420,  8.5462,  8.5501,  8.5539,  8.5577,  8.5618,  8.5662,\n",
      "          8.5712,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 8.9702,  9.2423,  9.0432,  8.8799,  8.7890,  8.7374,  8.7064,  8.6865,\n",
      "          8.6734,  8.6644,  8.6583,  8.6540,  9.2977, 10.3944,  9.2636,  8.9598,\n",
      "          8.8191,  8.7457,  8.7045,  8.6801,  8.6653,  8.6564,  8.6515,  8.6491,\n",
      "          8.6486,  8.6493,  8.6508,  8.6529,  8.6553,  8.6578,  8.6603,  8.6625,\n",
      "          9.3298, 10.1884,  9.1112,  8.8603,  8.7479,  8.6914,  8.6610,  8.6440,\n",
      "          8.6345,  8.6295,  8.6273,  8.6268,  8.6273,  8.6283,  8.6296,  8.6310,\n",
      "          8.6326,  8.6345,  8.6367,  8.6396,  8.6430,  8.6471,  8.6518,  8.6570,\n",
      "          8.6625,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 8.9062,  9.1984,  8.9809,  8.8116,  8.7190,  8.6678,  8.6382,  8.6208,\n",
      "          8.6103,  8.6042,  9.2954, 10.4619,  9.2593,  8.9254,  8.7748,  8.6989,\n",
      "          8.6580,  8.6352,  8.6226,  8.6161,  8.6134,  8.6132,  8.6146,  8.6171,\n",
      "          8.6200,  8.6232,  8.6262,  8.6289,  8.6310,  8.6325,  9.3409, 10.2492,\n",
      "          9.1134,  8.8384,  8.7176,  8.6584,  8.6273,  8.6102,  8.6006,  8.5952,\n",
      "          8.5925,  8.5914,  8.5917,  8.5932,  8.5958,  8.5995,  8.6043,  8.6100,\n",
      "          8.6164,  8.6235,  8.6310,  8.6389,  8.6469,  8.6550,  8.6631,  8.6711,\n",
      "          8.6789,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.1041,  9.3776,  9.1590,  8.9950,  8.9056,  8.8534,  8.8203,  8.7979,\n",
      "          8.7819,  8.7701,  8.7610,  8.7537,  8.7477,  8.7425,  9.3578, 10.4841,\n",
      "          9.3573,  9.0607,  8.9211,  8.8464,  8.8029,  8.7758,  8.7582,  8.7465,\n",
      "          8.7386,  8.7333,  8.7297,  8.7274,  8.7259,  8.7250,  8.7244,  8.7239,\n",
      "          8.7234,  8.7228,  8.7219,  9.3668, 10.2382,  9.1760,  8.9335,  8.8227,\n",
      "          8.7652,  8.7328,  8.7134,  8.7013,  8.6935,  8.6885,  8.6851,  8.6828,\n",
      "          8.6810,  8.6793,  8.6778,  8.6763,  8.6750,  8.6741,  8.6736,  8.6737,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.1115,  9.3719,  9.1360,  8.9614,  8.8676,  8.8143,  8.7816,  8.7602,\n",
      "          8.7458,  8.7356,  8.7284,  8.7231,  8.7191,  9.3483, 10.5676,  9.3770,\n",
      "          9.0562,  8.9076,  8.8299,  8.7858,  8.7595,  8.7431,  8.7327,  8.7262,\n",
      "          8.7222,  8.7198,  8.7183,  8.7173,  8.7164,  8.7153,  8.7139,  8.7121,\n",
      "          8.7100,  9.3721, 10.2801,  9.2033,  8.9418,  8.8218,  8.7596,  8.7243,\n",
      "          8.7025,  8.6882,  8.6785,  8.6719,  8.6676,  8.6652,  8.6646,  8.6654,\n",
      "          8.6674,  8.6704,  8.6743,  8.6787,  8.6836,  8.6888,  8.6941,  8.6995,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.1169,  9.2926,  9.0938,  8.9527,  8.8775,  8.8339,  8.8062,  8.7872,\n",
      "          9.3832, 10.3002,  9.3212,  9.0460,  8.9165,  8.8478,  8.8079,  8.7828,\n",
      "          8.7660,  8.7542,  8.7457,  8.7395,  8.7348,  8.7312,  8.7284,  8.7261,\n",
      "          8.7242,  8.7226,  8.7211,  9.3274, 10.1726,  9.1612,  8.9282,  8.8219,\n",
      "          8.7670,  8.7359,  8.7171,  8.7050,  8.6971,  8.6919,  8.6885,  8.6863,\n",
      "          8.6851,  8.6844,  8.6840,  8.6837,  8.6834,  8.6828,  8.6818,  8.6802,\n",
      "          8.6781,  8.6754,  8.6722,  8.6687,  8.6651,  8.6615,  8.6580,  8.6546,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.0366,  9.3376,  9.1417,  8.9798,  8.8912,  8.8409,  8.8097,  8.7889,\n",
      "          8.7741,  8.7632,  8.7546,  9.3637, 10.4650,  9.3674,  9.0650,  8.9241,\n",
      "          8.8500,  8.8076,  8.7815,  8.7646,  8.7533,  8.7454,  8.7398,  8.7357,\n",
      "          8.7325,  8.7299,  8.7275,  8.7250,  8.7223,  8.7192,  8.7156,  8.7115,\n",
      "          8.7070,  9.3325, 10.2149,  9.1735,  8.9311,  8.8197,  8.7624,  8.7299,\n",
      "          8.7098,  8.6964,  8.6869,  8.6799,  8.6747,  8.6710,  8.6686,  8.6673,\n",
      "          8.6672,  8.6679,  8.6694,  8.6715,  8.6740,  8.6768,  8.6797,  8.6826,\n",
      "          8.6855,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.2290,  9.4093,  9.1802,  9.0254,  8.9441,  8.8974,  8.8678,  8.8474,\n",
      "          8.8323,  9.3835, 10.4965,  9.4261,  9.1228,  8.9813,  8.9073,  8.8646,\n",
      "          8.8378,  8.8196,  8.8065,  8.7967,  8.7891,  8.7831,  8.7780,  8.7738,\n",
      "          8.7701,  8.7667,  8.7636,  9.3297, 10.3334,  9.2633,  9.0045,  8.8863,\n",
      "          8.8257,  8.7916,  8.7706,  8.7567,  8.7472,  8.7403,  8.7353,  9.3280,\n",
      "         10.0402,  9.1621,  8.9326,  8.8283,  8.7756,  8.7461,  8.7278,  8.7155,\n",
      "          8.7064,  8.6994,  8.6935,  8.6883,  8.6836,  8.6790,  8.6747,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,\n",
      "          1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977,  1.7977]],\n",
      "       device='cuda:0')\n",
      "Mask\n",
      "[ 9.173509   9.326847   9.045698   8.845648   8.727396   8.653463\n",
      "  8.604655   8.571239   8.547803   8.531082   8.518976   9.277231\n",
      " 10.499514   9.30436    8.954657   8.777567   8.674965   8.611334\n",
      "  8.570175   8.542946   8.524822   8.512868   8.505188   8.500509\n",
      "  8.497948   8.496884   8.49688    8.497616   8.498864   8.50046\n",
      "  8.502279   8.504233   9.290835  10.166311   9.0182495  8.737556\n",
      "  8.602338   8.5268135  8.481803   8.45414    8.437166   8.4272\n",
      "  8.422035   8.420258   8.420921   8.423365   8.427118   8.431837\n",
      "  8.43727    8.443222   8.449549   8.456135   8.462889   8.469736\n",
      "  8.47662    8.483485   8.490294   1.7977194  1.7977194  1.7977194\n",
      "  1.7977194  1.7977194  1.7977194  1.7977194  1.7977194  1.7977194\n",
      "  1.7977194  1.7977194  1.7977194  1.7977194  1.7977194  1.7977194\n",
      "  1.7977194  1.7977194  1.7977194  1.7977194  1.7977194  1.7977194\n",
      "  1.7977194  1.7977194  1.7977194  1.7977194  1.7977194  1.7977194\n",
      "  1.7977194  1.7977194  1.7977194]\n",
      "END Mask\n",
      "[[9.173509  9.326847  9.045698  ... 1.7977194 1.7977194 1.7977194]\n",
      " [8.908278  9.096685  8.854445  ... 1.7977194 1.7977194 1.7977194]\n",
      " [8.877897  9.034304  8.830355  ... 1.7977194 1.7977194 1.7977194]\n",
      " ...\n",
      " [9.116856  9.292591  9.093849  ... 1.7977194 1.7977194 1.7977194]\n",
      " [9.036642  9.337612  9.141671  ... 1.7977194 1.7977194 1.7977194]\n",
      " [9.229042  9.40931   9.180197  ... 1.7977194 1.7977194 1.7977194]]\n",
      "1 Name: 0    hamilton\n",
      "Name: driverRef, dtype: object\n",
      "4 Name: 3    alonso\n",
      "Name: driverRef, dtype: object\n",
      "815 Name: 814    perez\n",
      "Name: driverRef, dtype: object\n",
      "817 Name: 816    ricciardo\n",
      "Name: driverRef, dtype: object\n",
      "822 Name: 821    bottas\n",
      "Name: driverRef, dtype: object\n",
      "825 Name: 824    kevin_magnussen\n",
      "Name: driverRef, dtype: object\n",
      "830 Name: 829    max_verstappen\n",
      "Name: driverRef, dtype: object\n",
      "832 Name: 831    sainz\n",
      "Name: driverRef, dtype: object\n",
      "839 Name: 838    ocon\n",
      "Name: driverRef, dtype: object\n",
      "840 Name: 839    stroll\n",
      "Name: driverRef, dtype: object\n",
      "842 Name: 452    gasly\n",
      "Name: driverRef, dtype: object\n",
      "844 Name: 842    leclerc\n",
      "Name: driverRef, dtype: object\n",
      "846 Name: 844    norris\n",
      "Name: driverRef, dtype: object\n",
      "847 Name: 845    russell\n",
      "Name: driverRef, dtype: object\n",
      "848 Name: 846    albon\n",
      "Name: driverRef, dtype: object\n",
      "852 Name: 850    tsunoda\n",
      "Name: driverRef, dtype: object\n",
      "855 Name: 853    zhou\n",
      "Name: driverRef, dtype: object\n",
      "857 Name: 855    piastri\n",
      "Name: driverRef, dtype: object\n",
      "858 Name: 856    sargeant\n",
      "Name: driverRef, dtype: object\n",
      "\n",
      "\n",
      "Training model: model_b5_e150_lr0.0001_h256_l1_d0.2_w0.4_0.4_0.2.pth\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 1\n",
      "Average Loss: 41.874364\n",
      "Laptime Loss: 101.735584\n",
      "Position Loss: 2.829666\n",
      "Historical Loss: 0.241316\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 2\n",
      "Average Loss: 30.725284\n",
      "Laptime Loss: 74.101698\n",
      "Position Loss: 2.586403\n",
      "Historical Loss: 0.250216\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 3\n",
      "Average Loss: 30.531386\n",
      "Laptime Loss: 73.529483\n",
      "Position Loss: 2.675896\n",
      "Historical Loss: 0.246168\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 4\n",
      "Average Loss: 30.003609\n",
      "Laptime Loss: 72.261138\n",
      "Position Loss: 2.624475\n",
      "Historical Loss: 0.246817\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 5\n",
      "Average Loss: 29.557086\n",
      "Laptime Loss: 71.001074\n",
      "Position Loss: 2.767862\n",
      "Historical Loss: 0.247559\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 6\n",
      "Average Loss: 29.152651\n",
      "Laptime Loss: 70.024580\n",
      "Position Loss: 2.734240\n",
      "Historical Loss: 0.245612\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 7\n",
      "Average Loss: 28.855069\n",
      "Laptime Loss: 69.328229\n",
      "Position Loss: 2.687763\n",
      "Historical Loss: 0.243356\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 8\n",
      "Average Loss: 28.464014\n",
      "Laptime Loss: 68.396035\n",
      "Position Loss: 2.642274\n",
      "Historical Loss: 0.243449\n",
      "--------------------\n",
      "Epoch 9\n",
      "Average Loss: 28.557324\n",
      "Laptime Loss: 68.566670\n",
      "Position Loss: 2.704079\n",
      "Historical Loss: 0.245117\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 10\n",
      "Average Loss: 27.979312\n",
      "Laptime Loss: 67.068210\n",
      "Position Loss: 2.756490\n",
      "Historical Loss: 0.247157\n",
      "--------------------\n",
      "Epoch 11\n",
      "Average Loss: 27.985113\n",
      "Laptime Loss: 67.085494\n",
      "Position Loss: 2.755995\n",
      "Historical Loss: 0.242583\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 12\n",
      "Average Loss: 27.716813\n",
      "Laptime Loss: 66.527276\n",
      "Position Loss: 2.644252\n",
      "Historical Loss: 0.241007\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 13\n",
      "Average Loss: 27.667637\n",
      "Laptime Loss: 66.241772\n",
      "Position Loss: 2.803956\n",
      "Historical Loss: 0.246724\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 14\n",
      "Average Loss: 27.401902\n",
      "Laptime Loss: 65.614735\n",
      "Position Loss: 2.767367\n",
      "Historical Loss: 0.245303\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 15\n",
      "Average Loss: 27.232035\n",
      "Laptime Loss: 65.280225\n",
      "Position Loss: 2.673918\n",
      "Historical Loss: 0.251885\n",
      "--------------------\n",
      "Epoch 16\n",
      "Average Loss: 27.301276\n",
      "Laptime Loss: 65.421715\n",
      "Position Loss: 2.707540\n",
      "Historical Loss: 0.247868\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 17\n",
      "Average Loss: 27.089945\n",
      "Laptime Loss: 64.865730\n",
      "Position Loss: 2.735723\n",
      "Historical Loss: 0.246817\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 18\n",
      "Average Loss: 27.083142\n",
      "Laptime Loss: 64.756372\n",
      "Position Loss: 2.828183\n",
      "Historical Loss: 0.246601\n",
      "--------------------\n",
      "Epoch 19\n",
      "Average Loss: 27.083439\n",
      "Laptime Loss: 64.807222\n",
      "Position Loss: 2.778245\n",
      "Historical Loss: 0.246261\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 20\n",
      "Average Loss: 27.029497\n",
      "Laptime Loss: 64.873138\n",
      "Position Loss: 2.576020\n",
      "Historical Loss: 0.249166\n",
      "--------------------\n",
      "Epoch 21\n",
      "Average Loss: 27.038035\n",
      "Laptime Loss: 64.634178\n",
      "Position Loss: 2.838072\n",
      "Historical Loss: 0.245674\n",
      "--------------------\n",
      "Epoch 22\n",
      "Average Loss: 27.040006\n",
      "Laptime Loss: 64.858590\n",
      "Position Loss: 2.616564\n",
      "Historical Loss: 0.249722\n",
      "--------------------\n",
      "Epoch 23\n",
      "Average Loss: 27.052522\n",
      "Laptime Loss: 64.840358\n",
      "Position Loss: 2.666007\n",
      "Historical Loss: 0.249876\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 24\n",
      "Average Loss: 26.970290\n",
      "Laptime Loss: 64.568709\n",
      "Position Loss: 2.731768\n",
      "Historical Loss: 0.250494\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 25\n",
      "Average Loss: 26.901738\n",
      "Laptime Loss: 64.525096\n",
      "Position Loss: 2.608653\n",
      "Historical Loss: 0.241193\n",
      "--------------------\n",
      "Epoch 26\n",
      "Average Loss: 26.982435\n",
      "Laptime Loss: 64.670580\n",
      "Position Loss: 2.660074\n",
      "Historical Loss: 0.250865\n",
      "--------------------\n",
      "Epoch 27\n",
      "Average Loss: 26.981035\n",
      "Laptime Loss: 64.723199\n",
      "Position Loss: 2.604697\n",
      "Historical Loss: 0.249382\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 28\n",
      "Average Loss: 26.881547\n",
      "Laptime Loss: 64.394951\n",
      "Position Loss: 2.683313\n",
      "Historical Loss: 0.251205\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 29\n",
      "Average Loss: 26.859724\n",
      "Laptime Loss: 64.375467\n",
      "Position Loss: 2.649691\n",
      "Historical Loss: 0.248300\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 30\n",
      "Average Loss: 26.830567\n",
      "Laptime Loss: 64.228331\n",
      "Position Loss: 2.723857\n",
      "Historical Loss: 0.248455\n",
      "--------------------\n",
      "Epoch 31\n",
      "Average Loss: 26.843287\n",
      "Laptime Loss: 64.354044\n",
      "Position Loss: 2.631397\n",
      "Historical Loss: 0.245550\n",
      "--------------------\n",
      "Epoch 32\n",
      "Average Loss: 26.941477\n",
      "Laptime Loss: 64.513580\n",
      "Position Loss: 2.719407\n",
      "Historical Loss: 0.241409\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 33\n",
      "Average Loss: 26.800689\n",
      "Laptime Loss: 64.243718\n",
      "Position Loss: 2.633869\n",
      "Historical Loss: 0.248269\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 34\n",
      "Average Loss: 26.788295\n",
      "Laptime Loss: 64.173394\n",
      "Position Loss: 2.673424\n",
      "Historical Loss: 0.247837\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 35\n",
      "Average Loss: 26.756119\n",
      "Laptime Loss: 64.146818\n",
      "Position Loss: 2.618047\n",
      "Historical Loss: 0.250865\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 36\n",
      "Average Loss: 26.734690\n",
      "Laptime Loss: 64.093228\n",
      "Position Loss: 2.618047\n",
      "Historical Loss: 0.250896\n",
      "--------------------\n",
      "Epoch 37\n",
      "Average Loss: 26.739018\n",
      "Laptime Loss: 64.258036\n",
      "Position Loss: 2.467738\n",
      "Historical Loss: 0.243541\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 38\n",
      "Average Loss: 26.696818\n",
      "Laptime Loss: 64.021665\n",
      "Position Loss: 2.596292\n",
      "Historical Loss: 0.248177\n",
      "--------------------\n",
      "Epoch 39\n",
      "Average Loss: 26.748963\n",
      "Laptime Loss: 64.224648\n",
      "Position Loss: 2.522126\n",
      "Historical Loss: 0.251267\n",
      "--------------------\n",
      "Epoch 40\n",
      "Average Loss: 26.768757\n",
      "Laptime Loss: 64.121690\n",
      "Position Loss: 2.678863\n",
      "Historical Loss: 0.242676\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 41\n",
      "Average Loss: 26.654662\n",
      "Laptime Loss: 64.014713\n",
      "Position Loss: 2.495921\n",
      "Historical Loss: 0.252040\n",
      "--------------------\n",
      "Epoch 42\n",
      "Average Loss: 26.709930\n",
      "Laptime Loss: 64.090831\n",
      "Position Loss: 2.563164\n",
      "Historical Loss: 0.241656\n",
      "--------------------\n",
      "Epoch 43\n",
      "Average Loss: 26.657927\n",
      "Laptime Loss: 63.926553\n",
      "Position Loss: 2.593820\n",
      "Historical Loss: 0.248888\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 44\n",
      "Average Loss: 26.628005\n",
      "Laptime Loss: 63.849584\n",
      "Position Loss: 2.596292\n",
      "Historical Loss: 0.248269\n",
      "--------------------\n",
      "Epoch 45\n",
      "Average Loss: 26.651609\n",
      "Laptime Loss: 63.860819\n",
      "Position Loss: 2.645736\n",
      "Historical Loss: 0.244932\n",
      "--------------------\n",
      "Epoch 46\n",
      "Average Loss: 26.739651\n",
      "Laptime Loss: 64.103452\n",
      "Position Loss: 2.622003\n",
      "Historical Loss: 0.247342\n",
      "--------------------\n",
      "Epoch 47\n",
      "Average Loss: 26.690526\n",
      "Laptime Loss: 63.950558\n",
      "Position Loss: 2.653647\n",
      "Historical Loss: 0.244221\n",
      "--------------------\n",
      "Epoch 48\n",
      "Average Loss: 26.650830\n",
      "Laptime Loss: 63.858671\n",
      "Position Loss: 2.646724\n",
      "Historical Loss: 0.243356\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 49\n",
      "Average Loss: 26.557567\n",
      "Laptime Loss: 63.687827\n",
      "Position Loss: 2.580964\n",
      "Historical Loss: 0.250247\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 50\n",
      "Average Loss: 26.519506\n",
      "Laptime Loss: 63.606581\n",
      "Position Loss: 2.566131\n",
      "Historical Loss: 0.252101\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 51\n",
      "Average Loss: 26.479841\n",
      "Laptime Loss: 63.509566\n",
      "Position Loss: 2.564153\n",
      "Historical Loss: 0.251761\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 52\n",
      "Average Loss: 26.391338\n",
      "Laptime Loss: 63.305244\n",
      "Position Loss: 2.547342\n",
      "Historical Loss: 0.251514\n",
      "--------------------\n",
      "Epoch 53\n",
      "Average Loss: 26.511305\n",
      "Laptime Loss: 63.487996\n",
      "Position Loss: 2.666502\n",
      "Historical Loss: 0.247528\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 54\n",
      "Average Loss: 26.365638\n",
      "Laptime Loss: 63.219006\n",
      "Position Loss: 2.572559\n",
      "Historical Loss: 0.245056\n",
      "--------------------\n",
      "Epoch 55\n",
      "Average Loss: 26.446267\n",
      "Laptime Loss: 63.373778\n",
      "Position Loss: 2.613103\n",
      "Historical Loss: 0.257571\n",
      "--------------------\n",
      "Epoch 56\n",
      "Average Loss: 26.492334\n",
      "Laptime Loss: 63.504675\n",
      "Position Loss: 2.599753\n",
      "Historical Loss: 0.252812\n",
      "--------------------\n",
      "Epoch 57\n",
      "Average Loss: 26.404706\n",
      "Laptime Loss: 63.335356\n",
      "Position Loss: 2.550804\n",
      "Historical Loss: 0.251205\n",
      "--------------------\n",
      "Epoch 58\n",
      "Average Loss: 26.473166\n",
      "Laptime Loss: 63.410016\n",
      "Position Loss: 2.647219\n",
      "Historical Loss: 0.251360\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 59\n",
      "Average Loss: 26.364127\n",
      "Laptime Loss: 63.179987\n",
      "Position Loss: 2.605192\n",
      "Historical Loss: 0.250278\n",
      "--------------------\n",
      "Epoch 60\n",
      "Average Loss: 26.414675\n",
      "Laptime Loss: 63.316924\n",
      "Position Loss: 2.594314\n",
      "Historical Loss: 0.250896\n",
      "--------------------\n",
      "Epoch 61\n",
      "Average Loss: 26.390969\n",
      "Laptime Loss: 63.308478\n",
      "Position Loss: 2.543881\n",
      "Historical Loss: 0.250124\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 62\n",
      "Average Loss: 26.351901\n",
      "Laptime Loss: 63.165674\n",
      "Position Loss: 2.591842\n",
      "Historical Loss: 0.244468\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 63\n",
      "Average Loss: 26.338105\n",
      "Laptime Loss: 63.155150\n",
      "Position Loss: 2.566131\n",
      "Historical Loss: 0.247960\n",
      "--------------------\n",
      "Epoch 64\n",
      "Average Loss: 26.384621\n",
      "Laptime Loss: 63.179937\n",
      "Position Loss: 2.657602\n",
      "Historical Loss: 0.248022\n",
      "--------------------\n",
      "Epoch 65\n",
      "Average Loss: 26.372510\n",
      "Laptime Loss: 63.154542\n",
      "Position Loss: 2.652163\n",
      "Historical Loss: 0.249135\n",
      "--------------------\n",
      "Epoch 66\n",
      "Average Loss: 26.365913\n",
      "Laptime Loss: 63.172230\n",
      "Position Loss: 2.617553\n",
      "Historical Loss: 0.250000\n",
      "--------------------\n",
      "Epoch 67\n",
      "Average Loss: 26.431107\n",
      "Laptime Loss: 63.323379\n",
      "Position Loss: 2.628430\n",
      "Historical Loss: 0.251916\n",
      "--------------------\n",
      "Epoch 68\n",
      "Average Loss: 26.382557\n",
      "Laptime Loss: 63.254706\n",
      "Position Loss: 2.578492\n",
      "Historical Loss: 0.246384\n",
      "--------------------\n",
      "Epoch 69\n",
      "Average Loss: 26.371411\n",
      "Laptime Loss: 63.245522\n",
      "Position Loss: 2.558220\n",
      "Historical Loss: 0.249567\n",
      "--------------------\n",
      "Epoch 70\n",
      "Average Loss: 26.412279\n",
      "Laptime Loss: 63.247383\n",
      "Position Loss: 2.662546\n",
      "Historical Loss: 0.241533\n",
      "--------------------\n",
      "Epoch 71\n",
      "Average Loss: 26.347438\n",
      "Laptime Loss: 63.232716\n",
      "Position Loss: 2.513721\n",
      "Historical Loss: 0.244314\n",
      "--------------------\n",
      "Epoch 72\n",
      "Average Loss: 26.349761\n",
      "Laptime Loss: 63.139266\n",
      "Position Loss: 2.614586\n",
      "Historical Loss: 0.241100\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 73\n",
      "Average Loss: 26.328522\n",
      "Laptime Loss: 63.180745\n",
      "Position Loss: 2.519160\n",
      "Historical Loss: 0.242800\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 74\n",
      "Average Loss: 26.318537\n",
      "Laptime Loss: 63.166767\n",
      "Position Loss: 2.505315\n",
      "Historical Loss: 0.248517\n",
      "--------------------\n",
      "Epoch 75\n",
      "Average Loss: 26.412184\n",
      "Laptime Loss: 63.268638\n",
      "Position Loss: 2.636341\n",
      "Historical Loss: 0.250958\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 76\n",
      "Average Loss: 26.304306\n",
      "Laptime Loss: 63.113792\n",
      "Position Loss: 2.525587\n",
      "Historical Loss: 0.242769\n",
      "--------------------\n",
      "Epoch 77\n",
      "Average Loss: 26.354462\n",
      "Laptime Loss: 63.324518\n",
      "Position Loss: 2.436094\n",
      "Historical Loss: 0.251082\n",
      "--------------------\n",
      "Epoch 78\n",
      "Average Loss: 26.456995\n",
      "Laptime Loss: 63.418629\n",
      "Position Loss: 2.602225\n",
      "Historical Loss: 0.243263\n",
      "--------------------\n",
      "Epoch 79\n",
      "Average Loss: 26.381612\n",
      "Laptime Loss: 63.286291\n",
      "Position Loss: 2.543881\n",
      "Historical Loss: 0.247713\n",
      "--------------------\n",
      "Epoch 80\n",
      "Average Loss: 26.346304\n",
      "Laptime Loss: 63.084887\n",
      "Position Loss: 2.657108\n",
      "Historical Loss: 0.247528\n",
      "--------------------\n",
      "Epoch 81\n",
      "Average Loss: 26.466690\n",
      "Laptime Loss: 63.407870\n",
      "Position Loss: 2.636341\n",
      "Historical Loss: 0.245025\n",
      "--------------------\n",
      "Epoch 82\n",
      "Average Loss: 26.408019\n",
      "Laptime Loss: 63.219645\n",
      "Position Loss: 2.678368\n",
      "Historical Loss: 0.244067\n",
      "--------------------\n",
      "Epoch 83\n",
      "Average Loss: 26.416277\n",
      "Laptime Loss: 63.377774\n",
      "Position Loss: 2.539926\n",
      "Historical Loss: 0.245983\n",
      "--------------------\n",
      "Epoch 84\n",
      "Average Loss: 26.376213\n",
      "Laptime Loss: 63.278696\n",
      "Position Loss: 2.537948\n",
      "Historical Loss: 0.247775\n",
      "--------------------\n",
      "Epoch 85\n",
      "Average Loss: 26.384494\n",
      "Laptime Loss: 63.173981\n",
      "Position Loss: 2.665019\n",
      "Historical Loss: 0.244468\n",
      "--------------------\n",
      "Epoch 86\n",
      "Average Loss: 26.420102\n",
      "Laptime Loss: 63.344583\n",
      "Position Loss: 2.580470\n",
      "Historical Loss: 0.250402\n",
      "--------------------\n",
      "Epoch 87\n",
      "Average Loss: 26.438116\n",
      "Laptime Loss: 63.341302\n",
      "Position Loss: 2.631891\n",
      "Historical Loss: 0.244190\n",
      "--------------------\n",
      "Epoch 88\n",
      "Average Loss: 26.416560\n",
      "Laptime Loss: 63.353807\n",
      "Position Loss: 2.564153\n",
      "Historical Loss: 0.246879\n",
      "--------------------\n",
      "Epoch 89\n",
      "Average Loss: 26.378315\n",
      "Laptime Loss: 63.260695\n",
      "Position Loss: 2.562670\n",
      "Historical Loss: 0.244839\n",
      "--------------------\n",
      "Epoch 90\n",
      "Average Loss: 26.432788\n",
      "Laptime Loss: 63.371941\n",
      "Position Loss: 2.589370\n",
      "Historical Loss: 0.241316\n",
      "Early stopping triggered at epoch 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54359096/ipykernel_857652/2456267903.py:670: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('./2024/best_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 64.89579495329123\n",
      "Outputs\n",
      "O: tensor([[8.7816, 8.9665, 8.7194, 8.5831, 8.5168, 8.4811, 8.4609, 8.4492, 8.4422,\n",
      "         8.4381, 8.4355, 9.1519, 9.9823, 8.9639, 8.6746, 8.5463, 8.4817, 8.4467,\n",
      "         8.4269, 8.4157, 8.4092, 8.4057, 8.4039, 8.4033, 8.4033, 8.4039, 8.4049,\n",
      "         8.4061, 8.4076, 8.4093, 8.4113, 8.4133, 9.1461, 9.6662, 8.8057, 8.5639,\n",
      "         8.4588, 8.4071, 8.3801, 8.3656, 8.3582, 8.3548, 8.3537, 8.3541, 8.3554,\n",
      "         8.3573, 8.3596, 8.3621, 8.3649, 8.3678, 8.3709, 8.3740, 8.3773, 8.3806,\n",
      "         8.3840, 8.3875, 8.3911, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884]], device='cuda:0')\n",
      "O: tensor([[8.8710, 8.9322, 8.6627, 8.5402, 8.4807, 8.4473, 8.4271, 8.4144, 8.4062,\n",
      "         8.4011, 8.3982, 8.3968, 8.3965, 8.3970, 9.0975, 9.8349, 8.7924, 8.5797,\n",
      "         8.4784, 8.4278, 8.3998, 8.3838, 8.3746, 8.3695, 8.3671, 8.3663, 8.3667,\n",
      "         8.3677, 8.3692, 8.3709, 8.3729, 8.3749, 8.3770, 8.3792, 8.3814, 8.3835,\n",
      "         8.3857, 8.3878, 8.3900, 8.3920, 9.1179, 9.4900, 8.6377, 8.4720, 8.3943,\n",
      "         8.3574, 8.3382, 8.3280, 8.3228, 8.3203, 8.3195, 8.3195, 8.3201, 8.3210,\n",
      "         8.3220, 8.3230, 8.3240, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884]], device='cuda:0')\n",
      "O: tensor([[9.0935, 8.9298, 8.6781, 8.5522, 8.4849, 8.4465, 8.4238, 8.4099, 8.4014,\n",
      "         8.3964, 8.3937, 9.0742, 9.8334, 8.8168, 8.5975, 8.4912, 8.4371, 8.4067,\n",
      "         8.3892, 8.3792, 8.3737, 8.3713, 8.3708, 8.3716, 8.3732, 8.3755, 8.3782,\n",
      "         8.3812, 8.3844, 8.3877, 8.3911, 8.3946, 8.3981, 8.4017, 8.4054, 9.1253,\n",
      "         9.6070, 8.6903, 8.5102, 8.4263, 8.3869, 8.3667, 8.3563, 8.3514, 8.3497,\n",
      "         8.3499, 8.3512, 8.3532, 8.3556, 8.3583, 8.3612, 8.3642, 8.3672, 8.3702,\n",
      "         8.3732, 8.3762, 8.3793, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884]], device='cuda:0')\n",
      "O: tensor([[ 9.3354,  9.0634,  8.8127,  8.6845,  8.6197,  8.5839,  8.5628,  8.5499,\n",
      "          8.5421,  8.5375,  8.5351,  8.5344,  9.1810, 10.0253,  8.9681,  8.7364,\n",
      "          8.6280,  8.5739,  8.5442,  8.5276,  8.5184,  8.5137,  8.5119,  8.5119,\n",
      "          8.5131,  8.5151,  8.5176,  8.5204,  8.5235,  8.5266,  8.5299,  8.5332,\n",
      "          8.5365,  8.5399,  9.2176,  9.7991,  8.8615,  8.6654,  8.5773,  8.5361,\n",
      "          8.5151,  8.5042,  8.4989,  8.4969,  8.4967,  8.4977,  8.4993,  8.5014,\n",
      "          8.5036,  8.5060,  8.5085,  8.5109,  8.5133,  8.5157,  8.5180,  8.5202,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.0592,  9.0749,  8.8844,  8.7686,  8.7115,  8.6805,  8.6625,  8.6517,\n",
      "          8.6452,  8.6414,  8.6393,  9.1826, 10.0253,  9.0515,  8.8266,  8.7256,\n",
      "          8.6772,  8.6515,  8.6374,  8.6296,  8.6256,  8.6238,  8.6235,  8.6240,\n",
      "          8.6251,  8.6264,  8.6280,  8.6297,  8.6314,  9.2043,  9.8032,  8.9577,\n",
      "          8.7663,  8.6822,  8.6430,  8.6230,  8.6124,  8.6069,  8.6042,  8.6031,\n",
      "          8.6030,  8.6034,  8.6040,  8.6048,  8.6056,  8.6064,  8.6070,  8.6076,\n",
      "          8.6080,  8.6084,  8.6085,  8.6086,  8.6085,  8.6082,  8.6079,  8.6074,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[9.3213, 9.0289, 8.8080, 8.7071, 8.6568, 8.6276, 8.6088, 8.5955, 8.5855,\n",
      "         8.5777, 9.1842, 9.8537, 8.9457, 8.7460, 8.6556, 8.6092, 8.5822, 8.5651,\n",
      "         8.5537, 8.5460, 8.5408, 8.5375, 8.5356, 8.5348, 8.5350, 8.5358, 8.5372,\n",
      "         8.5390, 8.5411, 8.5434, 8.5459, 9.2055, 9.7220, 8.8144, 8.6608, 8.5856,\n",
      "         8.5494, 8.5295, 8.5181, 8.5116, 8.5083, 8.5068, 8.5067, 8.5075, 8.5089,\n",
      "         8.5106, 8.5126, 8.5147, 8.5170, 8.5193, 8.5216, 8.5239, 8.5263, 8.5286,\n",
      "         8.5309, 8.5331, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884]], device='cuda:0')\n",
      "O: tensor([[8.8771, 8.9386, 8.7215, 8.5769, 8.5016, 8.4613, 8.4403, 8.4297, 8.4249,\n",
      "         8.4233, 8.4235, 8.4248, 8.4266, 8.4287, 8.4310, 8.4333, 9.1629, 9.8399,\n",
      "         8.8841, 8.6358, 8.5199, 8.4610, 8.4293, 8.4120, 8.4026, 8.3978, 8.3955,\n",
      "         8.3947, 8.3947, 8.3951, 8.3958, 8.3965, 8.3973, 8.3981, 8.3988, 8.3995,\n",
      "         9.1481, 9.5887, 8.7841, 8.5680, 8.4721, 8.4242, 8.3985, 8.3840, 8.3756,\n",
      "         8.3708, 8.3680, 8.3665, 8.3657, 8.3655, 8.3655, 8.3659, 8.3665, 8.3673,\n",
      "         8.3683, 8.3694, 8.3708, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884]], device='cuda:0')\n",
      "O: tensor([[9.0628, 8.9183, 8.6856, 8.5707, 8.5095, 8.4742, 8.4525, 8.4384, 8.4289,\n",
      "         8.4223, 8.4179, 8.4152, 8.4139, 9.0884, 9.7760, 8.7891, 8.5934, 8.4981,\n",
      "         8.4494, 8.4216, 8.4049, 8.3948, 8.3888, 8.3856, 8.3843, 8.3843, 8.3853,\n",
      "         8.3869, 8.3890, 8.3915, 8.3942, 8.3970, 8.4000, 8.4031, 9.1145, 9.5273,\n",
      "         8.6584, 8.5036, 8.4268, 8.3899, 8.3703, 8.3598, 8.3544, 8.3522, 8.3519,\n",
      "         8.3527, 8.3543, 8.3563, 8.3586, 8.3611, 8.3638, 8.3664, 8.3692, 8.3719,\n",
      "         8.3747, 8.3775, 8.3802, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884]], device='cuda:0')\n",
      "O: tensor([[9.4092, 9.1383, 8.9316, 8.8302, 8.7768, 8.7448, 8.7236, 8.7083, 8.6965,\n",
      "         9.2291, 9.9706, 9.0856, 8.8780, 8.7814, 8.7310, 8.7010, 8.6816, 8.6683,\n",
      "         8.6588, 8.6520, 8.6470, 8.6435, 8.6411, 8.6396, 8.6388, 8.6386, 8.6388,\n",
      "         8.6394, 8.6402, 9.2218, 9.8544, 8.9545, 8.7944, 8.7127, 8.6724, 8.6495,\n",
      "         8.6357, 8.6273, 8.6221, 8.6190, 8.6173, 8.6166, 8.6164, 8.6167, 8.6172,\n",
      "         8.6179, 8.6186, 8.6194, 8.6203, 8.6211, 8.6219, 8.6226, 8.6233, 8.6238,\n",
      "         8.6243, 8.6247, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884]], device='cuda:0')\n",
      "O: tensor([[9.2352, 8.9628, 8.7387, 8.6349, 8.5831, 8.5533, 8.5339, 8.5201, 9.1184,\n",
      "         9.7723, 8.8877, 8.6804, 8.5871, 8.5390, 8.5108, 8.4926, 8.4804, 8.4719,\n",
      "         8.4659, 8.4619, 8.4594, 8.4581, 8.4578, 8.4582, 8.4592, 8.4607, 9.1177,\n",
      "         9.7027, 8.7686, 8.6030, 8.5227, 8.4833, 8.4611, 8.4480, 8.4402, 8.4357,\n",
      "         8.4333, 8.4324, 8.4326, 8.4335, 8.4348, 8.4365, 8.4384, 8.4405, 8.4427,\n",
      "         8.4449, 8.4472, 8.4495, 8.4518, 8.4540, 8.4563, 8.4585, 8.4607, 8.4629,\n",
      "         8.4650, 8.4670, 8.4690, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884]], device='cuda:0')\n",
      "O: tensor([[9.4280, 9.1451, 8.9536, 8.8587, 8.8087, 8.7785, 8.7582, 8.7434, 8.7318,\n",
      "         8.7224, 8.7146, 9.2464, 9.9812, 9.0874, 8.8960, 8.8059, 8.7594, 8.7316,\n",
      "         8.7136, 8.7012, 8.6924, 8.6860, 8.6814, 8.6781, 8.6759, 8.6745, 8.6737,\n",
      "         8.6734, 8.6734, 8.6738, 9.2516, 9.8339, 8.9694, 8.8198, 8.7432, 8.7057,\n",
      "         8.6844, 8.6717, 8.6637, 8.6588, 8.6559, 8.6541, 9.2339, 9.6940, 8.9421,\n",
      "         8.7967, 8.7271, 8.6944, 8.6764, 8.6659, 8.6595, 8.6555, 8.6529, 8.6513,\n",
      "         8.6502, 8.6495, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884]], device='cuda:0')\n",
      "O: tensor([[8.9254, 8.9482, 8.7320, 8.6164, 8.5573, 8.5252, 8.5069, 8.4961, 8.4898,\n",
      "         8.4864, 9.1516, 9.8820, 8.8706, 8.6594, 8.5622, 8.5151, 8.4898, 8.4758,\n",
      "         8.4682, 8.4646, 8.4635, 8.4640, 8.4656, 8.4678, 8.4706, 8.4736, 8.4768,\n",
      "         8.4802, 8.4837, 8.4872, 8.4908, 8.4944, 8.4980, 9.2023, 9.6352, 8.7442,\n",
      "         8.5754, 8.4987, 8.4639, 8.4466, 8.4381, 8.4345, 8.4336, 8.4343, 8.4360,\n",
      "         8.4383, 8.4408, 8.4436, 8.4465, 8.4494, 8.4524, 8.4553, 8.4582, 8.4611,\n",
      "         8.4640, 8.4668, 8.4695, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884]], device='cuda:0')\n",
      "O: tensor([[9.1132, 8.9969, 8.7833, 8.6750, 8.6209, 8.5907, 8.5719, 8.5592, 8.5501,\n",
      "         8.5436, 8.5389, 8.5357, 9.1626, 9.9113, 8.8982, 8.6998, 8.6086, 8.5640,\n",
      "         8.5388, 8.5236, 8.5142, 8.5085, 8.5052, 8.5036, 8.5032, 8.5036, 8.5048,\n",
      "         8.5063, 8.5082, 8.5103, 8.5125, 8.5149, 9.1831, 9.6855, 8.7667, 8.6083,\n",
      "         8.5331, 8.4987, 8.4806, 8.4710, 8.4660, 8.4637, 8.4632, 8.4637, 8.4648,\n",
      "         8.4664, 8.4681, 8.4701, 8.4721, 8.4742, 8.4762, 8.4783, 8.4803, 8.4823,\n",
      "         8.4843, 8.4862, 8.4881, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884]], device='cuda:0')\n",
      "O: tensor([[9.0813, 8.9810, 8.7444, 8.6266, 8.5682, 8.5366, 8.5180, 8.5064, 8.4989,\n",
      "         8.4942, 9.1450, 9.9626, 8.8917, 8.6687, 8.5689, 8.5211, 8.4951, 8.4803,\n",
      "         8.4719, 8.4673, 8.4652, 8.4648, 8.4656, 8.4672, 8.4694, 8.4719, 8.4747,\n",
      "         8.4777, 8.4809, 8.4841, 9.1803, 9.7389, 8.7679, 8.5860, 8.5044, 8.4678,\n",
      "         8.4494, 8.4402, 8.4359, 8.4345, 8.4348, 8.4361, 8.4381, 8.4404, 8.4430,\n",
      "         8.4457, 8.4485, 8.4513, 8.4541, 8.4569, 8.4597, 8.4624, 8.4651, 8.4678,\n",
      "         8.4703, 8.4728, 8.4752, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884]], device='cuda:0')\n",
      "O: tensor([[9.3126, 9.0597, 8.8343, 8.7324, 8.6823, 8.6532, 8.6342, 8.6204, 8.6099,\n",
      "         8.6014, 8.5945, 8.5890, 8.5847, 8.5814, 9.1906, 9.9744, 8.9464, 8.7506,\n",
      "         8.6611, 8.6166, 8.5908, 8.5745, 8.5636, 8.5560, 8.5509, 8.5475, 8.5453,\n",
      "         8.5442, 8.5437, 8.5439, 8.5444, 8.5452, 8.5461, 8.5472, 8.5484, 9.1976,\n",
      "         9.7327, 8.8025, 8.6496, 8.5755, 8.5409, 8.5219, 8.5109, 8.5044, 8.5006,\n",
      "         8.4986, 8.4975, 8.4972, 8.4973, 8.4976, 8.4981, 8.4986, 8.4991, 8.4996,\n",
      "         8.5000, 8.5003, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884]], device='cuda:0')\n",
      "O: tensor([[ 9.2924,  9.0596,  8.8208,  8.7111,  8.6573,  8.6267,  8.6074,  8.5941,\n",
      "          8.5845,  8.5774,  8.5722,  8.5684,  8.5658,  9.1778, 10.0322,  8.9570,\n",
      "          8.7431,  8.6462,  8.5990,  8.5724,  8.5564,  8.5464,  8.5402,  8.5365,\n",
      "          8.5345,  8.5337,  8.5338,  8.5346,  8.5358,  8.5373,  8.5391,  8.5410,\n",
      "          8.5430,  9.1938,  9.7873,  8.8298,  8.6580,  8.5780,  8.5413,  8.5220,\n",
      "          8.5115,  8.5059,  8.5031,  8.5020,  8.5020,  8.5026,  8.5037,  8.5050,\n",
      "          8.5064,  8.5078,  8.5093,  8.5108,  8.5122,  8.5136,  8.5148,  8.5160,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[9.3133, 9.0923, 8.9081, 8.8206, 8.7754, 8.7480, 8.7293, 8.7151, 9.2479,\n",
      "         9.8903, 9.0528, 8.8581, 8.7725, 8.7287, 8.7022, 8.6844, 8.6713, 8.6611,\n",
      "         8.6530, 8.6465, 8.6413, 8.6372, 8.6339, 8.6314, 8.6295, 8.6280, 8.6269,\n",
      "         9.2112, 9.8286, 8.9141, 8.7627, 8.6875, 8.6508, 8.6292, 8.6154, 8.6061,\n",
      "         8.5996, 8.5949, 8.5915, 8.5891, 8.5872, 8.5858, 8.5847, 8.5837, 8.5828,\n",
      "         8.5820, 8.5811, 8.5802, 8.5792, 8.5780, 8.5768, 8.5754, 8.5738, 8.5720,\n",
      "         8.5699, 8.5677, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884]], device='cuda:0')\n",
      "O: tensor([[9.1801, 9.0451, 8.8406, 8.7383, 8.6887, 8.6607, 8.6427, 8.6301, 8.6206,\n",
      "         8.6133, 8.6076, 9.2193, 9.9336, 8.9539, 8.7617, 8.6745, 8.6316, 8.6070,\n",
      "         8.5916, 8.5815, 8.5747, 8.5701, 8.5671, 8.5653, 8.5643, 8.5640, 8.5640,\n",
      "         8.5644, 8.5650, 8.5658, 8.5666, 8.5674, 8.5683, 9.2245, 9.7144, 8.7950,\n",
      "         8.6475, 8.5757, 8.5431, 8.5256, 8.5158, 8.5101, 8.5069, 8.5052, 8.5043,\n",
      "         8.5040, 8.5039, 8.5041, 8.5042, 8.5044, 8.5045, 8.5046, 8.5045, 8.5044,\n",
      "         8.5041, 8.5036, 8.5030, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884,\n",
      "         1.6884, 1.6884, 1.6884, 1.6884, 1.6884, 1.6884]], device='cuda:0')\n",
      "O: tensor([[ 9.4408,  9.1864,  8.9828,  8.8856,  8.8358,  8.8059,  8.7855,  8.7699,\n",
      "          8.7570,  9.2602, 10.0966,  9.1468,  8.9290,  8.8342,  8.7864,  8.7578,\n",
      "          8.7385,  8.7242,  8.7130,  8.7040,  8.6966,  8.6904,  8.6853,  8.6811,\n",
      "          8.6775,  8.6746,  8.6721,  9.2238,  9.9974,  9.0093,  8.8359,  8.7520,\n",
      "          8.7115,  8.6877,  8.6723,  8.6616,  8.6538,  8.6480,  8.6435,  9.1950,\n",
      "          9.8491,  8.9630,  8.7924,  8.7138,  8.6773,  8.6565,  8.6434,  8.6344,\n",
      "          8.6278,  8.6227,  8.6185,  8.6150,  8.6119,  8.6089,  8.6060,  1.6884,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,\n",
      "          1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884,  1.6884]],\n",
      "       device='cuda:0')\n",
      "Mask\n",
      "[8.781554  8.96646   8.719436  8.583091  8.516767  8.481076  8.460925\n",
      " 8.449201  8.442247  8.438067  8.435537  9.151868  9.982262  8.963933\n",
      " 8.674603  8.546251  8.481658  8.446654  8.426933  8.415656  8.409243\n",
      " 8.405714  8.403945  8.403282  8.403349  8.403923  8.404874  8.406121\n",
      " 8.407619  8.409336  8.411251  8.413348  9.146094  9.666223  8.805652\n",
      " 8.563922  8.458835  8.407144  8.380063  8.3656435 8.358201  8.354778\n",
      " 8.353734  8.354137  8.355441  8.357325  8.359597  8.362145  8.364897\n",
      " 8.367809  8.370852  8.37401   8.377267  8.380612  8.38404   8.387542\n",
      " 8.391112  1.6883771 1.6883771 1.6883771 1.6883771 1.6883771 1.6883771\n",
      " 1.6883771 1.6883771 1.6883771 1.6883771 1.6883771 1.6883771 1.6883771\n",
      " 1.6883771 1.6883771 1.6883771 1.6883771 1.6883771 1.6883771 1.6883771\n",
      " 1.6883771 1.6883771 1.6883771 1.6883771 1.6883771 1.6883771 1.6883771\n",
      " 1.6883771 1.6883771 1.6883771]\n",
      "END Mask\n",
      "[[8.781554  8.96646   8.719436  ... 1.6883771 1.6883771 1.6883771]\n",
      " [8.871036  8.932168  8.662653  ... 1.6883771 1.6883771 1.6883771]\n",
      " [9.093454  8.929797  8.678069  ... 1.6883771 1.6883771 1.6883771]\n",
      " ...\n",
      " [9.31329   9.092337  8.908129  ... 1.6883771 1.6883771 1.6883771]\n",
      " [9.180083  9.045117  8.840584  ... 1.6883771 1.6883771 1.6883771]\n",
      " [9.4408    9.186429  8.982783  ... 1.6883771 1.6883771 1.6883771]]\n",
      "1 Name: 0    hamilton\n",
      "Name: driverRef, dtype: object\n",
      "4 Name: 3    alonso\n",
      "Name: driverRef, dtype: object\n",
      "815 Name: 814    perez\n",
      "Name: driverRef, dtype: object\n",
      "817 Name: 816    ricciardo\n",
      "Name: driverRef, dtype: object\n",
      "822 Name: 821    bottas\n",
      "Name: driverRef, dtype: object\n",
      "825 Name: 824    kevin_magnussen\n",
      "Name: driverRef, dtype: object\n",
      "830 Name: 829    max_verstappen\n",
      "Name: driverRef, dtype: object\n",
      "832 Name: 831    sainz\n",
      "Name: driverRef, dtype: object\n",
      "839 Name: 838    ocon\n",
      "Name: driverRef, dtype: object\n",
      "840 Name: 839    stroll\n",
      "Name: driverRef, dtype: object\n",
      "842 Name: 452    gasly\n",
      "Name: driverRef, dtype: object\n",
      "844 Name: 842    leclerc\n",
      "Name: driverRef, dtype: object\n",
      "846 Name: 844    norris\n",
      "Name: driverRef, dtype: object\n",
      "847 Name: 845    russell\n",
      "Name: driverRef, dtype: object\n",
      "848 Name: 846    albon\n",
      "Name: driverRef, dtype: object\n",
      "852 Name: 850    tsunoda\n",
      "Name: driverRef, dtype: object\n",
      "855 Name: 853    zhou\n",
      "Name: driverRef, dtype: object\n",
      "857 Name: 855    piastri\n",
      "Name: driverRef, dtype: object\n",
      "858 Name: 856    sargeant\n",
      "Name: driverRef, dtype: object\n",
      "\n",
      "\n",
      "Training model: model_b5_e150_lr0.0001_h256_l1_d0.3_w0.6_0.3_0.1.pth\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 1\n",
      "Average Loss: 61.961291\n",
      "Laptime Loss: 101.802560\n",
      "Position Loss: 2.850433\n",
      "Historical Loss: 0.246230\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 2\n",
      "Average Loss: 45.146278\n",
      "Laptime Loss: 73.813968\n",
      "Position Loss: 2.776267\n",
      "Historical Loss: 0.250155\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 3\n",
      "Average Loss: 45.011021\n",
      "Laptime Loss: 73.633312\n",
      "Position Loss: 2.686774\n",
      "Historical Loss: 0.250000\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 4\n",
      "Average Loss: 44.178234\n",
      "Laptime Loss: 72.233261\n",
      "Position Loss: 2.709518\n",
      "Historical Loss: 0.254203\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 5\n",
      "Average Loss: 43.245817\n",
      "Laptime Loss: 70.655185\n",
      "Position Loss: 2.760445\n",
      "Historical Loss: 0.245705\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 6\n",
      "Average Loss: 42.653149\n",
      "Laptime Loss: 69.695481\n",
      "Position Loss: 2.705562\n",
      "Historical Loss: 0.241904\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 7\n",
      "Average Loss: 42.403996\n",
      "Laptime Loss: 69.265896\n",
      "Position Loss: 2.733251\n",
      "Historical Loss: 0.244808\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 8\n",
      "Average Loss: 41.932259\n",
      "Laptime Loss: 68.498272\n",
      "Position Loss: 2.695674\n",
      "Historical Loss: 0.245921\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 9\n",
      "Average Loss: 41.781217\n",
      "Laptime Loss: 68.247076\n",
      "Position Loss: 2.696663\n",
      "Historical Loss: 0.239710\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 10\n",
      "Average Loss: 41.356015\n",
      "Laptime Loss: 67.494128\n",
      "Position Loss: 2.783684\n",
      "Historical Loss: 0.244314\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 11\n",
      "Average Loss: 40.801573\n",
      "Laptime Loss: 66.570131\n",
      "Position Loss: 2.781706\n",
      "Historical Loss: 0.249815\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 12\n",
      "Average Loss: 40.630535\n",
      "Laptime Loss: 66.311124\n",
      "Position Loss: 2.729790\n",
      "Historical Loss: 0.249227\n",
      "--------------------\n",
      "Epoch 13\n",
      "Average Loss: 40.698911\n",
      "Laptime Loss: 66.369376\n",
      "Position Loss: 2.842027\n",
      "Historical Loss: 0.246755\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 14\n",
      "Average Loss: 40.152792\n",
      "Laptime Loss: 65.502808\n",
      "Position Loss: 2.755995\n",
      "Historical Loss: 0.243078\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 15\n",
      "Average Loss: 40.140482\n",
      "Laptime Loss: 65.475018\n",
      "Position Loss: 2.766873\n",
      "Historical Loss: 0.254079\n",
      "--------------------\n",
      "Epoch 16\n",
      "Average Loss: 40.323787\n",
      "Laptime Loss: 65.792784\n",
      "Position Loss: 2.744129\n",
      "Historical Loss: 0.248764\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 17\n",
      "Average Loss: 39.915862\n",
      "Laptime Loss: 65.117319\n",
      "Position Loss: 2.737701\n",
      "Historical Loss: 0.241595\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 18\n",
      "Average Loss: 39.697946\n",
      "Laptime Loss: 64.796549\n",
      "Position Loss: 2.651174\n",
      "Historical Loss: 0.246632\n",
      "--------------------\n",
      "Epoch 19\n",
      "Average Loss: 39.909521\n",
      "Laptime Loss: 65.134356\n",
      "Position Loss: 2.682818\n",
      "Historical Loss: 0.240606\n",
      "--------------------\n",
      "Epoch 20\n",
      "Average Loss: 39.864631\n",
      "Laptime Loss: 65.032665\n",
      "Position Loss: 2.734240\n",
      "Historical Loss: 0.247590\n",
      "--------------------\n",
      "Epoch 21\n",
      "Average Loss: 39.769361\n",
      "Laptime Loss: 64.833826\n",
      "Position Loss: 2.816316\n",
      "Historical Loss: 0.241687\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 22\n",
      "Average Loss: 39.627603\n",
      "Laptime Loss: 64.696266\n",
      "Position Loss: 2.618047\n",
      "Historical Loss: 0.244283\n",
      "--------------------\n",
      "Epoch 23\n",
      "Average Loss: 39.730684\n",
      "Laptime Loss: 64.832592\n",
      "Position Loss: 2.689740\n",
      "Historical Loss: 0.242058\n",
      "--------------------\n",
      "Epoch 24\n",
      "Average Loss: 39.716913\n",
      "Laptime Loss: 64.821465\n",
      "Position Loss: 2.665019\n",
      "Historical Loss: 0.245272\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 25\n",
      "Average Loss: 39.581031\n",
      "Laptime Loss: 64.588237\n",
      "Position Loss: 2.681335\n",
      "Historical Loss: 0.236867\n",
      "--------------------\n",
      "Epoch 26\n",
      "Average Loss: 39.588891\n",
      "Laptime Loss: 64.589774\n",
      "Position Loss: 2.702596\n",
      "Historical Loss: 0.242460\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 27\n",
      "Average Loss: 39.512223\n",
      "Laptime Loss: 64.512448\n",
      "Position Loss: 2.602225\n",
      "Historical Loss: 0.240853\n",
      "--------------------\n",
      "Epoch 28\n",
      "Average Loss: 39.515394\n",
      "Laptime Loss: 64.515261\n",
      "Position Loss: 2.606675\n",
      "Historical Loss: 0.242336\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 29\n",
      "Average Loss: 39.470910\n",
      "Laptime Loss: 64.381372\n",
      "Position Loss: 2.725340\n",
      "Historical Loss: 0.244839\n",
      "--------------------\n",
      "Epoch 30\n",
      "Average Loss: 39.559116\n",
      "Laptime Loss: 64.525460\n",
      "Position Loss: 2.730284\n",
      "Historical Loss: 0.247528\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 31\n",
      "Average Loss: 39.454128\n",
      "Laptime Loss: 64.413589\n",
      "Position Loss: 2.605192\n",
      "Historical Loss: 0.244159\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 32\n",
      "Average Loss: 39.425266\n",
      "Laptime Loss: 64.395712\n",
      "Position Loss: 2.544376\n",
      "Historical Loss: 0.245241\n",
      "--------------------\n",
      "Epoch 33\n",
      "Average Loss: 39.445873\n",
      "Laptime Loss: 64.379002\n",
      "Position Loss: 2.645241\n",
      "Historical Loss: 0.248980\n",
      "--------------------\n",
      "Epoch 34\n",
      "Average Loss: 39.499956\n",
      "Laptime Loss: 64.469357\n",
      "Position Loss: 2.644252\n",
      "Historical Loss: 0.250649\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 35\n",
      "Average Loss: 39.402749\n",
      "Laptime Loss: 64.281016\n",
      "Position Loss: 2.696168\n",
      "Historical Loss: 0.252874\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 36\n",
      "Average Loss: 39.288929\n",
      "Laptime Loss: 64.112547\n",
      "Position Loss: 2.653647\n",
      "Historical Loss: 0.253059\n",
      "--------------------\n",
      "Epoch 37\n",
      "Average Loss: 39.377996\n",
      "Laptime Loss: 64.217065\n",
      "Position Loss: 2.742645\n",
      "Historical Loss: 0.249629\n",
      "--------------------\n",
      "Epoch 38\n",
      "Average Loss: 39.311307\n",
      "Laptime Loss: 64.174555\n",
      "Position Loss: 2.605686\n",
      "Historical Loss: 0.248671\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 39\n",
      "Average Loss: 39.247476\n",
      "Laptime Loss: 64.075101\n",
      "Position Loss: 2.594314\n",
      "Historical Loss: 0.241193\n",
      "--------------------\n",
      "Epoch 40\n",
      "Average Loss: 39.381831\n",
      "Laptime Loss: 64.244727\n",
      "Position Loss: 2.703090\n",
      "Historical Loss: 0.240667\n",
      "--------------------\n",
      "Epoch 41\n",
      "Average Loss: 39.358179\n",
      "Laptime Loss: 64.228524\n",
      "Position Loss: 2.656119\n",
      "Historical Loss: 0.242274\n",
      "--------------------\n",
      "Epoch 42\n",
      "Average Loss: 39.459246\n",
      "Laptime Loss: 64.394455\n",
      "Position Loss: 2.661558\n",
      "Historical Loss: 0.241038\n",
      "--------------------\n",
      "Epoch 43\n",
      "Average Loss: 39.375781\n",
      "Laptime Loss: 64.255146\n",
      "Position Loss: 2.659085\n",
      "Historical Loss: 0.249660\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 44\n",
      "Average Loss: 39.190068\n",
      "Laptime Loss: 63.980118\n",
      "Position Loss: 2.589370\n",
      "Historical Loss: 0.251854\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 45\n",
      "Average Loss: 39.076303\n",
      "Laptime Loss: 63.733365\n",
      "Position Loss: 2.704574\n",
      "Historical Loss: 0.249104\n",
      "--------------------\n",
      "Epoch 46\n",
      "Average Loss: 39.237170\n",
      "Laptime Loss: 64.069534\n",
      "Position Loss: 2.570581\n",
      "Historical Loss: 0.242738\n",
      "--------------------\n",
      "Epoch 47\n",
      "Average Loss: 39.085662\n",
      "Laptime Loss: 63.748408\n",
      "Position Loss: 2.706057\n",
      "Historical Loss: 0.247991\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 48\n",
      "Average Loss: 38.968463\n",
      "Laptime Loss: 63.614139\n",
      "Position Loss: 2.585909\n",
      "Historical Loss: 0.242058\n",
      "--------------------\n",
      "Epoch 49\n",
      "Average Loss: 39.115637\n",
      "Laptime Loss: 63.840161\n",
      "Position Loss: 2.623486\n",
      "Historical Loss: 0.244932\n",
      "--------------------\n",
      "Epoch 50\n",
      "Average Loss: 39.086941\n",
      "Laptime Loss: 63.818942\n",
      "Position Loss: 2.570087\n",
      "Historical Loss: 0.245488\n",
      "--------------------\n",
      "Epoch 51\n",
      "Average Loss: 39.057495\n",
      "Laptime Loss: 63.736649\n",
      "Position Loss: 2.635847\n",
      "Historical Loss: 0.247497\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 52\n",
      "Average Loss: 38.910339\n",
      "Laptime Loss: 63.479086\n",
      "Position Loss: 2.660569\n",
      "Historical Loss: 0.247157\n",
      "--------------------\n",
      "Epoch 53\n",
      "Average Loss: 39.044807\n",
      "Laptime Loss: 63.706696\n",
      "Position Loss: 2.652658\n",
      "Historical Loss: 0.249907\n",
      "--------------------\n",
      "Epoch 54\n",
      "Average Loss: 38.983722\n",
      "Laptime Loss: 63.583560\n",
      "Position Loss: 2.696663\n",
      "Historical Loss: 0.245859\n",
      "--------------------\n",
      "Epoch 55\n",
      "Average Loss: 39.033744\n",
      "Laptime Loss: 63.687294\n",
      "Position Loss: 2.655130\n",
      "Historical Loss: 0.248269\n",
      "--------------------\n",
      "Epoch 56\n",
      "Average Loss: 39.019734\n",
      "Laptime Loss: 63.697690\n",
      "Position Loss: 2.587886\n",
      "Historical Loss: 0.247528\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 57\n",
      "Average Loss: 38.851591\n",
      "Laptime Loss: 63.413841\n",
      "Position Loss: 2.594314\n",
      "Historical Loss: 0.249907\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 58\n",
      "Average Loss: 38.843657\n",
      "Laptime Loss: 63.365528\n",
      "Position Loss: 2.664524\n",
      "Historical Loss: 0.249815\n",
      "--------------------\n",
      "Epoch 59\n",
      "Average Loss: 38.873760\n",
      "Laptime Loss: 63.467265\n",
      "Position Loss: 2.560692\n",
      "Historical Loss: 0.251916\n",
      "--------------------\n",
      "Epoch 60\n",
      "Average Loss: 38.932049\n",
      "Laptime Loss: 63.564080\n",
      "Position Loss: 2.561681\n",
      "Historical Loss: 0.250958\n",
      "--------------------\n",
      "Epoch 61\n",
      "Average Loss: 38.866222\n",
      "Laptime Loss: 63.456897\n",
      "Position Loss: 2.558220\n",
      "Historical Loss: 0.246168\n",
      "--------------------\n",
      "Epoch 62\n",
      "Average Loss: 38.898207\n",
      "Laptime Loss: 63.422998\n",
      "Position Loss: 2.733251\n",
      "Historical Loss: 0.244314\n",
      "--------------------\n",
      "Epoch 63\n",
      "Average Loss: 38.865566\n",
      "Laptime Loss: 63.422494\n",
      "Position Loss: 2.623980\n",
      "Historical Loss: 0.248733\n",
      "--------------------\n",
      "Epoch 64\n",
      "Average Loss: 38.874534\n",
      "Laptime Loss: 63.455253\n",
      "Position Loss: 2.588875\n",
      "Historical Loss: 0.247188\n",
      "--------------------\n",
      "Epoch 65\n",
      "Average Loss: 38.891510\n",
      "Laptime Loss: 63.471663\n",
      "Position Loss: 2.613597\n",
      "Historical Loss: 0.244314\n",
      "--------------------\n",
      "Epoch 66\n",
      "Average Loss: 38.976255\n",
      "Laptime Loss: 63.586792\n",
      "Position Loss: 2.665019\n",
      "Historical Loss: 0.246724\n",
      "--------------------\n",
      "Epoch 67\n",
      "Average Loss: 38.989755\n",
      "Laptime Loss: 63.644579\n",
      "Position Loss: 2.594808\n",
      "Historical Loss: 0.245643\n",
      "--------------------\n",
      "Epoch 68\n",
      "Average Loss: 39.040027\n",
      "Laptime Loss: 63.738258\n",
      "Position Loss: 2.573548\n",
      "Historical Loss: 0.250062\n",
      "--------------------\n",
      "Epoch 69\n",
      "Average Loss: 39.002589\n",
      "Laptime Loss: 63.658515\n",
      "Position Loss: 2.609147\n",
      "Historical Loss: 0.247342\n",
      "--------------------\n",
      "Epoch 70\n",
      "Average Loss: 38.890944\n",
      "Laptime Loss: 63.501340\n",
      "Position Loss: 2.551298\n",
      "Historical Loss: 0.247497\n",
      "--------------------\n",
      "Epoch 71\n",
      "Average Loss: 38.947995\n",
      "Laptime Loss: 63.544426\n",
      "Position Loss: 2.655624\n",
      "Historical Loss: 0.246508\n",
      "--------------------\n",
      "Epoch 72\n",
      "Average Loss: 38.885994\n",
      "Laptime Loss: 63.455802\n",
      "Position Loss: 2.625464\n",
      "Historical Loss: 0.248733\n",
      "Early stopping triggered at epoch 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54359096/ipykernel_857652/2456267903.py:670: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('./2024/best_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 64.6551240762839\n",
      "Outputs\n",
      "O: tensor([[ 9.6066,  9.6141,  9.2736,  9.0526,  8.9343,  8.8653,  8.8216,  8.7917,\n",
      "          8.7699,  8.7528,  8.7387,  9.2972, 10.6261,  9.6016,  9.2165,  9.0227,\n",
      "          8.9187,  8.8579,  8.8196,  8.7937,  8.7749,  8.7605,  8.7488,  8.7388,\n",
      "          8.7301,  8.7223,  8.7151,  8.7085,  8.7023,  8.6965,  8.6910,  8.6858,\n",
      "          9.2695, 10.2599,  9.3743,  9.0517,  8.8885,  8.8012,  8.7506,  8.7192,\n",
      "          8.6985,  8.6839,  8.6730,  8.6643,  8.6572,  8.6510,  8.6456,  8.6406,\n",
      "          8.6361,  8.6318,  8.6278,  8.6240,  8.6204,  8.6169,  8.6135,  8.6103,\n",
      "          8.6071,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.6375,  9.6546,  9.3182,  9.1041,  8.9879,  8.9189,  8.8744,  8.8436,\n",
      "          8.8208,  8.8029,  8.7882,  8.7757,  8.7647,  8.7550,  9.2440, 10.6904,\n",
      "          9.6015,  9.2464,  9.0605,  8.9593,  8.8989,  8.8602,  8.8335,  8.8140,\n",
      "          8.7989,  8.7866,  8.7764,  8.7675,  8.7596,  8.7526,  8.7462,  8.7404,\n",
      "          8.7351,  8.7301,  8.7255,  8.7213,  8.7173,  8.7135,  8.7100,  8.7067,\n",
      "          9.2292, 10.3179,  9.3734,  9.0867,  8.9343,  8.8511,  8.8017,  8.7702,\n",
      "          8.7487,  8.7332,  8.7214,  8.7120,  8.7042,  8.6975,  8.6916,  8.6864,\n",
      "          8.6816,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.5361,  9.6608,  9.3110,  9.0816,  8.9567,  8.8829,  8.8360,  8.8042,\n",
      "          8.7812,  8.7636,  8.7496,  9.1908, 10.7844,  9.6594,  9.2666,  9.0618,\n",
      "          8.9503,  8.8843,  8.8426,  8.8145,  8.7944,  8.7794,  8.7676,  8.7580,\n",
      "          8.7499,  8.7431,  8.7371,  8.7319,  8.7274,  8.7233,  8.7198,  8.7167,\n",
      "          8.7140,  8.7116,  8.7095,  9.1921, 10.4512,  9.4100,  9.0739,  8.8974,\n",
      "          8.8018,  8.7457,  8.7106,  8.6874,  8.6712,  8.6594,  8.6504,  8.6432,\n",
      "          8.6375,  8.6327,  8.6287,  8.6254,  8.6225,  8.6201,  8.6181,  8.6163,\n",
      "          8.6149,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.7791,  9.8047,  9.4300,  9.1956,  9.0678,  8.9919,  8.9431,  8.9095,\n",
      "          8.8848,  8.8655,  8.8499,  8.8366,  9.2833, 10.9395,  9.7673,  9.3712,\n",
      "          9.1640,  9.0508,  8.9834,  8.9403,  8.9108,  8.8893,  8.8728,  8.8596,\n",
      "          8.8487,  8.8394,  8.8313,  8.8242,  8.8179,  8.8123,  8.8073,  8.8028,\n",
      "          8.7988,  8.7952,  9.2811, 10.6127,  9.5094,  9.1649,  8.9838,  8.8855,\n",
      "          8.8275,  8.7907,  8.7659,  8.7483,  8.7350,  8.7246,  8.7163,  8.7093,\n",
      "          8.7034,  8.6984,  8.6940,  8.6903,  8.6870,  8.6842,  8.6817,  8.6795,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.3691,  9.5112,  9.1863,  8.9818,  8.8740,  8.8119,  8.7732,  8.7471,\n",
      "          8.7285,  8.7144,  8.7033,  9.1202, 10.5870,  9.5116,  9.1470,  8.9654,\n",
      "          8.8701,  8.8153,  8.7812,  8.7584,  8.7424,  8.7304,  8.7212,  8.7137,\n",
      "          8.7076,  8.7024,  8.6980,  8.6941,  8.6909,  9.1407, 10.3349,  9.3545,\n",
      "          9.0417,  8.8844,  8.8018,  8.7545,  8.7254,  8.7062,  8.6929,  8.6832,\n",
      "          8.6758,  8.6700,  8.6653,  8.6614,  8.6582,  8.6555,  8.6532,  8.6512,\n",
      "          8.6496,  8.6482,  8.6470,  8.6459,  8.6451,  8.6444,  8.6437,  8.6432,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.6612,  9.6835,  9.3618,  9.1661,  9.0592,  8.9947,  8.9520,  8.9213,\n",
      "          8.8976,  8.8783,  9.3099, 10.6575,  9.6640,  9.3237,  9.1484,  9.0524,\n",
      "          8.9942,  8.9556,  8.9281,  8.9070,  8.8901,  8.8759,  8.8637,  8.8530,\n",
      "          8.8434,  8.8350,  8.8273,  8.8205,  8.8144,  8.8089,  8.8039,  9.2863,\n",
      "         10.4195,  9.4568,  9.1734,  9.0239,  8.9422,  8.8928,  8.8605,  8.8378,\n",
      "          8.8207,  8.8072,  8.7962,  8.7869,  8.7789,  8.7719,  8.7658,  8.7605,\n",
      "          8.7558,  8.7516,  8.7479,  8.7447,  8.7418,  8.7393,  8.7371,  8.7351,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.4546,  9.6442,  9.2776,  9.0349,  8.9018,  8.8236,  8.7746,  8.7420,\n",
      "          8.7188,  8.7015,  8.6877,  8.6762,  8.6663,  8.6576,  8.6497,  8.6424,\n",
      "          9.1295, 10.6379,  9.5962,  9.1965,  8.9890,  8.8754,  8.8089,  8.7676,\n",
      "          8.7405,  8.7217,  8.7079,  8.6973,  8.6886,  8.6814,  8.6750,  8.6694,\n",
      "          8.6644,  8.6597,  8.6555,  8.6515,  9.1625, 10.2795,  9.3576,  9.0039,\n",
      "          8.8216,  8.7227,  8.6655,  8.6305,  8.6081,  8.5931,  8.5825,  8.5746,\n",
      "          8.5685,  8.5636,  8.5595,  8.5559,  8.5529,  8.5501,  8.5477,  8.5455,\n",
      "          8.5435,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.4442,  9.5745,  9.2526,  9.0440,  8.9302,  8.8625,  8.8191,  8.7891,\n",
      "          8.7672,  8.7502,  8.7365,  8.7250,  8.7152,  9.1568, 10.6012,  9.5479,\n",
      "          9.1973,  9.0139,  8.9140,  8.8546,  8.8167,  8.7909,  8.7722,  8.7580,\n",
      "          8.7467,  8.7375,  8.7298,  8.7232,  8.7175,  8.7125,  8.7081,  8.7043,\n",
      "          8.7009,  8.6980,  9.1738, 10.3093,  9.3655,  9.0705,  8.9145,  8.8295,\n",
      "          8.7793,  8.7475,  8.7262,  8.7111,  8.6999,  8.6913,  8.6844,  8.6787,\n",
      "          8.6740,  8.6700,  8.6666,  8.6638,  8.6613,  8.6591,  8.6573,  8.6557,\n",
      "          8.6544,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.6274,  9.6634,  9.3360,  9.1411,  9.0344,  8.9700,  8.9274,  8.8967,\n",
      "          8.8731,  9.2835, 10.6148,  9.6490,  9.3040,  9.1279,  9.0314,  8.9730,\n",
      "          8.9344,  8.9068,  8.8856,  8.8686,  8.8543,  8.8419,  8.8310,  8.8213,\n",
      "          8.8127,  8.8049,  8.7980,  8.7918,  8.7862,  9.2521, 10.4211,  9.4539,\n",
      "          9.1645,  9.0133,  8.9307,  8.8809,  8.8483,  8.8253,  8.8080,  8.7943,\n",
      "          8.7829,  8.7732,  8.7648,  8.7575,  8.7510,  8.7452,  8.7402,  8.7357,\n",
      "          8.7317,  8.7283,  8.7252,  8.7225,  8.7201,  8.7180,  8.7162,  8.7146,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.5599,  9.6407,  9.3069,  9.1039,  8.9930,  8.9264,  8.8828,  8.8517,\n",
      "          9.2550, 10.6482,  9.6453,  9.2821,  9.0969,  8.9958,  8.9349,  8.8950,\n",
      "          8.8667,  8.8454,  8.8283,  8.8142,  8.8021,  8.7916,  8.7823,  8.7740,\n",
      "          8.7666,  8.7600,  9.2181, 10.4672,  9.4641,  9.1542,  8.9926,  8.9045,\n",
      "          8.8517,  8.8173,  8.7932,  8.7753,  8.7612,  8.7497,  8.7401,  8.7318,\n",
      "          8.7246,  8.7182,  8.7127,  8.7078,  8.7035,  8.6996,  8.6963,  8.6933,\n",
      "          8.6907,  8.6884,  8.6863,  8.6844,  8.6828,  8.6813,  8.6800,  8.6788,\n",
      "          8.6776,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.6235,  9.6586,  9.3388,  9.1498,  9.0471,  8.9852,  8.9441,  8.9144,\n",
      "          8.8912,  8.8721,  8.8559,  9.2794, 10.6003,  9.6223,  9.2919,  9.1240,\n",
      "          9.0326,  8.9774,  8.9407,  8.9143,  8.8940,  8.8775,  8.8635,  8.8514,\n",
      "          8.8408,  8.8312,  8.8227,  8.8151,  8.8083,  8.8022,  9.2696, 10.3814,\n",
      "          9.4423,  9.1617,  9.0167,  8.9379,  8.8905,  8.8593,  8.8372,  8.8204,\n",
      "          8.8069,  8.7957,  9.2286, 10.1243,  9.3247,  9.0547,  8.9183,  8.8445,\n",
      "          8.8004,  8.7716,  8.7512,  8.7358,  8.7235,  8.7134,  8.7047,  8.6971,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.3676,  9.5521,  9.2076,  8.9935,  8.8814,  8.8174,  8.7782,  8.7524,\n",
      "          8.7344,  8.7212,  9.1639, 10.6725,  9.5611,  9.1767,  8.9859,  8.8861,\n",
      "          8.8292,  8.7945,  8.7718,  8.7562,  8.7450,  8.7365,  8.7299,  8.7245,\n",
      "          8.7201,  8.7165,  8.7134,  8.7107,  8.7085,  8.7066,  8.7049,  8.7035,\n",
      "          8.7024,  9.1910, 10.3628,  9.3613,  9.0459,  8.8868,  8.8037,  8.7565,\n",
      "          8.7280,  8.7097,  8.6973,  8.6887,  8.6823,  8.6775,  8.6738,  8.6708,\n",
      "          8.6684,  8.6664,  8.6648,  8.6634,  8.6623,  8.6614,  8.6606,  8.6600,\n",
      "          8.6595,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.4137,  9.5559,  9.2215,  9.0199,  8.9135,  8.8517,  8.8124,  8.7855,\n",
      "          8.7656,  8.7502,  8.7377,  8.7273,  9.1621, 10.6153,  9.5299,  9.1735,\n",
      "          8.9963,  8.9030,  8.8488,  8.8145,  8.7912,  8.7743,  8.7613,  8.7510,\n",
      "          8.7425,  8.7354,  8.7293,  8.7240,  8.7195,  8.7155,  8.7121,  8.7090,\n",
      "          9.1827, 10.3476,  9.3562,  9.0558,  8.9045,  8.8249,  8.7789,  8.7500,\n",
      "          8.7306,  8.7167,  8.7063,  8.6981,  8.6914,  8.6859,  8.6813,  8.6774,\n",
      "          8.6740,  8.6711,  8.6686,  8.6664,  8.6646,  8.6629,  8.6614,  8.6601,\n",
      "          8.6590,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.4248,  9.5931,  9.2261,  9.0087,  8.8968,  8.8335,  8.7946,  8.7686,\n",
      "          8.7501,  8.7362,  9.1701, 10.7729,  9.5924,  9.1921,  8.9985,  8.8991,\n",
      "          8.8429,  8.8084,  8.7856,  8.7695,  8.7576,  8.7484,  8.7409,  8.7348,\n",
      "          8.7297,  8.7254,  8.7217,  8.7185,  8.7157,  8.7133,  9.1934, 10.5004,\n",
      "          9.4088,  9.0726,  8.9076,  8.8229,  8.7752,  8.7461,  8.7271,  8.7139,\n",
      "          8.7043,  8.6971,  8.6913,  8.6867,  8.6829,  8.6797,  8.6771,  8.6748,\n",
      "          8.6729,  8.6713,  8.6699,  8.6687,  8.6677,  8.6668,  8.6660,  8.6653,\n",
      "          8.6647,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.5608,  9.6487,  9.3071,  9.1102,  9.0059,  8.9445,  8.9045,  8.8761,\n",
      "          8.8544,  8.8368,  8.8220,  8.8092,  8.7979,  8.7878,  9.2308, 10.6518,\n",
      "          9.5756,  9.2342,  9.0645,  8.9745,  8.9215,  8.8870,  8.8628,  8.8445,\n",
      "          8.8298,  8.8177,  8.8072,  8.7981,  8.7901,  8.7829,  8.7765,  8.7707,\n",
      "          8.7655,  8.7609,  8.7566,  9.2353, 10.3513,  9.3769,  9.0910,  8.9473,\n",
      "          8.8712,  8.8265,  8.7977,  8.7776,  8.7625,  8.7506,  8.7407,  8.7323,\n",
      "          8.7249,  8.7185,  8.7127,  8.7074,  8.7027,  8.6984,  8.6944,  8.6908,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.5748,  9.6657,  9.2962,  9.0825,  8.9701,  8.9053,  8.8643,  8.8362,\n",
      "          8.8155,  8.7994,  8.7863,  8.7752,  8.7657,  9.2016, 10.7606,  9.6105,\n",
      "          9.2343,  9.0484,  8.9509,  8.8945,  8.8589,  8.8348,  8.8172,  8.8037,\n",
      "          8.7928,  8.7838,  8.7761,  8.7694,  8.7636,  8.7585,  8.7540,  8.7499,\n",
      "          8.7464,  9.2178, 10.4643,  9.4199,  9.1022,  8.9437,  8.8606,  8.8127,\n",
      "          8.7828,  8.7627,  8.7482,  8.7372,  8.7284,  8.7212,  8.7152,  8.7099,\n",
      "          8.7054,  8.7014,  8.6980,  8.6949,  8.6921,  8.6896,  8.6874,  8.6854,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.4406,  9.5329,  9.2272,  9.0499,  8.9547,  8.8978,  8.8599,  8.8323,\n",
      "          9.2227, 10.4313,  9.5251,  9.1983,  9.0377,  8.9516,  8.9000,  8.8657,\n",
      "          8.8409,  8.8215,  8.8056,  8.7920,  8.7801,  8.7695,  8.7600,  8.7513,\n",
      "          8.7435,  8.7364,  8.7300,  9.1766, 10.2626,  9.3425,  9.0670,  8.9289,\n",
      "          8.8549,  8.8107,  8.7816,  8.7606,  8.7444,  8.7312,  8.7199,  8.7100,\n",
      "          8.7012,  8.6932,  8.6860,  8.6794,  8.6734,  8.6679,  8.6628,  8.6581,\n",
      "          8.6537,  8.6495,  8.6456,  8.6418,  8.6381,  8.6345,  8.6310,  8.6274,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.4384,  9.5762,  9.2366,  9.0389,  8.9356,  8.8759,  8.8380,  8.8118,\n",
      "          8.7924,  8.7771,  8.7646,  9.2077, 10.6263,  9.5492,  9.1922,  9.0177,\n",
      "          8.9267,  8.8740,  8.8406,  8.8176,  8.8008,  8.7877,  8.7770,  8.7681,\n",
      "          8.7605,  8.7539,  8.7481,  8.7429,  8.7383,  8.7341,  8.7304,  8.7270,\n",
      "          8.7239,  9.2163, 10.3478,  9.3462,  9.0515,  8.9048,  8.8283,  8.7840,\n",
      "          8.7561,  8.7370,  8.7231,  8.7123,  8.7035,  8.6962,  8.6899,  8.6844,\n",
      "          8.6795,  8.6750,  8.6710,  8.6673,  8.6638,  8.6605,  8.6573,  8.6543,\n",
      "          8.6513,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "O: tensor([[ 9.5335,  9.5946,  9.2571,  9.0688,  8.9704,  8.9127,  8.8750,  8.8477,\n",
      "          8.8264,  9.2268, 10.5716,  9.5722,  9.2186,  9.0500,  8.9619,  8.9103,\n",
      "          8.8765,  8.8522,  8.8333,  8.8177,  8.8043,  8.7926,  8.7820,  8.7724,\n",
      "          8.7637,  8.7557,  8.7484,  9.1977, 10.3768,  9.3918,  9.0902,  8.9444,\n",
      "          8.8684,  8.8239,  8.7950,  8.7743,  8.7584,  8.7453,  8.7340,  9.1515,\n",
      "         10.1223,  9.2727,  8.9833,  8.8457,  8.7744,  8.7329,  8.7060,  8.6868,\n",
      "          8.6719,  8.6596,  8.6490,  8.6395,  8.6309,  8.6230,  8.6157,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,\n",
      "          1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914,  1.5914]],\n",
      "       device='cuda:0')\n",
      "Mask\n",
      "[ 9.606597   9.614125   9.273561   9.052623   8.9343405  8.865305\n",
      "  8.821609   8.791749   8.769856   8.7527685  8.738709   9.297179\n",
      " 10.626142   9.601554   9.216507   9.02274    8.918714   8.857936\n",
      "  8.819615   8.793677   8.774909   8.76047    8.7487545  8.738821\n",
      "  8.730112   8.722287   8.715131   8.708508   8.702325   8.696515\n",
      "  8.691031   8.685837   9.269533  10.259906   9.374305   9.051723\n",
      "  8.888504   8.801162   8.750606   8.719246   8.698496   8.683887\n",
      "  8.672967   8.664344   8.657201   8.651048   8.64559    8.640637\n",
      "  8.636072   8.631812   8.6278     8.623995   8.620364   8.616879\n",
      "  8.613519   8.610258   8.607081   1.5913922  1.5913922  1.5913922\n",
      "  1.5913922  1.5913922  1.5913922  1.5913922  1.5913922  1.5913922\n",
      "  1.5913922  1.5913922  1.5913922  1.5913922  1.5913922  1.5913922\n",
      "  1.5913922  1.5913922  1.5913922  1.5913922  1.5913922  1.5913922\n",
      "  1.5913922  1.5913922  1.5913922  1.5913922  1.5913922  1.5913922\n",
      "  1.5913922  1.5913922  1.5913922]\n",
      "END Mask\n",
      "[[9.606597  9.614125  9.273561  ... 1.5913922 1.5913922 1.5913922]\n",
      " [9.637534  9.654648  9.318195  ... 1.5913922 1.5913922 1.5913922]\n",
      " [9.536082  9.660804  9.311032  ... 1.5913922 1.5913922 1.5913922]\n",
      " ...\n",
      " [9.440551  9.532918  9.227183  ... 1.5913922 1.5913922 1.5913922]\n",
      " [9.438437  9.576233  9.23662   ... 1.5913922 1.5913922 1.5913922]\n",
      " [9.533476  9.594555  9.257083  ... 1.5913922 1.5913922 1.5913922]]\n",
      "1 Name: 0    hamilton\n",
      "Name: driverRef, dtype: object\n",
      "4 Name: 3    alonso\n",
      "Name: driverRef, dtype: object\n",
      "815 Name: 814    perez\n",
      "Name: driverRef, dtype: object\n",
      "817 Name: 816    ricciardo\n",
      "Name: driverRef, dtype: object\n",
      "822 Name: 821    bottas\n",
      "Name: driverRef, dtype: object\n",
      "825 Name: 824    kevin_magnussen\n",
      "Name: driverRef, dtype: object\n",
      "830 Name: 829    max_verstappen\n",
      "Name: driverRef, dtype: object\n",
      "832 Name: 831    sainz\n",
      "Name: driverRef, dtype: object\n",
      "839 Name: 838    ocon\n",
      "Name: driverRef, dtype: object\n",
      "840 Name: 839    stroll\n",
      "Name: driverRef, dtype: object\n",
      "842 Name: 452    gasly\n",
      "Name: driverRef, dtype: object\n",
      "844 Name: 842    leclerc\n",
      "Name: driverRef, dtype: object\n",
      "846 Name: 844    norris\n",
      "Name: driverRef, dtype: object\n",
      "847 Name: 845    russell\n",
      "Name: driverRef, dtype: object\n",
      "848 Name: 846    albon\n",
      "Name: driverRef, dtype: object\n",
      "852 Name: 850    tsunoda\n",
      "Name: driverRef, dtype: object\n",
      "855 Name: 853    zhou\n",
      "Name: driverRef, dtype: object\n",
      "857 Name: 855    piastri\n",
      "Name: driverRef, dtype: object\n",
      "858 Name: 856    sargeant\n",
      "Name: driverRef, dtype: object\n",
      "\n",
      "\n",
      "Training model: model_b5_e150_lr0.0001_h256_l1_d0.3_w0.4_0.4_0.2.pth\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 1\n",
      "Average Loss: 41.908029\n",
      "Laptime Loss: 101.801498\n",
      "Position Loss: 2.845488\n",
      "Historical Loss: 0.246168\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 2\n",
      "Average Loss: 30.680210\n",
      "Laptime Loss: 73.808589\n",
      "Position Loss: 2.766873\n",
      "Historical Loss: 0.250124\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 3\n",
      "Average Loss: 30.572863\n",
      "Laptime Loss: 73.630782\n",
      "Position Loss: 2.675896\n",
      "Historical Loss: 0.250958\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 4\n",
      "Average Loss: 30.035529\n",
      "Laptime Loss: 72.252293\n",
      "Position Loss: 2.709518\n",
      "Historical Loss: 0.254017\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 5\n",
      "Average Loss: 29.427627\n",
      "Laptime Loss: 70.668710\n",
      "Position Loss: 2.777750\n",
      "Historical Loss: 0.245210\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 6\n",
      "Average Loss: 29.016014\n",
      "Laptime Loss: 69.718324\n",
      "Position Loss: 2.700618\n",
      "Historical Loss: 0.242182\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 7\n",
      "Average Loss: 28.777600\n",
      "Laptime Loss: 69.153857\n",
      "Position Loss: 2.667985\n",
      "Historical Loss: 0.244314\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 8\n",
      "Average Loss: 28.488598\n",
      "Laptime Loss: 68.416704\n",
      "Position Loss: 2.681335\n",
      "Historical Loss: 0.246910\n",
      "--------------------\n",
      "Epoch 9\n",
      "Average Loss: 28.515382\n",
      "Laptime Loss: 68.355513\n",
      "Position Loss: 2.808405\n",
      "Historical Loss: 0.249073\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 10\n",
      "Average Loss: 28.221650\n",
      "Laptime Loss: 67.636926\n",
      "Position Loss: 2.789122\n",
      "Historical Loss: 0.256150\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 11\n",
      "Average Loss: 27.765220\n",
      "Laptime Loss: 66.494925\n",
      "Position Loss: 2.792089\n",
      "Historical Loss: 0.252070\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 12\n",
      "Average Loss: 27.594207\n",
      "Laptime Loss: 66.156745\n",
      "Position Loss: 2.700124\n",
      "Historical Loss: 0.257293\n",
      "--------------------\n",
      "Epoch 13\n",
      "Average Loss: 27.685676\n",
      "Laptime Loss: 66.276209\n",
      "Position Loss: 2.810878\n",
      "Historical Loss: 0.254203\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 14\n",
      "Average Loss: 27.384711\n",
      "Laptime Loss: 65.469750\n",
      "Position Loss: 2.866749\n",
      "Historical Loss: 0.250556\n",
      "best_model saved\n",
      "--------------------\n",
      "Epoch 15\n",
      "Average Loss: 27.311503\n",
      "Laptime Loss: 65.360306\n",
      "Position Loss: 2.793078\n",
      "Historical Loss: 0.250742\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def pad_sequence_data_2024(sequences, targets, sequence_lengths_2024, pad_length=87):\n",
    "    # sequence_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in sequences]\n",
    "    # target_tensors = [torch.tensor(tgt, dtype=torch.float32) for tgt in targets]\n",
    "    # padded_sequences = pad_sequence(sequence_tensors, batch_first=True)\n",
    "    # padded_targets = pad_sequence(target_tensors, batch_first=True)\n",
    "    # return padded_sequences, padded_targets, sequence_lengths_2024\n",
    "\n",
    "    sequence_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in sequences]\n",
    "    target_tensors = [torch.tensor(tgt, dtype=torch.float32) for tgt in targets]\n",
    "    \n",
    "    # Manually pad or truncate each sequence to the fixed length\n",
    "    padded_sequences = []\n",
    "    padded_targets = []\n",
    "    \n",
    "    for seq, tgt in zip(sequence_tensors, target_tensors):\n",
    "        # Pad sequences (if shorter than pad_length) or truncate them (if longer)\n",
    "        if seq.size(0) < pad_length:\n",
    "            padded_seq = F.pad(seq, (0, 0, 0, pad_length - seq.size(0)))  # Pad along the time dimension\n",
    "        else:\n",
    "            padded_seq = seq[:pad_length]  # Truncate if it's longer than pad_length\n",
    "            \n",
    "        # Pad targets similarly (only time dimension, since targets are 1D)\n",
    "        if tgt.size(0) < pad_length:\n",
    "            padded_tgt = F.pad(tgt, (0, pad_length - tgt.size(0)))  # Only pad the time dimension\n",
    "        else:\n",
    "            padded_tgt = tgt[:pad_length]  # Truncate if it's longer\n",
    "            \n",
    "        padded_sequences.append(padded_seq)\n",
    "        padded_targets.append(padded_tgt)\n",
    "    \n",
    "    # Stack the padded sequences to create batch tensors\n",
    "    padded_sequences = torch.stack(padded_sequences, dim=0)\n",
    "    padded_targets = torch.stack(padded_targets, dim=0)\n",
    "    \n",
    "    return padded_sequences, padded_targets, sequence_lengths_2024\n",
    "\n",
    "\n",
    "def evaluate_2024_season(model):\n",
    "    # Step 1: Load the full dataset for the 2024 season\n",
    "    combined_df_2024 = pd.read_csv('laptimestest3.csv')  # Load the full dataset\n",
    "    combined_df_2024 = combined_df_2024.sort_values(by=['raceId', 'driverId', 'lap']) \n",
    "\n",
    "    # Select only the data for the 2024 season (all rounds)\n",
    "    combined_df_2024_season = combined_df_2024[combined_df_2024['year'] == 2024]\n",
    "\n",
    "    # Apply the same scaling as done on the training data (ensure it's using the same scaler)\n",
    "    # combined_df_2023_season[features] = feature_scaler.transform(combined_df_2023_season[features])\n",
    "    # combined_df_2023_season[target] = target_scaler.transform(combined_df_2023_season[[target]])\n",
    "\n",
    "    combined_df_2024_season = custom_scaler(combined_df_2024_season)\n",
    "\n",
    "    # Step 2: Form sequences for all rounds in 2024\n",
    "    sequences_2024 = []\n",
    "    targets_2024 = []  # Store the true target values\n",
    "    sequence_lengths_2024 = []\n",
    "\n",
    "    for (raceId, driverId), group in combined_df_2024_season.groupby(['raceId', 'driverId']):\n",
    "        group = group.sort_values(by='lap')  # sort by lap within the group\n",
    "        seq_data = group[features].values  # extract feature columns\n",
    "        seq_target = group[target].values  # extract target values (true lap times)\n",
    "        sequences_2024.append(seq_data)\n",
    "        targets_2024.append(seq_target)\n",
    "        sequence_lengths_2024.append(len(seq_data))  # Store the sequence lengths\n",
    "\n",
    "    padded_sequences_2024, padded_targets_2024, sequence_lengths_2024 = pad_sequence_data_2024(sequences_2024, targets_2024, sequence_lengths_2024)\n",
    "    \n",
    "    test_dataset_2024 = TestRaceDataset(padded_sequences_2024, padded_targets_2024, sequence_lengths_2024)\n",
    "    test_data_loader_2024 = DataLoader(test_dataset_2024, batch_size=32, shuffle=False)\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        for batch_X, batch_y, batch_lengths in test_data_loader_2024:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_lengths = batch_lengths.to(device)\n",
    "            \n",
    "            outputs = model(batch_X, batch_lengths)  # Get the model outputs\n",
    "            \n",
    "            # Apply masking to handle padded sequences\n",
    "            # mask = torch.arange(outputs.size(1))[None, :] < batch_lengths[:, None]\n",
    "            mask = torch.arange(outputs.size(1), device=batch_lengths.device)[None, :] < batch_lengths[:, None]\n",
    "            masked_outputs = outputs[mask]\n",
    "            masked_targets = batch_y[mask]\n",
    "\n",
    "            # Calculate loss for the current batch\n",
    "            loss = criterion(masked_outputs, masked_targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_predictions.append(masked_outputs.cpu().numpy())\n",
    "\n",
    "    # Step 5: Post-process the predictions and calculate final loss\n",
    "    # all_predictions = np.concatenate(all_predictions, axis=0)  # Concatenate predictions\n",
    "    total_loss /= len(test_data_loader_2024)  # Calculate average loss over all batches\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def pad_sequence_data_2024_round(sequences, sequence_lengths_2024):\n",
    "    sequence_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in sequences]\n",
    "    padded_sequences = pad_sequence(sequence_tensors, batch_first=True)\n",
    "    return padded_sequences, sequence_lengths_2024\n",
    "\n",
    "\n",
    "def predict_round_22(model):\n",
    "\n",
    "    combined_df_2024 = pd.read_csv('laptimestest3.csv')  # Load the full dataset\n",
    "    combined_df_2024 = combined_df_2024.sort_values(by=['raceId', 'driverId', 'lap']) \n",
    "\n",
    "    # Select round 1 of the 2024 season\n",
    "    combined_df_2024_round1 = combined_df_2024[(combined_df_2024['year'] == 2024) & (combined_df_2024['round'] == 1)]\n",
    "\n",
    "    driver_ids = combined_df_2024_round1.groupby(['raceId', 'driverId']).apply(lambda group: group['driverId'].iloc[0]).values\n",
    "\n",
    "\n",
    "    # Apply the same scaling as done on the training data (ensure it's using the same scaler)\n",
    "\n",
    "    combined_df_2024_round1 = custom_scaler(combined_df_2024_round1)\n",
    "\n",
    "\n",
    "    # Step 2: Form sequences for round 1, 2024\n",
    "    sequences_2024 = []\n",
    "    sequence_lengths_2024 = []\n",
    "\n",
    "    for (raceId, driverId), group in combined_df_2024_round1.groupby(['raceId', 'driverId']):\n",
    "        group = group.sort_values(by='lap')  # sort by lap within the group\n",
    "        seq_data = group[features].values  # extract feature columns\n",
    "        sequences_2024.append(seq_data)\n",
    "        sequence_lengths_2024.append(len(seq_data))  # Store the sequence lengths\n",
    "\n",
    "    padded_sequences_2024, sequence_lengths_2024 = pad_sequence_data_2024_round(sequences_2024,sequence_lengths_2024)\n",
    "\n",
    "    test_dataset_2024 = TestRaceDatasetRound(padded_sequences_2024, sequence_lengths_2024)\n",
    "    # test_data_loader_2024 = DataLoader(test_dataset_2024, batch_size=32, shuffle=False)\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    print(\"Outputs\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_dataset_2024)):\n",
    "            batch_X, length = test_dataset_2024[i]\n",
    "            length = torch.tensor([length], dtype=torch.int64).cpu()  # Convert to 1D tensor\n",
    "\n",
    "            batch_X = batch_X.to(device)\n",
    "            length = length.to(device)\n",
    "            outputs = model(batch_X.unsqueeze(0), length)  # Add batch dimension to X\n",
    "\n",
    "            # mask = torch.arange(outputs.size(1))[None, :] < length[:, None]\n",
    "\n",
    "            # masked_outputs = outputs[mask]\n",
    "            print(f'O: {outputs}')\n",
    "            all_predictions.append(outputs)\n",
    "\n",
    "    # Step 4: Post-process the predictions\n",
    "    all_predictions = torch.cat(all_predictions, dim=0).cpu().numpy()  # Concatenate predictions\n",
    "\n",
    "    all_predictions_original_scale = all_predictions\n",
    "\n",
    "    prediction_features = []\n",
    "    masked_preds = []\n",
    "\n",
    "    print('Mask')\n",
    "    print(all_predictions_original_scale[0])\n",
    "    for i in range(len(test_dataset_2024)):\n",
    "        prediction_features.append(test_dataset_2024[i][0])\n",
    "        \n",
    "        sequence_length = test_dataset_2024[i][1] if isinstance(test_dataset_2024[i][1], torch.Tensor) else torch.tensor(test_dataset_2024[i][1])\n",
    "\n",
    "        # Generate the mask for valid indices based on sequence length\n",
    "        mask = torch.arange(len(all_predictions_original_scale[i])) < sequence_length\n",
    "\n",
    "        # Apply the mask by slicing, since mask is now compatible with the 1D array\n",
    "        masked_output = all_predictions_original_scale[i][mask.numpy()]\n",
    "\n",
    "        masked_preds.append(masked_output)\n",
    "\n",
    "\n",
    "    print('END Mask')\n",
    "\n",
    "    # Print or return the predictions\n",
    "    print(all_predictions_original_scale)\n",
    "\n",
    "\n",
    "    predictions_df = pd.DataFrame(masked_preds)\n",
    "    \n",
    "\n",
    "    # Each driver will have all their lap features in one set of consecutive rows\n",
    "    prediction_features_array = np.concatenate([np.array(driver_laps) for driver_laps in prediction_features], axis=0)\n",
    "\n",
    "    # prediction_features_df = pd.DataFrame(prediction_features_array, columns=features)\n",
    "\n",
    "\n",
    "    driverinfo = pd.read_csv('f1db_csv/drivers.csv')\n",
    "\n",
    "    driver_id_name = []\n",
    "    for id in driver_ids:\n",
    "        name = driverinfo.loc[driverinfo['driverId'] == int(id), 'driverRef']\n",
    "        print(f'{id} Name: {name}')\n",
    "        if(name.empty):\n",
    "            name = None\n",
    "            id = None\n",
    "        else:\n",
    "            # id_name = [id, name]\n",
    "            driver_id_name.append(name.values[0])\n",
    "\n",
    "\n",
    "    predictions_df.index = driver_id_name\n",
    "\n",
    "    return predictions_df\n",
    "    \n",
    "def custom_scaler(df):\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    # Scale specific features\n",
    "    scaling_factors = {\n",
    "        'raceId': 100,\n",
    "        'driverId': 10,\n",
    "        'constructorId': 10,\n",
    "        'year': 100,\n",
    "        'q1milli': 10,\n",
    "        'q2milli': 10,\n",
    "        'q3milli': 10,\n",
    "        'Driver_Season_Points': 10,\n",
    "        'Races_before': 10,\n",
    "        'milliseconds_y': 10000\n",
    "    }\n",
    "    \n",
    "    # Apply scaling to main features\n",
    "    for feature, factor in scaling_factors.items():\n",
    "        if feature in df_scaled.columns:\n",
    "            df_scaled[feature] = df_scaled[feature] / factor\n",
    "    \n",
    "    # # Apply scaling to P1-P20 prefixed features\n",
    "    # for i in range(1, 21):\n",
    "    #     prefix = f'P{i}_'\n",
    "    #     for feature, factor in scaling_factors.items():\n",
    "    #         prefixed_feature = f'{prefix}{feature}'\n",
    "    #         if prefixed_feature in df_scaled.columns:\n",
    "    #             df_scaled[prefixed_feature] = df_scaled[prefixed_feature] / factor\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "def pad_sequence_data(sequences, targets):\n",
    "    sequence_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in sequences]\n",
    "    target_tensors = [torch.tensor(tgt, dtype=torch.float32) for tgt in targets]\n",
    "    padded_sequences = pad_sequence(sequence_tensors, batch_first=True)\n",
    "    padded_targets = pad_sequence(target_tensors, batch_first=True)\n",
    "    return padded_sequences, padded_targets, sequence_lengths\n",
    "\n",
    "\n",
    "def calculate_race_positions(lap_times, mask):\n",
    "    \"\"\"\n",
    "    Calculate total race time and positions for each driver\n",
    "    lap_times: tensor of shape [batch_size, num_laps]\n",
    "    mask: tensor of shape [batch_size, num_laps]\n",
    "    \"\"\"\n",
    "    # Sum valid lap times for each driver\n",
    "    masked_times = lap_times * mask.float()\n",
    "    total_times = torch.sum(masked_times, dim=1)  # [batch_size]\n",
    "    \n",
    "    # Get positions (argsort gives positions in ascending order - fastest first)\n",
    "    positions = torch.argsort(total_times)\n",
    "    \n",
    "    return total_times, positions\n",
    "\n",
    "def get_driver_features(batch_X, features):\n",
    "    \"\"\"\n",
    "    Extract historical performance features for each driver in the batch\n",
    "    \"\"\"\n",
    "    # Get indices of relevant features\n",
    "    points_idx = features.index('Driver_Season_Points')\n",
    "    wins_idx = features.index('driverwins')\n",
    "    podiums_idx = features.index('Podiums')\n",
    "    \n",
    "    # Extract features from first timestep (they remain constant for each sequence)\n",
    "    driver_points = batch_X[:, 0, points_idx]\n",
    "    driver_wins = batch_X[:, 0, wins_idx]\n",
    "    driver_podiums = batch_X[:, 0, podiums_idx]\n",
    "    \n",
    "    # Combine features into a single performance score\n",
    "    historical_performance = (\n",
    "        driver_points + \n",
    "        driver_wins * 25 +  # Weight wins more heavily\n",
    "        driver_podiums * 15  # Weight podiums less than wins\n",
    "    )\n",
    "    \n",
    "    return historical_performance\n",
    "\n",
    "def historical_performance_penalty(pred_positions, historical_performance):\n",
    "    \"\"\"\n",
    "    Penalize predictions that deviate from historical performance expectations\n",
    "    \"\"\"\n",
    "    # Get expected positions based on historical performance (higher score -> better position)\n",
    "    expected_positions = torch.argsort(historical_performance, descending=True)\n",
    "    \n",
    "    # Convert to float and normalize to [0, 1] range\n",
    "    pred_positions_norm = pred_positions.float() / (pred_positions.size(0) - 1)\n",
    "    expected_positions_norm = expected_positions.float() / (expected_positions.size(0) - 1)\n",
    "    \n",
    "    # Calculate MSE between predicted and expected positions\n",
    "    position_penalty = torch.mean((pred_positions_norm - expected_positions_norm) ** 2)\n",
    "    \n",
    "    return position_penalty\n",
    "\n",
    "def calculate_time_decay_weights(years, current_year=2024, old_decay=0.5, recent_decay=0.1):\n",
    "    \"\"\"\n",
    "    Calculate weights with different decay rates for old and recent data\n",
    "    old_decay: stronger decay for 2014-2018\n",
    "    recent_decay: gentler decay for 2019-2022\n",
    "    \"\"\"\n",
    "    years = np.array(years)\n",
    "    if years.ndim == 0:\n",
    "        years = np.array([years])\n",
    "    \n",
    "    weights = np.zeros_like(years, dtype=float)\n",
    "    \n",
    "    for i, year in enumerate(years):\n",
    "        if (year*100) < 2019:\n",
    "            # Stronger decay for older years (2014-2018)\n",
    "            years_from_2018 = 2018 - (year*100)\n",
    "            weights[i] = np.exp(-old_decay * years_from_2018)\n",
    "        else:\n",
    "            # Gentler decay for recent years (2019-2022)\n",
    "            years_from_current = current_year - (year*100)\n",
    "            weights[i] = 1.0 * np.exp(-recent_decay * years_from_current)\n",
    "    \n",
    "    # Add small constant to prevent zero weights\n",
    "    weights = weights + 0.01\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    return weights[0] if len(weights) == 1 else weights\n",
    "\n",
    "def combined_loss_function(outputs, targets, mask, batch_X, features, \n",
    "                         laptime_weight=0.3, position_weight=0.4, historical_weight=0.3):\n",
    "    \"\"\"\n",
    "    Combine laptime MSE, position, and historical performance losses with time decay\n",
    "    \"\"\"\n",
    "    # Get years and calculate time weights\n",
    "    years = batch_X[:, 0, features.index('year')].cpu().numpy()\n",
    "    time_weights = torch.tensor([calculate_time_decay_weights(year) for year in years], \n",
    "                              dtype=torch.float32, \n",
    "                              device=outputs.device)\n",
    "    \n",
    "    # Laptime MSE loss\n",
    "    masked_outputs = outputs[mask]\n",
    "    masked_targets = targets[mask]\n",
    "    laptime_loss = criterion(masked_outputs, masked_targets)\n",
    "    \n",
    "    # Position loss\n",
    "    pred_times, pred_positions = calculate_race_positions(outputs, mask)\n",
    "    true_times, true_positions = calculate_race_positions(targets, mask)\n",
    "    position_loss = criterion(pred_positions.float(), true_positions.float())\n",
    "    \n",
    "    # Historical performance loss\n",
    "    historical_performance = get_driver_features(batch_X, features)\n",
    "    historical_loss = historical_performance_penalty(pred_positions, historical_performance)\n",
    "    \n",
    "    # Apply time weights to each loss component\n",
    "    weighted_laptime_loss = torch.mean(laptime_loss * time_weights)\n",
    "    weighted_position_loss = torch.mean(position_loss * time_weights)\n",
    "    weighted_historical_loss = torch.mean(historical_loss * time_weights)\n",
    "    \n",
    "    # Combine losses with weights\n",
    "    total_loss = (\n",
    "        laptime_weight * weighted_laptime_loss + \n",
    "        position_weight * weighted_position_loss +\n",
    "        historical_weight * weighted_historical_loss\n",
    "    )\n",
    "    \n",
    "    return total_loss, weighted_laptime_loss, weighted_position_loss, weighted_historical_loss\n",
    "    \n",
    "# Create a custom PyTorch Dataset\n",
    "class RaceDataset(Dataset):\n",
    "    def __init__(self, X, y, lengths):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i], self.lengths[i]\n",
    "    \n",
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # Multiple LSTM layers with dropout\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0  # Dropout between LSTM layers\n",
    "        )\n",
    "        \n",
    "        # Dropout layer for the output of the last LSTM layer\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # Batch normalization layer\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the padded sequences\n",
    "        packed_input = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass through LSTM layers\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        \n",
    "        # Unpack the output\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True, total_length=87)\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        output = self.batch_norm(output.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        # Apply dropout\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # Apply the fully connected layer\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        # Squeeze the last dimension to match the shape of batch_y\n",
    "        output = output.squeeze(-1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TestRaceDataset(Dataset):\n",
    "    def __init__(self, X, y, lengths):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i], self.lengths[i]\n",
    "    \n",
    "\n",
    "class TestRaceDatasetRound(Dataset):\n",
    "    def __init__(self, X, lengths):\n",
    "        self.X = X\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.lengths[i]\n",
    "\n",
    "\n",
    "# Define parameter ranges to test\n",
    "batch_sizes = [5, 8, 16] \n",
    "# [5, 8, 16, 32] \n",
    "num_epochs = [150]\n",
    "learning_rates = [0.0001]\n",
    "# [0.0001, 0.0005, 0.001] \n",
    "hidden_sizes = [256, 512] \n",
    "# [128, 256, 512]\n",
    "lstm_layers = [1, 2, 3]\n",
    "dropout_rates = [0.1, 0.2, 0.3]\n",
    "\n",
    "# Weight distributions (laptime, position, historical) must sum to 1\n",
    "weight_distributions = [\n",
    "    (0.6, 0.3, 0.1),\n",
    "    (0.4, 0.4, 0.2)\n",
    "]\n",
    "\n",
    "# [\n",
    "#     (0.6, 0.3, 0.1),\n",
    "#     (0.4, 0.4, 0.2),\n",
    "#     (0.3, 0.4, 0.3)\n",
    "# ]\n",
    "\n",
    "results = []\n",
    "if os.path.exists('./2024/model_results2024.csv'):\n",
    "    results_df = pd.read_csv('./2024/model_results2024.csv')\n",
    "else:\n",
    "    # Initialize an empty DataFrame if file does not exist\n",
    "    results_df = pd.DataFrame(columns=['model_name', 'batch_size', 'epochs', 'learning_rate',\n",
    "                                       'hidden_size', 'lstm_layers', 'dropout',\n",
    "                                       'laptime_weight', 'position_weight', 'historical_weight',\n",
    "                                       'train_loss', 'test_loss'])\n",
    "\n",
    "# Convert model_name column to a set for faster lookup (optional but efficient for large datasets)\n",
    "existing_model_names = set(results_df['model_name']) if not results_df.empty else set()\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for epochs in num_epochs:\n",
    "        for lr in learning_rates:\n",
    "            for hidden_size in hidden_sizes:\n",
    "                for num_layers in lstm_layers:\n",
    "                    for dropout in dropout_rates:\n",
    "                        for weights in weight_distributions:\n",
    "                            # Create model name based on configuration\n",
    "                            model_name = f\"model_b{batch_size}_e{epochs}_lr{lr}_h{hidden_size}_l{num_layers}_d{dropout}_w{weights[0]}_{weights[1]}_{weights[2]}.pth\"\n",
    "                            print(f\"\\n\\nTraining model: {model_name}\")\n",
    "\n",
    "                            if model_name in existing_model_names:\n",
    "                                print(f\"Skipping {model_name} as it already exists.\")\n",
    "                                continue  # Skip this iteration\n",
    "                            \n",
    "                            set_seed(42)\n",
    "                            combined_df = pd.read_csv('laptimestest3.csv')\n",
    "                            combined_df = combined_df.sort_values(by=['raceId', 'driverId', 'lap']) \n",
    "\n",
    "                            features = ['raceId','circuitId','driverId','constructorId', 'grid', 'year', 'round', 'lap', 'q1milli', 'q2milli', 'q3milli', \n",
    "                                        'Driver_Season_Points', 'driverwins', 'YOB', 'Races_before', 'Races_won', 'Podiums', 'isSafetyCar', 'isSafetyCarPrev', \n",
    "                                        'isPitting','tyre_age', 'tyre_compound', 'isVET', 'isZHO', 'isVER', 'isTSU', 'isSTR', 'isMSC', 'isSAR', 'isRIC', \n",
    "                                        'isSAI', 'isRUS', 'isPIA', 'isPER', 'isOCO', 'isNOR', 'isMAG', 'isLEC', 'isLAW', 'isLAT', 'isHUL', 'isHAM', \n",
    "                                        'isGAS', 'isDEV', 'isBOT', 'isBEA', 'isALO', 'isALB', 'isRBR', 'isFER', 'isMER', 'isALP', 'isMCL', 'isALF', \n",
    "                                        'isAST', 'isHAA', 'isATR', 'isWIL']\n",
    "\n",
    "                            target = 'milliseconds_y'\n",
    "\n",
    "                            combined_df = combined_df[combined_df['year']<2024]\n",
    "\n",
    "\n",
    "                            combined_df = custom_scaler(combined_df)\n",
    "\n",
    "                            # forming sequences\n",
    "                            sequences = []\n",
    "                            targets = []\n",
    "                            sequence_lengths = []  # Store sequence lengths\n",
    "\n",
    "                            for (raceId, driverId), group in combined_df.groupby(['raceId', 'driverId']):\n",
    "                                group = group.sort_values(by='lap')  # sort by lap within the group\n",
    "\n",
    "                                if (group['year'] >= 2024).any():\n",
    "                                    continue  # Skip this group if it contains year 2024 or later\n",
    "\n",
    "                                seq_data = group[features].values  # extract feature columns\n",
    "                                seq_target = group[target].values  # extract target values\n",
    "                                sequences.append(seq_data)\n",
    "                                targets.append(seq_target)\n",
    "                                sequence_lengths.append(len(seq_data))  # Store the sequence lengths\n",
    "\n",
    "                            padded_sequences, padded_targets, sequence_lengths = pad_sequence_data(sequences, targets)\n",
    "                            \n",
    "                            dataset = RaceDataset(padded_sequences, padded_targets, sequence_lengths)\n",
    "                            \n",
    "                            \n",
    "\n",
    "                            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            # Initialize model with current configuration\n",
    "                            model = LSTMModel(\n",
    "                                input_size=len(features),\n",
    "                                hidden_size=hidden_size,\n",
    "                                output_size=1,\n",
    "                                num_layers=num_layers,\n",
    "                                dropout=dropout\n",
    "                            ).to(device)\n",
    "                            \n",
    "                            # Initialize optimizer\n",
    "                            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # Create data loader\n",
    "                            data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "                            \n",
    "                            # Train model\n",
    "                            criterion = torch.nn.MSELoss()\n",
    "                            # Modified training loop with learning rate scheduling\n",
    "                            model.train()\n",
    "\n",
    "                            best_loss = float('inf')\n",
    "                            patience = 15\n",
    "                            patience_counter = 0\n",
    "                            for epoch in range(epochs):\n",
    "                                # Your existing training loop code here\n",
    "\n",
    "                                epoch_loss = 0\n",
    "                                epoch_laptime_loss = 0\n",
    "                                epoch_position_loss = 0\n",
    "                                epoch_hist_loss = 0\n",
    "                                batch_count = 0\n",
    "                                \n",
    "                                for batch_X, batch_y, batch_lengths in data_loader:\n",
    "                                    optimizer.zero_grad()\n",
    "\n",
    "                                    batch_X = batch_X.to(device)\n",
    "                                    batch_y = batch_y.to(device)\n",
    "                                    batch_lengths = batch_lengths.to(device)\n",
    "                                    \n",
    "                                    outputs = model(batch_X, batch_lengths)\n",
    "                                    mask = torch.arange(outputs.size(1), device=batch_lengths.device)[None, :] < batch_lengths[:, None]\n",
    "\n",
    "                                    \n",
    "                                    # Calculate combined loss\n",
    "                                    loss, laptime_loss, pos_loss, hist_loss = combined_loss_function(\n",
    "                                        outputs, batch_y, mask,batch_X, features, weights[0], weights[1], weights[2]\n",
    "                                    )\n",
    "                                    \n",
    "                                    loss.backward()\n",
    "                                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                                    optimizer.step()\n",
    "                                    \n",
    "                                    epoch_loss += loss.item()\n",
    "                                    epoch_laptime_loss += laptime_loss.item()\n",
    "                                    epoch_position_loss += pos_loss.item()\n",
    "                                    epoch_hist_loss += hist_loss.item()\n",
    "                                    batch_count += 1\n",
    "                                \n",
    "                                avg_epoch_loss = epoch_loss / batch_count\n",
    "                                avg_laptime_loss = epoch_laptime_loss / batch_count\n",
    "                                avg_position_loss = epoch_position_loss / batch_count\n",
    "                                avg_hist_loss = epoch_hist_loss / batch_count\n",
    "\n",
    "                                # Step the scheduler\n",
    "                                scheduler.step(avg_epoch_loss)\n",
    "\n",
    "\n",
    "                                # Early stopping\n",
    "                                if avg_epoch_loss < best_loss:\n",
    "                                    best_loss = avg_epoch_loss\n",
    "                                    patience_counter = 0\n",
    "                                    # Save the best model\n",
    "                                    # torch.save(model.state_dict(), 'best_model.pth')\n",
    "                                    torch.save(model, './2024/best_model.pth')\n",
    "                                    print('best_model saved')\n",
    "                                else:\n",
    "                                    patience_counter += 1\n",
    "                                    if patience_counter >= patience:\n",
    "                                        print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                                        break\n",
    "                                \n",
    "                                print('--------------------')\n",
    "                                print(f\"Epoch {epoch+1}\")\n",
    "                                print(f\"Average Loss: {avg_epoch_loss:.6f}\")\n",
    "                                print(f\"Laptime Loss: {avg_laptime_loss:.6f}\")\n",
    "                                print(f\"Position Loss: {avg_position_loss:.6f}\")\n",
    "                                print(f\"Historical Loss: {avg_hist_loss:.6f}\")\n",
    "                                \n",
    "                                \n",
    "                            # model.load_state_dict(torch.load('best_model.pth'))\n",
    "                            model = torch.load('./2024/best_model.pth')\n",
    "                            model.eval()\n",
    "                            \n",
    "                            # Test on 2024 season\n",
    "                            test_loss = evaluate_2024_season(model)\n",
    "                            print(f'Test loss: {test_loss}')\n",
    "                            round_22_preds = predict_round_22(model)\n",
    "\n",
    "                            # Save results\n",
    "                            new_result = {\n",
    "                                'model_name': model_name,\n",
    "                                'batch_size': batch_size,\n",
    "                                'epochs': epochs,\n",
    "                                'learning_rate': lr,\n",
    "                                'hidden_size': hidden_size,\n",
    "                                'lstm_layers': num_layers,\n",
    "                                'dropout': dropout,\n",
    "                                'laptime_weight': weights[0],\n",
    "                                'position_weight': weights[1],\n",
    "                                'historical_weight': weights[2],\n",
    "                                'train_loss': best_loss,  # Replace with actual training loss\n",
    "                                'test_loss': test_loss   # Replace with actual test loss\n",
    "                            }\n",
    "                            results_df = pd.concat([results_df, pd.DataFrame([new_result])], ignore_index=True)\n",
    "\n",
    "                            torch.save(model, f'./2024/test/{model_name}.pth')\n",
    "\n",
    "                            # Step 5: Save updated results back to CSV\n",
    "                            results_df.to_csv('./2024/model_results2024.csv', index=False)\n",
    "                            \n",
    "                            round_22_preds.to_csv(f'./2024/Pred-{model_name}.csv')\n",
    "                            # # Save results after each model\n",
    "                            # pd.DataFrame(results).to_csv('model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
